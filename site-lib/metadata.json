{"createdTime":1769100154988,"shownInTree":["overview/categories/injection/alignment-loss.html","overview/categories/injection/joint-modeling.html","overview/categories/injection/none.html","overview/categories/injection/tokenizer.html","overview/categories/rep_signal/external-vfm.html","overview/categories/rep_signal/internal.html","overview/categories/rep_signal/none.html","overview/categories/space/flexible.html","overview/categories/space/latent.html","overview/categories/space/pixel.html","overview/categories/training/end-to-end.html","overview/categories/training/flexible.html","overview/categories/training/two-stage.html","overview/data/bildschirmfoto-2026-01-21-um-15.59.33.png","overview/data/bildschirmfoto-2026-01-21-um-15.59.33.html","overview/data/bildschirmfoto-2026-01-21-um-16.09.13.png","overview/data/bildschirmfoto-2026-01-21-um-16.09.13.html","overview/data/bildschirmfoto-2026-01-21-um-16.13.27.png","overview/data/bildschirmfoto-2026-01-21-um-16.13.27.html","overview/data/bildschirmfoto-2026-01-21-um-16.19.47.png","overview/data/bildschirmfoto-2026-01-21-um-16.19.47.html","overview/data/bildschirmfoto-2026-01-21-um-16.25.46.png","overview/data/bildschirmfoto-2026-01-21-um-16.25.46.html","overview/data/bildschirmfoto-2026-01-21-um-16.31.07.png","overview/data/bildschirmfoto-2026-01-21-um-16.31.07.html","overview/data/bildschirmfoto-2026-01-21-um-16.35.48.png","overview/data/bildschirmfoto-2026-01-21-um-16.35.48.html","overview/data/bildschirmfoto-2026-01-21-um-16.41.45.png","overview/data/bildschirmfoto-2026-01-21-um-16.41.45.html","overview/data/bildschirmfoto-2026-01-21-um-16.45.44.png","overview/data/bildschirmfoto-2026-01-21-um-16.45.44.html","overview/data/bildschirmfoto-2026-01-21-um-16.50.44.png","overview/data/bildschirmfoto-2026-01-21-um-16.50.44.html","overview/data/bildschirmfoto-2026-01-21-um-16.54.26.png","overview/data/bildschirmfoto-2026-01-21-um-16.54.26.html","overview/data/bildschirmfoto-2026-01-21-um-17.04.23.png","overview/data/bildschirmfoto-2026-01-21-um-17.04.23.html","overview/maps/all-papers.html","overview/maps/directionsandquestions.html","overview/maps/visualoverview.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/libackbasicslet2025.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html","overview/papers/2025/wangdiffusedisperseimage2025.html","overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","overview/papers/2025/yupixelditpixeldiffusion2025.html","overview/papers/2025/wangrepaworksit2025a.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2025/wurepresentationentanglementgeneration2025.html","overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/readme.html"],"attachments":["overview/data/bildschirmfoto-2026-01-21-um-15.59.33.png","overview/data/bildschirmfoto-2026-01-21-um-16.09.13.png","overview/data/bildschirmfoto-2026-01-21-um-16.13.27.png","overview/data/bildschirmfoto-2026-01-21-um-16.19.47.png","overview/data/bildschirmfoto-2026-01-21-um-16.25.46.png","overview/data/bildschirmfoto-2026-01-21-um-16.31.07.png","overview/data/bildschirmfoto-2026-01-21-um-16.35.48.png","overview/data/bildschirmfoto-2026-01-21-um-16.41.45.png","overview/data/bildschirmfoto-2026-01-21-um-16.45.44.png","overview/data/bildschirmfoto-2026-01-21-um-16.50.44.png","overview/data/bildschirmfoto-2026-01-21-um-16.54.26.png","overview/data/bildschirmfoto-2026-01-21-um-17.04.23.png","site-lib/scripts/graph-wasm.wasm","site-lib/fonts/94f2f163d4b698242fef.otf","site-lib/fonts/72505e6a122c6acd5471.woff2","site-lib/fonts/2d5198822ab091ce4305.woff2","site-lib/fonts/c8ba52b05a9ef10f4758.woff2","site-lib/fonts/cb10ffd7684cd9836a05.woff2","site-lib/fonts/293fd13dbca5a3e450ef.woff2","site-lib/fonts/085cb93e613ba3d40d2b.woff2","site-lib/fonts/b5f0f109bc88052d4000.woff2","site-lib/fonts/cbe0ae49c52c920fd563.woff2","site-lib/fonts/535a6cf662596b3bd6a6.woff2","site-lib/fonts/70cc7ff27245e82ad414.ttf","site-lib/fonts/454577c22304619db035.ttf","site-lib/fonts/52ac8f3034507f1d9e53.ttf","site-lib/fonts/05b618077343fbbd92b7.ttf","site-lib/fonts/4bb6ac751d1c5478ff3a.woff2","site-lib/media/6155340132a851f6089e.svg","site-lib/media/2308ab1944a6bfa5c5b8.svg","site-lib/fonts/mathjax_zero.woff","site-lib/fonts/mathjax_main-regular.woff","site-lib/fonts/mathjax_main-bold.woff","site-lib/fonts/mathjax_math-italic.woff","site-lib/fonts/mathjax_main-italic.woff","site-lib/fonts/mathjax_math-bolditalic.woff","site-lib/fonts/mathjax_size1-regular.woff","site-lib/fonts/mathjax_size2-regular.woff","site-lib/fonts/mathjax_size3-regular.woff","site-lib/fonts/mathjax_size4-regular.woff","site-lib/fonts/mathjax_ams-regular.woff","site-lib/fonts/mathjax_calligraphic-regular.woff","site-lib/fonts/mathjax_calligraphic-bold.woff","site-lib/fonts/mathjax_fraktur-regular.woff","site-lib/fonts/mathjax_fraktur-bold.woff","site-lib/fonts/mathjax_sansserif-regular.woff","site-lib/fonts/mathjax_sansserif-bold.woff","site-lib/fonts/mathjax_sansserif-italic.woff","site-lib/fonts/mathjax_script-regular.woff","site-lib/fonts/mathjax_typewriter-regular.woff","site-lib/fonts/mathjax_vector-regular.woff","site-lib/fonts/mathjax_vector-bold.woff","site-lib/html/file-tree-content.html","site-lib/scripts/webpage.js","site-lib/scripts/graph-wasm.js","site-lib/scripts/graph-render-worker.js","site-lib/media/favicon.png","site-lib/styles/obsidian.css","site-lib/styles/other-plugins.css","site-lib/styles/global-variable-styles.css","site-lib/styles/main-styles.css","site-lib/rss.xml"],"allFiles":["overview/readme.html","overview/maps/directionsandquestions.html","overview/maps/all-papers.html","overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2025/libackbasicslet2025.html","overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","overview/papers/2025/wangrepaworksit2025a.html","overview/papers/2025/wurepresentationentanglementgeneration2025.html","overview/papers/2025/yupixelditpixeldiffusion2025.html","overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","overview/papers/2025/wangdiffusedisperseimage2025.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/maps/visualoverview.html","overview/data/bildschirmfoto-2026-01-21-um-17.04.23.png","overview/data/bildschirmfoto-2026-01-21-um-17.04.23.html","overview/data/bildschirmfoto-2026-01-21-um-16.54.26.png","overview/data/bildschirmfoto-2026-01-21-um-16.54.26.html","overview/data/bildschirmfoto-2026-01-21-um-16.50.44.png","overview/data/bildschirmfoto-2026-01-21-um-16.50.44.html","overview/data/bildschirmfoto-2026-01-21-um-16.45.44.png","overview/data/bildschirmfoto-2026-01-21-um-16.45.44.html","overview/data/bildschirmfoto-2026-01-21-um-16.41.45.png","overview/data/bildschirmfoto-2026-01-21-um-16.41.45.html","overview/data/bildschirmfoto-2026-01-21-um-16.35.48.png","overview/data/bildschirmfoto-2026-01-21-um-16.35.48.html","overview/data/bildschirmfoto-2026-01-21-um-16.31.07.png","overview/data/bildschirmfoto-2026-01-21-um-16.31.07.html","overview/data/bildschirmfoto-2026-01-21-um-16.25.46.png","overview/data/bildschirmfoto-2026-01-21-um-16.25.46.html","overview/data/bildschirmfoto-2026-01-21-um-16.19.47.png","overview/data/bildschirmfoto-2026-01-21-um-16.19.47.html","overview/data/bildschirmfoto-2026-01-21-um-16.13.27.png","overview/data/bildschirmfoto-2026-01-21-um-16.13.27.html","overview/data/bildschirmfoto-2026-01-21-um-16.09.13.png","overview/data/bildschirmfoto-2026-01-21-um-16.09.13.html","overview/data/bildschirmfoto-2026-01-21-um-15.59.33.png","overview/data/bildschirmfoto-2026-01-21-um-15.59.33.html","overview/categories/training/flexible.html","overview/categories/injection/none.html","overview/categories/training/two-stage.html","overview/categories/training/end-to-end.html","overview/categories/space/flexible.html","overview/categories/injection/tokenizer.html","overview/categories/injection/joint-modeling.html","overview/categories/injection/alignment-loss.html","overview/categories/rep_signal/none.html","overview/categories/rep_signal/internal.html","overview/categories/rep_signal/external-vfm.html","overview/categories/space/pixel.html","overview/categories/space/latent.html","site-lib/scripts/graph-wasm.wasm","site-lib/fonts/94f2f163d4b698242fef.otf","site-lib/fonts/72505e6a122c6acd5471.woff2","site-lib/fonts/2d5198822ab091ce4305.woff2","site-lib/fonts/c8ba52b05a9ef10f4758.woff2","site-lib/fonts/cb10ffd7684cd9836a05.woff2","site-lib/fonts/293fd13dbca5a3e450ef.woff2","site-lib/fonts/085cb93e613ba3d40d2b.woff2","site-lib/fonts/b5f0f109bc88052d4000.woff2","site-lib/fonts/cbe0ae49c52c920fd563.woff2","site-lib/fonts/535a6cf662596b3bd6a6.woff2","site-lib/fonts/70cc7ff27245e82ad414.ttf","site-lib/fonts/454577c22304619db035.ttf","site-lib/fonts/52ac8f3034507f1d9e53.ttf","site-lib/fonts/05b618077343fbbd92b7.ttf","site-lib/fonts/4bb6ac751d1c5478ff3a.woff2","site-lib/media/6155340132a851f6089e.svg","site-lib/media/2308ab1944a6bfa5c5b8.svg","site-lib/fonts/mathjax_zero.woff","site-lib/fonts/mathjax_main-regular.woff","site-lib/fonts/mathjax_main-bold.woff","site-lib/fonts/mathjax_math-italic.woff","site-lib/fonts/mathjax_main-italic.woff","site-lib/fonts/mathjax_math-bolditalic.woff","site-lib/fonts/mathjax_size1-regular.woff","site-lib/fonts/mathjax_size2-regular.woff","site-lib/fonts/mathjax_size3-regular.woff","site-lib/fonts/mathjax_size4-regular.woff","site-lib/fonts/mathjax_ams-regular.woff","site-lib/fonts/mathjax_calligraphic-regular.woff","site-lib/fonts/mathjax_calligraphic-bold.woff","site-lib/fonts/mathjax_fraktur-regular.woff","site-lib/fonts/mathjax_fraktur-bold.woff","site-lib/fonts/mathjax_sansserif-regular.woff","site-lib/fonts/mathjax_sansserif-bold.woff","site-lib/fonts/mathjax_sansserif-italic.woff","site-lib/fonts/mathjax_script-regular.woff","site-lib/fonts/mathjax_typewriter-regular.woff","site-lib/fonts/mathjax_vector-regular.woff","site-lib/fonts/mathjax_vector-bold.woff","site-lib/html/file-tree-content.html","site-lib/scripts/webpage.js","site-lib/scripts/graph-wasm.js","site-lib/scripts/graph-render-worker.js","site-lib/media/favicon.png","site-lib/styles/obsidian.css","site-lib/styles/other-plugins.css","site-lib/styles/global-variable-styles.css","site-lib/styles/main-styles.css"],"webpages":{"overview/categories/injection/alignment-loss.html":{"title":"alignment-loss","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/injection/alignment-loss.html","pathToRoot":"../../..","attachments":[],"createdTime":1768817841833,"modifiedTime":1768817841833,"sourceSize":0,"sourcePath":"Overview/Categories/injection/alignment-loss.md","exportPath":"overview/categories/injection/alignment-loss.html","showInTree":true,"treeOrder":3,"backlinks":["overview/readme.html","overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/wangrepaworksit2025a.html","overview/papers/2025/yupixelditpixeldiffusion2025.html"],"type":"markdown"},"overview/categories/injection/joint-modeling.html":{"title":"joint-modeling","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/injection/joint-modeling.html","pathToRoot":"../../..","attachments":[],"createdTime":1768817848608,"modifiedTime":1768817848608,"sourceSize":0,"sourcePath":"Overview/Categories/injection/joint-modeling.md","exportPath":"overview/categories/injection/joint-modeling.html","showInTree":true,"treeOrder":4,"backlinks":["overview/readme.html","overview/papers/2025/wurepresentationentanglementgeneration2025.html"],"type":"markdown"},"overview/categories/injection/none.html":{"title":"none","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/injection/none.html","pathToRoot":"../../..","attachments":[],"createdTime":1768839687558,"modifiedTime":1768839687558,"sourceSize":0,"sourcePath":"Overview/Categories/injection/none.md","exportPath":"overview/categories/injection/none.html","showInTree":true,"treeOrder":5,"backlinks":["overview/readme.html","overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","overview/papers/2025/wangdiffusedisperseimage2025.html","overview/papers/2025/libackbasicslet2025.html"],"type":"markdown"},"overview/categories/injection/tokenizer.html":{"title":"tokenizer","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/injection/tokenizer.html","pathToRoot":"../../..","attachments":[],"createdTime":1768817854941,"modifiedTime":1768817854941,"sourceSize":0,"sourcePath":"Overview/Categories/injection/tokenizer.md","exportPath":"overview/categories/injection/tokenizer.html","showInTree":true,"treeOrder":6,"backlinks":["overview/readme.html","overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html"],"type":"markdown"},"overview/categories/rep_signal/external-vfm.html":{"title":"external-vfm","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/rep_signal/external-vfm.html","pathToRoot":"../../..","attachments":[],"createdTime":1768817759883,"modifiedTime":1768817759883,"sourceSize":0,"sourcePath":"Overview/Categories/rep_signal/external-vfm.md","exportPath":"overview/categories/rep_signal/external-vfm.html","showInTree":true,"treeOrder":8,"backlinks":["overview/readme.html","overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html","overview/papers/2025/wangrepaworksit2025a.html","overview/papers/2025/yupixelditpixeldiffusion2025.html","overview/papers/2025/wurepresentationentanglementgeneration2025.html"],"type":"markdown"},"overview/categories/rep_signal/internal.html":{"title":"internal","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/rep_signal/internal.html","pathToRoot":"../../..","attachments":[],"createdTime":1768817808699,"modifiedTime":1768817808699,"sourceSize":0,"sourcePath":"Overview/Categories/rep_signal/internal.md","exportPath":"overview/categories/rep_signal/internal.html","showInTree":true,"treeOrder":9,"backlinks":["overview/readme.html","overview/papers/2025/wangdiffusedisperseimage2025.html"],"type":"markdown"},"overview/categories/rep_signal/none.html":{"title":"none","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/rep_signal/none.html","pathToRoot":"../../..","attachments":[],"createdTime":1768817821509,"modifiedTime":1768817821509,"sourceSize":0,"sourcePath":"Overview/Categories/rep_signal/none.md","exportPath":"overview/categories/rep_signal/none.html","showInTree":true,"treeOrder":10,"backlinks":["overview/readme.html","overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","overview/papers/2025/libackbasicslet2025.html"],"type":"markdown"},"overview/categories/space/flexible.html":{"title":"flexible","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/space/flexible.html","pathToRoot":"../../..","attachments":[],"createdTime":1768817872627,"modifiedTime":1768817872627,"sourceSize":0,"sourcePath":"Overview/Categories/space/flexible.md","exportPath":"overview/categories/space/flexible.html","showInTree":true,"treeOrder":12,"backlinks":["overview/readme.html","overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html","overview/papers/2025/wangrepaworksit2025a.html","overview/papers/2025/wangdiffusedisperseimage2025.html","overview/papers/2025/wurepresentationentanglementgeneration2025.html"],"type":"markdown"},"overview/categories/space/latent.html":{"title":"latent","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/space/latent.html","pathToRoot":"../../..","attachments":[],"createdTime":1768577924932,"modifiedTime":1768577924932,"sourceSize":0,"sourcePath":"Overview/Categories/space/latent.md","exportPath":"overview/categories/space/latent.html","showInTree":true,"treeOrder":13,"backlinks":["overview/readme.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html"],"type":"markdown"},"overview/categories/space/pixel.html":{"title":"pixel","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/space/pixel.html","pathToRoot":"../../..","attachments":[],"createdTime":1768577936836,"modifiedTime":1768577936836,"sourceSize":0,"sourcePath":"Overview/Categories/space/pixel.md","exportPath":"overview/categories/space/pixel.html","showInTree":true,"treeOrder":14,"backlinks":["overview/readme.html","overview/papers/2025/libackbasicslet2025.html","overview/papers/2025/yupixelditpixeldiffusion2025.html"],"type":"markdown"},"overview/categories/training/end-to-end.html":{"title":"end-to-end","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/training/end-to-end.html","pathToRoot":"../../..","attachments":[],"createdTime":1768818908083,"modifiedTime":1768818908083,"sourceSize":0,"sourcePath":"Overview/Categories/training/end-to-end.md","exportPath":"overview/categories/training/end-to-end.html","showInTree":true,"treeOrder":16,"backlinks":["overview/readme.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/wangdiffusedisperseimage2025.html","overview/papers/2025/libackbasicslet2025.html","overview/papers/2025/yupixelditpixeldiffusion2025.html"],"type":"markdown"},"overview/categories/training/flexible.html":{"title":"flexible","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/training/flexible.html","pathToRoot":"../../..","attachments":[],"createdTime":1769004290137,"modifiedTime":1769004290137,"sourceSize":0,"sourcePath":"Overview/Categories/training/flexible.md","exportPath":"overview/categories/training/flexible.html","showInTree":true,"treeOrder":17,"backlinks":[],"type":"markdown"},"overview/categories/training/two-stage.html":{"title":"two-stage","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/training/two-stage.html","pathToRoot":"../../..","attachments":[],"createdTime":1768818913606,"modifiedTime":1768818913606,"sourceSize":0,"sourcePath":"Overview/Categories/training/two-stage.md","exportPath":"overview/categories/training/two-stage.html","showInTree":true,"treeOrder":18,"backlinks":["overview/readme.html","overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html","overview/papers/2025/wangrepaworksit2025a.html","overview/papers/2025/wurepresentationentanglementgeneration2025.html"],"type":"markdown"},"overview/data/bildschirmfoto-2026-01-21-um-15.59.33.html":{"title":"Bildschirmfoto 2026-01-21 um 15.59.33","icon":"","description":"<img src=\"overview/data/bildschirmfoto-2026-01-21-um-15.59.33.png\" target=\"_self\">","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":".","fullURL":"overview/data/bildschirmfoto-2026-01-21-um-15.59.33.html","pathToRoot":"../..","attachments":["overview/data/bildschirmfoto-2026-01-21-um-15.59.33.html"],"createdTime":1769007576952,"modifiedTime":1769007576953,"sourceSize":510254,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 15.59.33.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-15.59.33.html","showInTree":true,"treeOrder":20,"backlinks":[],"type":"attachment"},"overview/data/bildschirmfoto-2026-01-21-um-16.09.13.html":{"title":"Bildschirmfoto 2026-01-21 um 16.09.13","icon":"","description":"<img src=\"overview/data/bildschirmfoto-2026-01-21-um-16.09.13.png\" target=\"_self\">","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":".","fullURL":"overview/data/bildschirmfoto-2026-01-21-um-16.09.13.html","pathToRoot":"../..","attachments":["overview/data/bildschirmfoto-2026-01-21-um-16.09.13.html"],"createdTime":1769008157392,"modifiedTime":1769008157393,"sourceSize":182049,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.09.13.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.09.13.html","showInTree":true,"treeOrder":21,"backlinks":[],"type":"attachment"},"overview/data/bildschirmfoto-2026-01-21-um-16.13.27.html":{"title":"Bildschirmfoto 2026-01-21 um 16.13.27","icon":"","description":"<img src=\"overview/data/bildschirmfoto-2026-01-21-um-16.13.27.png\" target=\"_self\">","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":".","fullURL":"overview/data/bildschirmfoto-2026-01-21-um-16.13.27.html","pathToRoot":"../..","attachments":["overview/data/bildschirmfoto-2026-01-21-um-16.13.27.html"],"createdTime":1769008410327,"modifiedTime":1769008410330,"sourceSize":51226,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.13.27.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.13.27.html","showInTree":true,"treeOrder":22,"backlinks":[],"type":"attachment"},"overview/data/bildschirmfoto-2026-01-21-um-16.19.47.html":{"title":"Bildschirmfoto 2026-01-21 um 16.19.47","icon":"","description":"<img src=\"overview/data/bildschirmfoto-2026-01-21-um-16.19.47.png\" target=\"_self\">","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":".","fullURL":"overview/data/bildschirmfoto-2026-01-21-um-16.19.47.html","pathToRoot":"../..","attachments":["overview/data/bildschirmfoto-2026-01-21-um-16.19.47.html"],"createdTime":1769008792834,"modifiedTime":1769008792835,"sourceSize":72150,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.19.47.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.19.47.html","showInTree":true,"treeOrder":23,"backlinks":[],"type":"attachment"},"overview/data/bildschirmfoto-2026-01-21-um-16.25.46.html":{"title":"Bildschirmfoto 2026-01-21 um 16.25.46","icon":"","description":"<img src=\"overview/data/bildschirmfoto-2026-01-21-um-16.25.46.png\" target=\"_self\">","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":".","fullURL":"overview/data/bildschirmfoto-2026-01-21-um-16.25.46.html","pathToRoot":"../..","attachments":["overview/data/bildschirmfoto-2026-01-21-um-16.25.46.html"],"createdTime":1769009151595,"modifiedTime":1769009151598,"sourceSize":58217,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.25.46.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.25.46.html","showInTree":true,"treeOrder":24,"backlinks":[],"type":"attachment"},"overview/data/bildschirmfoto-2026-01-21-um-16.31.07.html":{"title":"Bildschirmfoto 2026-01-21 um 16.31.07","icon":"","description":"<img src=\"overview/data/bildschirmfoto-2026-01-21-um-16.31.07.png\" target=\"_self\">","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":".","fullURL":"overview/data/bildschirmfoto-2026-01-21-um-16.31.07.html","pathToRoot":"../..","attachments":["overview/data/bildschirmfoto-2026-01-21-um-16.31.07.html"],"createdTime":1769009473904,"modifiedTime":1769009473904,"sourceSize":71462,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.31.07.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.31.07.html","showInTree":true,"treeOrder":25,"backlinks":[],"type":"attachment"},"overview/data/bildschirmfoto-2026-01-21-um-16.35.48.html":{"title":"Bildschirmfoto 2026-01-21 um 16.35.48","icon":"","description":"<img src=\"overview/data/bildschirmfoto-2026-01-21-um-16.35.48.png\" target=\"_self\">","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":".","fullURL":"overview/data/bildschirmfoto-2026-01-21-um-16.35.48.html","pathToRoot":"../..","attachments":["overview/data/bildschirmfoto-2026-01-21-um-16.35.48.html"],"createdTime":1769009752892,"modifiedTime":1769009752893,"sourceSize":118668,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.35.48.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.35.48.html","showInTree":true,"treeOrder":26,"backlinks":[],"type":"attachment"},"overview/data/bildschirmfoto-2026-01-21-um-16.41.45.html":{"title":"Bildschirmfoto 2026-01-21 um 16.41.45","icon":"","description":"<img src=\"overview/data/bildschirmfoto-2026-01-21-um-16.41.45.png\" target=\"_self\">","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":".","fullURL":"overview/data/bildschirmfoto-2026-01-21-um-16.41.45.html","pathToRoot":"../..","attachments":["overview/data/bildschirmfoto-2026-01-21-um-16.41.45.html"],"createdTime":1769010108424,"modifiedTime":1769010108424,"sourceSize":191958,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.41.45.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.41.45.html","showInTree":true,"treeOrder":27,"backlinks":[],"type":"attachment"},"overview/data/bildschirmfoto-2026-01-21-um-16.45.44.html":{"title":"Bildschirmfoto 2026-01-21 um 16.45.44","icon":"","description":"<img src=\"overview/data/bildschirmfoto-2026-01-21-um-16.45.44.png\" target=\"_self\">","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":".","fullURL":"overview/data/bildschirmfoto-2026-01-21-um-16.45.44.html","pathToRoot":"../..","attachments":["overview/data/bildschirmfoto-2026-01-21-um-16.45.44.html"],"createdTime":1769010348759,"modifiedTime":1769010348763,"sourceSize":287460,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.45.44.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.45.44.html","showInTree":true,"treeOrder":28,"backlinks":[],"type":"attachment"},"overview/data/bildschirmfoto-2026-01-21-um-16.50.44.html":{"title":"Bildschirmfoto 2026-01-21 um 16.50.44","icon":"","description":"<img src=\"overview/data/bildschirmfoto-2026-01-21-um-16.50.44.png\" target=\"_self\">","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":".","fullURL":"overview/data/bildschirmfoto-2026-01-21-um-16.50.44.html","pathToRoot":"../..","attachments":["overview/data/bildschirmfoto-2026-01-21-um-16.50.44.html"],"createdTime":1769010647407,"modifiedTime":1769010647407,"sourceSize":24009,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.50.44.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.50.44.html","showInTree":true,"treeOrder":29,"backlinks":[],"type":"attachment"},"overview/data/bildschirmfoto-2026-01-21-um-16.54.26.html":{"title":"Bildschirmfoto 2026-01-21 um 16.54.26","icon":"","description":"<img src=\"overview/data/bildschirmfoto-2026-01-21-um-16.54.26.png\" target=\"_self\">","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":".","fullURL":"overview/data/bildschirmfoto-2026-01-21-um-16.54.26.html","pathToRoot":"../..","attachments":["overview/data/bildschirmfoto-2026-01-21-um-16.54.26.html"],"createdTime":1769010869864,"modifiedTime":1769010869864,"sourceSize":469338,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.54.26.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.54.26.html","showInTree":true,"treeOrder":30,"backlinks":[],"type":"attachment"},"overview/data/bildschirmfoto-2026-01-21-um-17.04.23.html":{"title":"Bildschirmfoto 2026-01-21 um 17.04.23","icon":"","description":"<img src=\"overview/data/bildschirmfoto-2026-01-21-um-17.04.23.png\" target=\"_self\">","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":".","fullURL":"overview/data/bildschirmfoto-2026-01-21-um-17.04.23.html","pathToRoot":"../..","attachments":["overview/data/bildschirmfoto-2026-01-21-um-17.04.23.html"],"createdTime":1769011465911,"modifiedTime":1769011465912,"sourceSize":256193,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 17.04.23.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-17.04.23.html","showInTree":true,"treeOrder":31,"backlinks":[],"type":"attachment"},"overview/maps/all-papers.html":{"title":"All-Papers","icon":"","description":"Note that the imagenet_fid score is computed on ImageNet 256x256 resolution, w/o classifier free guidance, after 400K training steps, using a SiT, if not marked with a \"*\" at the end of the score. For details on the setup for the FID with \"*\" click onto the respective summary page and go to Results.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":["overview/papers/2025/wangdiffusedisperseimage2025.html","overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","overview/papers/2025/yupixelditpixeldiffusion2025.html","overview/papers/2025/libackbasicslet2025.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2025/wurepresentationentanglementgeneration2025.html","overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2025/wangrepaworksit2025a.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html"],"author":"","coverImageURL":"","fullURL":"overview/maps/all-papers.html","pathToRoot":"../..","attachments":[],"createdTime":1768577986054,"modifiedTime":1769091767311,"sourceSize":451,"sourcePath":"Overview/Maps/All-Papers.md","exportPath":"overview/maps/all-papers.html","showInTree":true,"treeOrder":33,"backlinks":["overview/readme.html"],"type":"markdown"},"overview/maps/directionsandquestions.html":{"title":"DirectionsAndQuestions","icon":"","description":"In the following I try to structure my perspective on what approaches for improving the way diffusion models learn can try. I aimed to put it into a more high level/mathematical perspective which is further away from the actual implementation design choices used.\nI also formulated some ideas and questions: I hope this gives us a good reference point to discuss where this project is headed more precisely. I would be very happy on feedback and potential ideas from your side ü§ó Structured to be easily referenced :)I am aware that some of the motivations/goals are hard to frame mathematically, e.g. pixel-space diffusion might be preferable over latent-space diffusion, simplify since this simplifies the architecture and reduces the number of external models required etc.I am most comfortable with the flow-matching framework, so everything is written from that perspective.A: Problem: What properties of the target distribution (or a coupling with the base (gaussian) distribution) allow for a easily learnable velocity field? (simplify training)\nsmooth conditional velocity field (e.g. small Lipschitz constant)? (simplify inference, since less NFEs needed later on)\nB: Theoretical questions/ideas:\nCan we obtain sample complexity bounds for the diffusion training as a function of the target distribution? -&gt; simplified result (target distribution is GMM with uniform weights and identity covariance matrices) that exists here is Theorem 2.1 in MAETok paper (see <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://arxiv.org/pdf/2502.03444#page=2.60\" target=\"_self\">https://arxiv.org/pdf/2502.03444#page=2.60</a>)\nCan we get smoothness guarantees (Lipschitz bounds) for the optimal velocity field based on properties on or a coupling ?\nC: Empirical questions/ideas:\n<br>Measure variance of the conditional velocity field , where randomness comes from , to understand how hard the learning problem is (model essentially has to average out directions if variance is high (idea is similar to flow-matching vs rectified flows trajectories, see Paths Crossed at the Wrong Time part in <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://alechelbling.com/blog/rectified-flow/#:~:text=with%20fewer%20crossings.-,Paths%20Crossed%20at%20the%20Wrong%20Time,-Our%20learned%20flow\" target=\"_self\">https://alechelbling.com/blog/rectified-flow/#:~:text=with%20fewer%20crossings.-,Paths%20Crossed%20at%20the%20Wrong%20Time,-Our%20learned%20flow</a>))\n<br>Compute empirical statistics on curvature of solution trajectories when diffusion model is learned on specialized latents (e.g. <a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/lengREPAEUnlockingVAE2025a\" data-href=\"Overview/Papers/2025/lengREPAEUnlockingVAE2025a\" href=\"overview/papers/2025/lengrepaeunlockingvae2025a.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">REPA-E</a>, <a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/zhengDiffusionTransformersRepresentation2025\" data-href=\"Overview/Papers/2025/zhengDiffusionTransformersRepresentation2025\" href=\"overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">RAE</a> or <a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/skorokhodovImprovingDiffusabilityAutoencoders2025\" data-href=\"Overview/Papers/2025/skorokhodovImprovingDiffusabilityAutoencoders2025\" href=\"overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">FT-SE</a>) vs classic SD-VAE\nA: Problem: How does modeling the joint velocity field of images and representations reduce the complexity of learning the marginal dynamics?B: Theoretical question/idea:\nIf the marginal velocity field (just learning to denoise the (semantic) VFM-based token) is simple to learn and mutual information between and is large, does that make it easier (to be quantified) to learn than just the marginal velocity field ?\nC: Empirical question/idea:\nCompare isolated joint-modeling (e.g. REG/ReDi w/o REPA, using different approaches to obtain the additional token) with baseline SiT on convergence speed, A: Problem: How does an additional alignment loss affect the learning problem?B: Theoretical questions/ideas:\nCan we get bounds on the distance between learned distribution and target distribution that are dependent on the quality of the representations?<br>\n-&gt; <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://arxiv.org/abs/2507.08980\" target=\"_self\">https://arxiv.org/abs/2507.08980</a> provides a TV bound on learned distribution and target distribution derived from the DDPM framework (see Theorem 1 in REED <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://arxiv.org/pdf/2507.08980)\" target=\"_self\">https://arxiv.org/pdf/2507.08980)</a>; could we make the bound more explicit on the quality of the representation (e.g. bound their by a function of )?\nHow does the alignment loss modify the loss landscape, e.g. improve smoothness?\nC: Empirical ideas:\n<br>Investigate gradient similarity between and during training -&gt; essentially this idea was covered in <a data-tooltip-position=\"top\" aria-label=\"wangREPAWorksIt2025a\" data-href=\"wangREPAWorksIt2025a\" href=\"overview/papers/2025/wangrepaworksit2025a.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">HASTE</a> but could be investigated more thoroughly for different types of alignment losses\nCompute statistics of Hessians (e.g. condition number) of and and compare them -&gt; information on loss landscape\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1. Target distribution","level":3,"id":"1._Target_distribution_0"},{"heading":"2. Joint-Modeling","level":3,"id":"2._Joint-Modeling_0"},{"heading":"3. Alignment loss","level":3,"id":"3._Alignment_loss_0"}],"links":["overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","overview/papers/2025/wangrepaworksit2025a.html"],"author":"","coverImageURL":"","fullURL":"overview/maps/directionsandquestions.html","pathToRoot":"../..","attachments":[],"createdTime":1769003617263,"modifiedTime":1769100098513,"sourceSize":4651,"sourcePath":"Overview/Maps/DirectionsAndQuestions.md","exportPath":"overview/maps/directionsandquestions.html","showInTree":true,"treeOrder":34,"backlinks":["overview/readme.html"],"type":"markdown"},"overview/maps/visualoverview.html":{"title":"VisualOverview","icon":"","description":"<a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/liBackBasicsLet2025\" data-href=\"Overview/Papers/2025/liBackBasicsLet2025\" href=\"overview/papers/2025/libackbasicslet2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Back to Basics: Let Denoising Generative Models Denoise</a>Idea: Just Image Transformers (JiT) predict in pixel space to leverage low dimensionality of data manifold.<br><img alt=\"Bildschirmfoto 2026-01-21 um 15.59.33.png\" src=\"overview/data/bildschirmfoto-2026-01-21-um-15.59.33.png\" target=\"_self\" style=\"width: 220px; max-width: 100%;\">FID: 2.79 | space: pixel | training-regime: end-to-end<br><a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/chenDiffusionAutoencodersAre2025\" data-href=\"Overview/Papers/2025/chenDiffusionAutoencodersAre2025\" href=\"overview/papers/2025/chendiffusionautoencodersare2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Diffusion Autoencoders are Scalable Image Tokenizers</a>Idea: Replace the VAE decocer with a diffusion model conditioned on the latent of an encoder to not rely on complex losses (e.g. GAN &amp; LPIPS).<br><img alt=\"Bildschirmfoto 2026-01-21 um 16.35.48.png\" src=\"overview/data/bildschirmfoto-2026-01-21-um-16.35.48.png\" target=\"_self\" style=\"width: 300px; max-width: 100%;\">FID: 6.29* | space: latent | training-regime: end-to-end<br><a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/singhWhatMattersRepresentation2025\" data-href=\"Overview/Papers/2025/singhWhatMattersRepresentation2025\" href=\"overview/papers/2025/singhwhatmattersrepresentation2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">WHAT MATTERS FOR REPRESENTATION ALIGNMENT: GLOBAL INFORMATION OR SPATIAL STRUCTURE?</a>Idea: iREPA tweaks REPA by replacing the projection MLP with a conv layer and adding spatial normalization to better preserve ‚Äúspatial structure‚Äù during teacher-to-diffusion feature alignment.<br><img alt=\"Bildschirmfoto 2026-01-21 um 16.41.45.png\" src=\"overview/data/bildschirmfoto-2026-01-21-um-16.41.45.png\" target=\"_self\">FID: 7.52 | space: flexible | training-regime: two-stage<br><a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/zhengDiffusionTransformersRepresentation2025\" data-href=\"Overview/Papers/2025/zhengDiffusionTransformersRepresentation2025\" href=\"overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Diffusion Transformers With Represenation Autoencoders</a>Idea: Train the diffusion model in the latent space of a strong VFM (e.g. DINOv2).<br><img alt=\"Bildschirmfoto 2026-01-21 um 16.13.27.png\" src=\"overview/data/bildschirmfoto-2026-01-21-um-16.13.27.png\" target=\"_self\" style=\"width: 300px; max-width: 100%;\">FID: 2.16* | space: latent | training-regime: two-stage<br><a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/skorokhodovImprovingDiffusabilityAutoencoders2025\" data-href=\"Overview/Papers/2025/skorokhodovImprovingDiffusabilityAutoencoders2025\" href=\"overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Improving the Diffusability of Autoencoders</a> Idea: They fine tune the VAE using scale equivariance, such that the latent space has less high-frequency signals, simplifying the training task for the diffusion model.<br><img alt=\"Bildschirmfoto 2026-01-21 um 17.04.23.png\" src=\"overview/data/bildschirmfoto-2026-01-21-um-17.04.23.png\" target=\"_self\">FID: 9.61* | space: latent | training-regime: two-stage<br><a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2024/yuRepresentationAlignmentGeneration2025\" data-href=\"Overview/Papers/2024/yuRepresentationAlignmentGeneration2025\" href=\"overview/papers/2024/yurepresentationalignmentgeneration2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think</a>Idea: REPA adds a representation alignment loss that aligns the diffusion models hidden representations to those of a vision foundation model (VFM) to speed up DiT/SiT training.<br><img alt=\"Bildschirmfoto 2026-01-21 um 16.45.44.png\" src=\"overview/data/bildschirmfoto-2026-01-21-um-16.45.44.png\" target=\"_self\" style=\"width: 300px; max-width: 100%;\">FID: 7.9 | space: flexible | training-regime: two-stage<br><a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/lengREPAEUnlockingVAE2025a\" data-href=\"Overview/Papers/2025/lengREPAEUnlockingVAE2025a\" href=\"overview/papers/2025/lengrepaeunlockingvae2025a.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers</a>Idea: End-to-end training where the VAE component is optimized by backpropagating the REPA loss.<br><img alt=\"Bildschirmfoto 2026-01-21 um 16.25.46.png\" src=\"overview/data/bildschirmfoto-2026-01-21-um-16.25.46.png\" target=\"_self\" style=\"width: 200px; max-width: 100%;\">FID: 4.07 | space: latent | training-regime: end-to-end<br><a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/yuPixelDiTPixelDiffusion2025\" data-href=\"Overview/Papers/2025/yuPixelDiTPixelDiffusion2025\" href=\"overview/papers/2025/yupixelditpixeldiffusion2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">PixelDiT: Pixel Diffusion Transformers for Image Generation</a>Idea: Dual level transformer based architecture that focuses on semantics first and pixel-level afterwards to operate efficiently in pixel space.<br><img alt=\"Bildschirmfoto 2026-01-21 um 16.19.47.png\" src=\"overview/data/bildschirmfoto-2026-01-21-um-16.19.47.png\" target=\"_self\" style=\"width: 250px; max-width: 100%;\">\nFID: 2.36* | space: pixel | training-regime: end-to-end<br><a data-tooltip-position=\"top\" aria-label=\"wangREPAWorksIt2025a\" data-href=\"wangREPAWorksIt2025a\" href=\"overview/papers/2025/wangrepaworksit2025a.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">REPA Works Until It Doesn‚Äôt: Early-Stopped, Holistic Alignment Supercharges Diffusion Training</a>Idea: Compared to always-on REPA feature alignment for SiT/DiT, HASTE adds teacher attention-map distillation and turns alignment off mid-training via a stage-wise termination switch.<br><img alt=\"Bildschirmfoto 2026-01-21 um 16.50.44.png\" src=\"overview/data/bildschirmfoto-2026-01-21-um-16.50.44.png\" target=\"_self\" style=\"width: 350px; max-width: 100%;\">FID: 8.9 | space: flexible | training-regime: two-stage<br><a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/wangDiffuseDisperseImage2025\" data-href=\"Overview/Papers/2025/wangDiffuseDisperseImage2025\" href=\"overview/papers/2025/wangdiffusedisperseimage2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Diffuse and Disperse: Image Generation with Representation Regularization</a> Idea: Use a dispersive loss so that the model leverages the full space (contrastive loss w/o positive pairs).<br><img alt=\"Bildschirmfoto 2026-01-21 um 16.09.13.png\" src=\"overview/data/bildschirmfoto-2026-01-21-um-16.09.13.png\" target=\"_self\">FID: 16.68 | space: flexible | training-regime: end-to-end<br><a data-tooltip-position=\"top\" aria-label=\"wuRepresentationEntanglementGeneration2025\" data-href=\"wuRepresentationEntanglementGeneration2025\" href=\"overview/papers/2025/wurepresentationentanglementgeneration2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think</a> Idea: Additionally to the REPA loss, learn to denoise the CLS token.<br>\n<img alt=\"Bildschirmfoto 2026-01-21 um 16.31.07.png\" src=\"overview/data/bildschirmfoto-2026-01-21-um-16.31.07.png\" target=\"_self\" style=\"width: 200px; max-width: 100%;\">FID: 4.6 | space: flexible | training-regime: two-stage<br><a data-tooltip-position=\"top\" aria-label=\"kouzelisBoostingGenerativeImage2025\" data-href=\"kouzelisBoostingGenerativeImage2025\" href=\"overview/papers/2025/kouzelisboostinggenerativeimage2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Boosting Generative Image Modeling via Joint Image-Feature Synthesis</a>Idea: The diffusion model jointly learn the latents of a VAE and the latents of a VFM, additionally to representation alignment.<br><img alt=\"Bildschirmfoto 2026-01-21 um 16.54.26.png\" src=\"overview/data/bildschirmfoto-2026-01-21-um-16.54.26.png\" target=\"_self\">FID: 9.4 | space: flexible | training-regime: two-stagevery similar methods\nessentially PCA (ReDi) vs CLS (REG) Latent space Pixel space Flexible","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Representation quality improvement via ‚Ä¶","level":2,"id":"Representation_quality_improvement_via_‚Ä¶_0"},{"heading":"Recent Developments in generative models in the image domain","level":1,"id":"Recent_Developments_in_generative_models_in_the_image_domain_0"},{"heading":"External VFM","level":3,"id":"External_VFM_0"},{"heading":"Internal Regularization","level":3,"id":"Internal_Regularization_0"},{"heading":"None","level":3,"id":"None_0"},{"heading":"Injection of Representation via ‚Ä¶","level":3,"id":"Injection_of_Representation_via_‚Ä¶_0"},{"heading":"Alignment loss","level":4,"id":"Alignment_loss_0"},{"heading":"JiT","level":4,"id":"JiT_0"},{"heading":"DiTo","level":4,"id":"DiTo_0"},{"heading":"iREPA","level":4,"id":"iREPA_0"},{"heading":"RAE","level":4,"id":"RAE_0"},{"heading":"FT-SE","level":4,"id":"FT-SE_0"},{"heading":"REPA","level":4,"id":"REPA_0"},{"heading":"REPA-E","level":4,"id":"REPA-E_0"},{"heading":"PixelDiT","level":4,"id":"PixelDiT_0"},{"heading":"HASTE","level":4,"id":"HASTE_0"},{"heading":"Diffuse &amp; Disperse","level":4,"id":"Diffuse_&_Disperse_0"},{"heading":"Tokenizer","level":4,"id":"Tokenizer_0"},{"heading":"REG","level":4,"id":"REG_0"},{"heading":"ReDi","level":4,"id":"ReDi_0"},{"heading":"Legend","level":3,"id":"Legend_0"}],"links":["overview/papers/2025/libackbasicslet2025.html","overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2025/yupixelditpixeldiffusion2025.html","overview/papers/2025/wangrepaworksit2025a.html","overview/papers/2025/wangdiffusedisperseimage2025.html","overview/papers/2025/wurepresentationentanglementgeneration2025.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html"],"author":"","coverImageURL":".","fullURL":"overview/maps/visualoverview.html","pathToRoot":"../..","attachments":["overview/data/bildschirmfoto-2026-01-21-um-15.59.33.html","overview/data/bildschirmfoto-2026-01-21-um-16.35.48.html","overview/data/bildschirmfoto-2026-01-21-um-16.41.45.html","overview/data/bildschirmfoto-2026-01-21-um-16.13.27.html","overview/data/bildschirmfoto-2026-01-21-um-17.04.23.html","overview/data/bildschirmfoto-2026-01-21-um-16.45.44.html","overview/data/bildschirmfoto-2026-01-21-um-16.25.46.html","overview/data/bildschirmfoto-2026-01-21-um-16.19.47.html","overview/data/bildschirmfoto-2026-01-21-um-16.50.44.html","overview/data/bildschirmfoto-2026-01-21-um-16.09.13.html","overview/data/bildschirmfoto-2026-01-21-um-16.31.07.html","overview/data/bildschirmfoto-2026-01-21-um-16.54.26.html"],"createdTime":1769002725757,"modifiedTime":1769074479155,"sourceSize":10534,"sourcePath":"Overview/Maps/VisualOverview.canvas","exportPath":"overview/maps/visualoverview.html","showInTree":true,"treeOrder":35,"backlinks":["overview/readme.html"],"type":"canvas"},"overview/papers/2024/yurepresentationalignmentgeneration2025.html":{"title":"Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2410.06940\" target=\"_self\">http://arxiv.org/abs/2410.06940</a>\nRelated:: They add a representation alignment loss that aligns the diffusion models hidden representations to those of a vision foundation model (VFM) to speed up DiT/SiT training.They motivate their work by previous findings that better diffusion models learn hidden representations are semantically more meaningful (higher linear probing accuracy). Representations are already weakly aligned (measured via CKNNA) with those of DINOv2. To improve alignment they add an alignment loss to their overall lossHere denotes the patch index, the hidden representation at a fixed layer, the VFMs representation and a similarity function, e.g. cosine similarity. is a 3-layer MLP that is learned during training. They find that alignment at earlier layers (e.g. 8 out of 24 layers) yields best results.\nThe overall loss becomes where is a hyperparameter used to control the amount of alignment.\n<br>Space: <a data-tooltip-position=\"top\" aria-label=\"Overview/Categories/space/flexible\" data-href=\"Overview/Categories/space/flexible\" href=\"overview/categories/space/flexible.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">flexible</a>\n<br>Rep signal: <a data-href=\"external-vfm\" href=\"overview/categories/rep_signal/external-vfm.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">external-vfm</a>\n<br>Injection: <a data-href=\"alignment-loss\" href=\"overview/categories/injection/alignment-loss.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">alignment-loss</a>\n<br>Training: <a data-href=\"two-stage\" href=\"overview/categories/training/two-stage.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">two-stage</a> <br>ImageNet: FID 7.9 @ 256x256 px, w/o cfg, 400K training steps, DINOv2-B, SiT-XL/2 (see Table 8 in <a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/singhWhatMattersRepresentation2025\" data-href=\"Overview/Papers/2025/singhWhatMattersRepresentation2025\" href=\"overview/papers/2025/singhwhatmattersrepresentation2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">singhWhatMattersRepresentation2025</a>) <br>Why are earlier layers more suitable for alignment? They argue that later layers focus on high-frequency details, but <a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/singhWhatMattersRepresentation2025\" data-href=\"Overview/Papers/2025/singhWhatMattersRepresentation2025\" href=\"overview/papers/2025/singhwhatmattersrepresentation2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">singhWhatMattersRepresentation2025</a> use early layers (e.g. 4, 6 or 8, see Table 1c) as well, who don't focus on semantic/high-level structure.\nThey focus mainly on LDM (SiT is their base backbone model) - more extensive results in pixel diffusion might be interesting.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results","level":2,"id":"Results_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"}],"links":["overview/categories/space/flexible.html","overview/categories/rep_signal/external-vfm.html","overview/categories/injection/alignment-loss.html","overview/categories/training/two-stage.html","overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2025/singhwhatmattersrepresentation2025.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2024/yurepresentationalignmentgeneration2025.html","pathToRoot":"../../..","attachments":[],"createdTime":1768820192430,"modifiedTime":1769075125359,"sourceSize":2778,"sourcePath":"Overview/Papers/2024/yuRepresentationAlignmentGeneration2025.md","exportPath":"overview/papers/2024/yurepresentationalignmentgeneration2025.html","showInTree":true,"treeOrder":38,"backlinks":["overview/papers/2025/singhwhatmattersrepresentation2025.html"],"type":"markdown"},"overview/papers/2025/libackbasicslet2025.html":{"title":"Back to Basics: Let Denoising Generative Models Denoise","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2511.13720\" target=\"_self\">http://arxiv.org/abs/2511.13720</a>\nRelated:: The minimizer of and prediction losses have the (high) ambient dimension and hence the diffusion model has a difficult training target. They propose to let the model predict the ground truth during denoising to leverage low dimensionality of the support of the data dimension.Originally diffusion models where trained to predict the clean image from a noised version . The community has shifted to using noise (-prediction) or velocity (-prediction) instead, which the authors explain as likely being \"legacy reasons\".\nFrom the flow-matching framework they present the relations between and prediction as well as the , and losses. The nine possible combinations (see Table 1) are all not equivalent (pointwise equal everywhere). However, a scalar reweighting can be applied to obtain equivalence.\nMotivated by this the Li and He investigate the effect of both loss reweighting and noise-level shifts. While the latter can lead to FID improvement (see Table 3), \"catastrophic failure\" of and prediction in higher resolution regimes (e.g. px; see Table 2) can not be prevented.\nThey use aggressive patch sizes (image_size/ 16) and are still able to train their model effectively, which they explain by the low dimensionality of the data. Overall, their architecture is just a DiT with standard timestep and class conditioning (AdaLN-Zero). They only let the model predict and use a velocity-loss formulation.\n<br>Space: <a data-href=\"pixel\" href=\"overview/categories/space/pixel.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">pixel</a>\n<br>Rep signal: <a data-tooltip-position=\"top\" aria-label=\"Overview/Categories/rep_signal/none\" data-href=\"Overview/Categories/rep_signal/none\" href=\"overview/categories/rep_signal/none.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">none</a>\n<br>Injection: <a data-tooltip-position=\"top\" aria-label=\"Overview/Categories/injection/none\" data-href=\"Overview/Categories/injection/none\" href=\"overview/categories/injection/none.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">none</a>\n<br>Training: <a data-href=\"end-to-end\" href=\"overview/categories/training/end-to-end.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">end-to-end</a> ImageNet: FID 2.79 @ 256x256 px, w/ cfg, 200 epochs (lowest epoch number they show), JiT-L as diffusion backbone (similar param number as SiT-L) More ablations on training loss convergence against iteration steps would be interesting to compare against other methods (they only report after 200 and 600 epochs) They use several \"engineering hacks\" such as bottleneck embedding, RoPE/qk-norm, in-context class tokens, dropout/early stop (for H- and G-models) but don't ablate the contribution of those components\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results","level":2,"id":"Results_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"}],"links":["overview/categories/space/pixel.html","overview/categories/rep_signal/none.html","overview/categories/injection/none.html","overview/categories/training/end-to-end.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/libackbasicslet2025.html","pathToRoot":"../../..","attachments":[],"createdTime":1768927936821,"modifiedTime":1769075240846,"sourceSize":2778,"sourcePath":"Overview/Papers/2025/liBackBasicsLet2025.md","exportPath":"overview/papers/2025/libackbasicslet2025.html","showInTree":true,"treeOrder":40,"backlinks":[],"type":"markdown"},"overview/papers/2025/kouzelisboostinggenerativeimage2025.html":{"title":"Boosting Generative Image Modeling via Joint Image-Feature Synthesis","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2504.16064\" target=\"_self\">http://arxiv.org/abs/2504.16064</a>\n<br>Related:: <a data-href=\"wuRepresentationEntanglementGeneration2025\" href=\"overview/papers/2025/wurepresentationentanglementgeneration2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">wuRepresentationEntanglementGeneration2025</a> They propose ReDi, a generative framework, in which the diffusion model jointly learns the latents of a VAE and the latents of a VFM.With ReDi, they jointly model (i) VAE latents and (ii) the latents of a pretrained VFM (in their experiments, they use DINOv2-B).\nThey consider two approaches to feed the VAE latents and the visual representation tokens to the SiT/DiT:\nMerged Tokens: Here both token are transformed separately to the same dimension and then summed channel-wise to obtain Separate Tokens: Tokens are just concatenated along the sequence dimension They use the merged token approach by default to reduce computational complexity.\nTo bring VAE latents and VFM latents to similar channel sizes, they apply PCA on the VFM latents. Furthermore, they apply representation guidance during inference to control the influence of the VFMs representations, which is similarly designed to CFG.\nBy combining ReDi with REPA, they are able to improve convergence speed and achievable FID.\n<br>Space: <a data-tooltip-position=\"top\" aria-label=\"Overview/Categories/space/flexible\" data-href=\"Overview/Categories/space/flexible\" href=\"overview/categories/space/flexible.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">flexible</a>\n<br>Rep signal: <a data-href=\"external-vfm\" href=\"overview/categories/rep_signal/external-vfm.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">external-vfm</a>\n<br>Injection: <a data-href=\"tokenizer\" href=\"overview/categories/injection/tokenizer.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">tokenizer</a>\n<br>Training: <a data-href=\"two-stage\" href=\"overview/categories/training/two-stage.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">two-stage</a> ImageNet: FID 9.4 @ 256x256 px, w/o cfg, 400K training steps, SiT-L as diffusion backbone and DINOv2-B for external representations (see Table 1) They don't provide explanations of why\n(i) this approach works well (/better than REPA)\n(ii) they choose DINOv2 and why other VFMs perform worse (see openreview W2 in rebuttal to Reviewer 8DbC20)\nHow does it scale to higher resolutions (e.g. )?\nMissing understanding for why REPA and ReDi is complementary\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results","level":2,"id":"Results_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"}],"links":["overview/papers/2025/wurepresentationentanglementgeneration2025.html","overview/categories/space/flexible.html","overview/categories/rep_signal/external-vfm.html","overview/categories/injection/tokenizer.html","overview/categories/training/two-stage.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/kouzelisboostinggenerativeimage2025.html","pathToRoot":"../../..","attachments":[],"createdTime":1768908441822,"modifiedTime":1769075252694,"sourceSize":2712,"sourcePath":"Overview/Papers/2025/kouzelisBoostingGenerativeImage2025.md","exportPath":"overview/papers/2025/kouzelisboostinggenerativeimage2025.html","showInTree":true,"treeOrder":41,"backlinks":["overview/papers/2025/wurepresentationentanglementgeneration2025.html"],"type":"markdown"},"overview/papers/2025/wangdiffusedisperseimage2025.html":{"title":"Diffuse and Disperse: Image Generation with Representation Regularization","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2506.09027\" target=\"_self\">http://arxiv.org/abs/2506.09027</a>\nRelated:: Additionally to the diffusion loss, they add a representation regularization, called dispersive loss, that does not use external models, and is fully self-supervised.Their method aims to simplify the diffusion training by explicitly improving the representation quality of the model's internal representations of the data. They define dispersive loss functions as contrastive losses without positive pairs, for example (their Table 1) This is supposed to encourage the diffusion model to have its representations spread out in space.\nThe authors argue that as the regression diffusion loss already acts similarly to the loss part between positive pairs, the latter is redundant when training diffusion models.\n<br>Space: <a data-tooltip-position=\"top\" aria-label=\"Overview/Categories/space/flexible\" data-href=\"Overview/Categories/space/flexible\" href=\"overview/categories/space/flexible.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">flexible</a>\n<br>Rep signal: <a data-href=\"internal\" href=\"overview/categories/rep_signal/internal.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">internal</a>\n<br>Injection: <a data-tooltip-position=\"top\" aria-label=\"Overview/Categories/injection/none\" data-href=\"Overview/Categories/injection/none\" href=\"overview/categories/injection/none.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">none</a>\n<br>Training: <a data-href=\"end-to-end\" href=\"overview/categories/training/end-to-end.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">end-to-end</a> ImageNet: FID 16.68 @ 256x256 px, w/o cfg, 400K training steps, SiT-L as diffusion backbone (see Figure 4) They don't provide a theoretical explanation of why their method works\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results","level":2,"id":"Results_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"}],"links":["overview/categories/space/flexible.html","overview/categories/rep_signal/internal.html","overview/categories/injection/none.html","overview/categories/training/end-to-end.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/wangdiffusedisperseimage2025.html","pathToRoot":"../../..","attachments":[],"createdTime":1768923759178,"modifiedTime":1769075156037,"sourceSize":2365,"sourcePath":"Overview/Papers/2025/wangDiffuseDisperseImage2025.md","exportPath":"overview/papers/2025/wangdiffusedisperseimage2025.html","showInTree":true,"treeOrder":42,"backlinks":[],"type":"markdown"},"overview/papers/2025/chendiffusionautoencodersare2025.html":{"title":"Diffusion Autoencoders are Scalable Image Tokenizers","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2501.18593\" target=\"_self\">http://arxiv.org/abs/2501.18593</a>\nRelated:: They replace the VAE component with a diffusion model that is conditioned on the latent representation of an encoder to simplify the learning procedure and not rely on external models (GAN, LPIPS loss in standard VAE).They motivate their work by noting that the VAE component used in modern latent diffusion models is based on several \"heuristic\" losses (LPIPS, GAN and reconstruction loss). They propose to instead use a diffusion loss to learn both the encoder and decoder jointly. They condition their diffusion decoder (U-Net) by simply concatenating it to the input, i.e. where is the noised input and is the output of the encoder (during inference the output of the latent diffusion model; transformed to correct resolution via nearest neighbors if latent resolution differs).\n<br>Space: <a data-href=\"latent\" href=\"overview/categories/space/latent.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">latent</a>\n<br>Rep signal: <a data-href=\"Overview/Categories/rep_signal/none\" href=\"overview/categories/rep_signal/none.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Overview/Categories/rep_signal/none</a>\n<br>Injection: <a data-href=\"injection/none\" href=\"overview/categories/injection/none.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">injection/none</a>\n<br>Training: <a data-href=\"end-to-end\" href=\"overview/categories/training/end-to-end.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">end-to-end</a> ImageNet: FID 6.29 @ 256x256 px, w cfg of 2, DiT-XL/2 as diffusion backbone, !number of training steps is not reported They have very limited quantitative results, particularly in the generative domain\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results","level":2,"id":"Results_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"}],"links":["overview/categories/space/latent.html","overview/categories/rep_signal/none.html","overview/categories/injection/none.html","overview/categories/training/end-to-end.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/chendiffusionautoencodersare2025.html","pathToRoot":"../../..","attachments":[],"createdTime":1768838020230,"modifiedTime":1769075264844,"sourceSize":1937,"sourcePath":"Overview/Papers/2025/chenDiffusionAutoencodersAre2025.md","exportPath":"overview/papers/2025/chendiffusionautoencodersare2025.html","showInTree":true,"treeOrder":43,"backlinks":["overview/papers/2025/singhwhatmattersrepresentation2025.html"],"type":"markdown"},"overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html":{"title":"Diffusion Transformers with Representation Autoencoders","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2510.11690\" target=\"_self\">http://arxiv.org/abs/2510.11690</a>\nRelated:: This paper proposes to train the diffusion model in the latent space of a VFM instead of a VAE for improved generation quality.They want to simplify the training procedure, not by guiding the diffusion model via an external model (e.g. REPA) but by learning the latent of a VFM. This reduces hyperparameters and simplifies the architecture.\nTo decode the latents, they train a ViT with their objective as where and denote the encoder (VFM, precisely DINOv2-B/SigLIP2-B and MAE-B) and the decoder (ViT-XL) respectively. They get competitive/better reconstruction FID scores with RAEs compared to typical SD-VAE (see Table 1). To make the RAE decoder more robust, they train it on a smoothed distribution (noise-augmented decoding), effectively learning to decode where denotes the training set processed by the RAE encoder. This enables the decoder to perform well during the generative part.\nThey further show that the unique minimizer of the flow matching loss is representable via a stack of standard DiT-Block iff , where is the dimension of the tokens. They argue that this is due to the full rank of the noise (see also Back To Basics).<br>\nInstead of scaling the width of all the DiT blocks, which is computationally prohibitive, they use a wide DDT (<a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://arxiv.org/pdf/2504.05741v1\" target=\"_self\">https://arxiv.org/pdf/2504.05741v1</a>) head after the standard DiT.<br>\nBuilding on (<a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://arxiv.org/pdf/2403.03206\" target=\"_self\">https://arxiv.org/pdf/2403.03206</a>) they propose a schedule shift but using the effective data dimension computed as \"number of tokens times their dimensionality\".<br>\nThey don't include CLS tokens that contain strong semantic information in the training target (cf. arguments in <a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/singhWhatMattersRepresentation2025\" data-href=\"Overview/Papers/2025/singhWhatMattersRepresentation2025\" href=\"overview/papers/2025/singhwhatmattersrepresentation2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">singhWhatMattersRepresentation2025</a>)\n<br>Space: <a data-href=\"latent\" href=\"overview/categories/space/latent.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">latent</a>\n<br>Rep signal: <a data-href=\"external-vfm\" href=\"overview/categories/rep_signal/external-vfm.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">external-vfm</a>\n<br>Injection: <a data-href=\"tokenizer\" href=\"overview/categories/injection/tokenizer.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">tokenizer</a>\n<br>Training: <a data-href=\"two-stage\" href=\"overview/categories/training/two-stage.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">two-stage</a> ImageNet: FID 2.16 @ 256x256 px, w/o cfg, 80 epochs - about 400k training steps as batchsize = 256: and , LightningDiT-XL as diffusion backbone and train on the latents of DINOv2-B How does the autoencoder part RAE work on large scale (e.g. LAION dataset)?\nThey use DINOv2 as it works best in the generative part, however they don't provide any explanation for that. What makes their features good for representation? (compare this to Diffusability, MAETok Theorem 1 and What matter)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results","level":2,"id":"Results_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"}],"links":["overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/categories/space/latent.html","overview/categories/rep_signal/external-vfm.html","overview/categories/injection/tokenizer.html","overview/categories/training/two-stage.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","pathToRoot":"../../..","attachments":[],"createdTime":1768840091876,"modifiedTime":1769075186930,"sourceSize":3679,"sourcePath":"Overview/Papers/2025/zhengDiffusionTransformersRepresentation2025.md","exportPath":"overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","showInTree":true,"treeOrder":44,"backlinks":["overview/maps/directionsandquestions.html"],"type":"markdown"},"overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html":{"title":"Improving the Diffusability of Autoencoders","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2502.14831\" target=\"_self\">http://arxiv.org/abs/2502.14831</a>\nRelated:: They fine tune the VAE using scale equivariance, such that the latent space has less high-frequency signals, simplifying the training task for the diffusion model.Their key assumption is that diffusion models \"synthesize low-frequency components first and add high-frequency ones on top\" later, which results in pictures that are perceived as realistic by humans, as we focus more on structure and composition than on high-level details. Consider the per-frequency observation model where denotes an orthonormal transform (e.g. discrete cosine transform (DCT)). The per-frequency SNR now becomesSo in order for the diffusion model to focus on low-frequencies in the high noise regime one needs to be very large for .\nIn experiments they find that the frequency spectrum in the RGB domain resembles that structure, whereas in modern VAEs (e.g. FluxAE) a lot more energy is in the high-frequency domain (see Figure 4).\nThey argue that downsampling removes high-frequency components and thus propose to regularize the model via where and are the downsampled versions of the input image and the latent respectively and is a distance metric ( + LPIPS). They refer to this regularization as Scale Equivariance Regularization.\n<br>Space: <a data-href=\"latent\" href=\"overview/categories/space/latent.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">latent</a>\n<br>Rep signal: <a data-tooltip-position=\"top\" aria-label=\"Overview/Categories/rep_signal/none\" data-href=\"Overview/Categories/rep_signal/none\" href=\"overview/categories/rep_signal/none.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">none</a>\n<br>Injection: <a data-tooltip-position=\"top\" aria-label=\"Overview/Categories/injection/none\" data-href=\"Overview/Categories/injection/none\" href=\"overview/categories/injection/none.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">none</a>\n<br>Training: <a data-href=\"two-stage\" href=\"overview/categories/training/two-stage.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">two-stage</a> ImageNet: FID 9.61 @ 256x256 px, w/o cfg, 400K training steps, DiT-L as diffusion backbone They don't provide rigorous results that characterize good latent spaces based on spectral decomposition They have a non-negligible amount of FLOP overhead ( of the total FLOPs for regularization (see openreview Rebuttal to Reviewer JqrQ)) and it makes training more complex with additional hyperparameters\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results","level":2,"id":"Results_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"}],"links":["overview/categories/space/latent.html","overview/categories/rep_signal/none.html","overview/categories/injection/none.html","overview/categories/training/two-stage.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","pathToRoot":"../../..","attachments":[],"createdTime":1768901150386,"modifiedTime":1769075226334,"sourceSize":3221,"sourcePath":"Overview/Papers/2025/skorokhodovImprovingDiffusabilityAutoencoders2025.md","exportPath":"overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","showInTree":true,"treeOrder":45,"backlinks":["overview/maps/directionsandquestions.html"],"type":"markdown"},"overview/papers/2025/yupixelditpixeldiffusion2025.html":{"title":"PixelDiT: Pixel Diffusion Transformers for Image Generation","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2511.20645\" target=\"_self\">http://arxiv.org/abs/2511.20645</a>\nRelated:: PixelDiT is a end-to-end pixel space diffusion transformer that splits up the low- and high-level generation task by combining a patch-level and a pixel-level DiT in one architecture.The authors argue that latent diffusion‚Äôs two-stage setup (pretrained autoencoder + diffusion in latent space) introduces a reconstruction bottleneck and can accumulate errors, especially in high-frequency details, motivating end-to-end pixel-space training.\nPixelDiT‚Äôs core design is a dual-level transformer: a patch-level DiT handles global semantics with an aggressive patch size (short token sequence), while a pixel-level DiT refines local texture via dense per-pixel token modeling.\nTwo mechanisms make the pixel pathway efficient:\n(i) pixel-wise AdaLN produces per-pixel modulation parameters from semantic tokens (so pixel updates are context-aligned), and\n(ii) pixel token compaction, which compresses pixel tokens within each patch before attention and expands them back after, making self-attention computationally feasible while keeping per-pixel updates.\nTheir training uses the Rectified Flow formulation with a velocity-matching loss, and additionally a REPA loss on mid-level patch tokens to match features from a frozen VFM (DINOv2).\n<br>Space: <a data-href=\"pixel\" href=\"overview/categories/space/pixel.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">pixel</a>\n<br>Rep signal: <a data-href=\"external-vfm\" href=\"overview/categories/rep_signal/external-vfm.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">external-vfm</a>\n<br>Injection: <a data-href=\"alignment-loss\" href=\"overview/categories/injection/alignment-loss.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">alignment-loss</a>\n<br>Training: <a data-href=\"end-to-end\" href=\"overview/categories/training/end-to-end.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">end-to-end</a> ImageNet: FID 2.36* @ 256x256 px, with cfg, 80 epochs - since batch_size = 256 corresponds to ~400k training steps, PixelDiT-XL (797M params), DINOv2-B for external representations No ablation that shows REPA vs architectural design contribution to FID\nThey only provide results with cfg, making it less comparable Conditioning / representations:\nLosses:\nArchitecture notes:\nPatch-level modulation input image gets patchified to where is the number of tokens is now projected to hidden size as , where In the DiT blocks, that follows the simple Transformer Block structure, i.e. input -&gt; skip-connection -&gt; normalization (they use RMSNorm) -&gt; Adaptive LayerNorm -&gt; multi-head attention -&gt; scaling -&gt; MLP-block also with RMSNorm, AdaLN and Gating\nThey use the outputs of the patch-pathway tokens together with timestep embedding as conditioning via for the AdaLN modulation parameters on pixel level diffusion\nPixel-Level Modulation\nthey then use pixel-level pathway in a reduced hidden dimension to make per-pixel computation efficient\nthey reshape their input to they project their semantic conditioning token to be in via a linear projection and reshaping to get new modulation parameters which are distinct at every pixel; this pixel-level preciseness allows them do just do self-attention on patch-level in the pixel-level modulation to make it computationally feasible Training notes:\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results","level":2,"id":"Results_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"},{"heading":"Details","level":1,"id":"Details_0"},{"heading":"Method sketch","level":2,"id":"Method_sketch_0"}],"links":["overview/categories/space/pixel.html","overview/categories/rep_signal/external-vfm.html","overview/categories/injection/alignment-loss.html","overview/categories/training/end-to-end.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/yupixelditpixeldiffusion2025.html","pathToRoot":"../../..","attachments":[],"createdTime":1768988319732,"modifiedTime":1769075195861,"sourceSize":4059,"sourcePath":"Overview/Papers/2025/yuPixelDiTPixelDiffusion2025.md","exportPath":"overview/papers/2025/yupixelditpixeldiffusion2025.html","showInTree":true,"treeOrder":46,"backlinks":[],"type":"markdown"},"overview/papers/2025/wangrepaworksit2025a.html":{"title":"REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2505.16792\" target=\"_self\">http://arxiv.org/abs/2505.16792</a>\nRelated:: Compared to always-on REPA feature alignment for SiT/DiT, their proposed method HASTE adds teacher attention-map distillation and turns alignment off mid-training via a stage-wise termination switch.This paper investigates why REPA improves convergence early in training but makes the diffusion training harder at later epochs. They do a gradient based analysis, where they empirically measure the cosine similarity They observe, that Based on this observation the authors argue that in the beginning the REPA helps the diffusion model, then becomes irrelevant and later conflicts with the diffusion objective. Hence, they propose to switch off the alignment loss. Two methods are considered: (i) Switching off once or simply (ii) switching off once fixed number of iterations is reached.<br>\nAdditionally, they compose their alignment loss as where is the common REPA-loss and is so called Attention alignment, recently proposed in <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://arxiv.org/pdf/2411.09702\" target=\"_self\">https://arxiv.org/pdf/2411.09702</a>.\nPrecisely, where the sum is taken over chosen layers and attention heads and is the cross entropy loss. and denote the query and key matrices of the student (DiT) and the teacher (VFM) respectively. The authors choose mid layers (e.g. 4-7) for ATTA. <br>Space: <a data-tooltip-position=\"top\" aria-label=\"Overview/Categories/space/flexible\" data-href=\"Overview/Categories/space/flexible\" href=\"overview/categories/space/flexible.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">flexible</a>\n<br>Rep signal: <a data-href=\"external-vfm\" href=\"overview/categories/rep_signal/external-vfm.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">external-vfm</a>\n<br>Injection: <a data-href=\"alignment-loss\" href=\"overview/categories/injection/alignment-loss.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">alignment-loss</a>\n<br>Training: <a data-href=\"two-stage\" href=\"overview/categories/training/two-stage.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">two-stage</a> ImageNet: FID 8.9 @ 256x256 px, w/o cfg, 400K training steps, SiT-L as diffusion backbone, DINOv2-B for external representations (see Table 9) Missing rigorous explanation of why mid layer attention alignment works best\nNo detailed ablations on the effect of the choice of the VFM on performance No theoretical details that give intuition why the gradient angle misalignment occurs, only informal explanations\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results","level":2,"id":"Results_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"}],"links":["overview/categories/space/flexible.html","overview/categories/rep_signal/external-vfm.html","overview/categories/injection/alignment-loss.html","overview/categories/training/two-stage.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/wangrepaworksit2025a.html","pathToRoot":"../../..","attachments":[],"createdTime":1768916177239,"modifiedTime":1769075213626,"sourceSize":3463,"sourcePath":"Overview/Papers/2025/wangREPAWorksIt2025a.md","exportPath":"overview/papers/2025/wangrepaworksit2025a.html","showInTree":true,"treeOrder":47,"backlinks":["overview/maps/directionsandquestions.html"],"type":"markdown"},"overview/papers/2025/lengrepaeunlockingvae2025a.html":{"title":"REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2504.10483\" target=\"_self\">http://arxiv.org/abs/2504.10483</a>\nRelated:: Building on REPA, the authors propose an end-to-end training procedure, where the VAE component is optimized during training by backpropagating the REPA loss. REPA-E aims to improve and simplify the LDM training procedure, by ‚Äì instead of having a two stage procedure ‚Äì using an end-to-end pipeline. They find that simply backpropagating the diffusion loss to the VAE is ineffective and leads to a simple latent space of the VAE (see Table 1). In this case the space is easier to denoise, but results in bad generation performance.\nTo mitigate this collapse, they use a VFM as an anchor and backpropagate the REPA loss. Their overall loss is of the form where refer to the parameters for the LDM, VAE and trainable REPA projection layer, respectively. They propose to normalize the latent space using a BatchNorm layer before passing the VAEs output to the SiT component.\nIn their standard procedure they leverage weights of a pretrained VAE, however they show that training the VAE and the Diffusion Model in an end-to-end fashion manner performs on a very similar level (see Table 8).\n<br>Space: <a data-href=\"latent\" href=\"overview/categories/space/latent.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">latent</a>\n<br>Rep signal: <a data-href=\"external-vfm\" href=\"overview/categories/rep_signal/external-vfm.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">external-vfm</a>\n<br>Injection: <a data-href=\"alignment-loss\" href=\"overview/categories/injection/alignment-loss.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">alignment-loss</a>\n<br>Training: <a data-href=\"end-to-end\" href=\"overview/categories/training/end-to-end.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">end-to-end</a> ImageNet: FID 4.07 @ 256x256 px, w/o cfg, 400K training steps, DINOv2-B if external, SiT-XL (see Figure 2b) ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results","level":2,"id":"Results_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"}],"links":["overview/categories/space/latent.html","overview/categories/rep_signal/external-vfm.html","overview/categories/injection/alignment-loss.html","overview/categories/training/end-to-end.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/lengrepaeunlockingvae2025a.html","pathToRoot":"../../..","attachments":[],"createdTime":1768828626381,"modifiedTime":1769075245293,"sourceSize":2138,"sourcePath":"Overview/Papers/2025/lengREPAEUnlockingVAE2025a.md","exportPath":"overview/papers/2025/lengrepaeunlockingvae2025a.html","showInTree":true,"treeOrder":48,"backlinks":["overview/maps/directionsandquestions.html"],"type":"markdown"},"overview/papers/2025/wurepresentationentanglementgeneration2025.html":{"title":"Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2507.01467\" target=\"_self\">http://arxiv.org/abs/2507.01467</a>\n<br>Related:: <a data-href=\"kouzelisBoostingGenerativeImage2025\" href=\"overview/papers/2025/kouzelisboostinggenerativeimage2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">kouzelisBoostingGenerativeImage2025</a>\nIn *Representation Entanglement Generation* (REG) the diffusion transformer learns to denoise both VAE latents of the image, as well as one additional semantic token from a VFM to also provide representation guidance during inference.REG argues that REPA misses out on generative gains from discriminative representation, as it is leveraged only explicitly during training. They thus propose to jointly learn VAE latents and a single addition token that is obtained through the VFM. In practice they use the CLS token of the VFM, as it yields the best empirical results compared to other approaches (e.g. learnable token, avg(latent_features), avg(DINOv2 features); see Table 5).\nDuring training they use both (i) representation alignment in REPA style, and (ii) the joint modeling.\nAt inference, they also generate the semantic token to \"actively guide and enhance the generation quality\".\n<br>Space: <a data-tooltip-position=\"top\" aria-label=\"Overview/Categories/space/flexible\" data-href=\"Overview/Categories/space/flexible\" href=\"overview/categories/space/flexible.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">flexible</a>\n<br>Rep signal: <a data-href=\"external-vfm\" href=\"overview/categories/rep_signal/external-vfm.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">external-vfm</a>\n<br>Injection: <a data-href=\"joint-modeling\" href=\"overview/categories/injection/joint-modeling.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">joint-modeling</a>\n<br>Training: <a data-href=\"two-stage\" href=\"overview/categories/training/two-stage.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">two-stage</a> ImageNet: FID 4.6 @ 256x256 px, w/o cfg, 400K training steps, SiT-L as diffusion backbone and DINOv2-B for external representations Their main motivation for the joint modeling is to have representation guidance during inference, however they don't provide any explanation of why the joint modeling gives any advantages during inference\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results","level":2,"id":"Results_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"}],"links":["overview/papers/2025/kouzelisboostinggenerativeimage2025.html","overview/categories/space/flexible.html","overview/categories/rep_signal/external-vfm.html","overview/categories/injection/joint-modeling.html","overview/categories/training/two-stage.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/wurepresentationentanglementgeneration2025.html","pathToRoot":"../../..","attachments":[],"createdTime":1768993065907,"modifiedTime":1769075203943,"sourceSize":2201,"sourcePath":"Overview/Papers/2025/wuRepresentationEntanglementGeneration2025.md","exportPath":"overview/papers/2025/wurepresentationentanglementgeneration2025.html","showInTree":true,"treeOrder":49,"backlinks":["overview/papers/2025/kouzelisboostinggenerativeimage2025.html"],"type":"markdown"},"overview/papers/2025/singhwhatmattersrepresentation2025.html":{"title":"What matters for Representation Alignment: Global Information or Spatial Structure?","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2512.10794\" target=\"_self\">http://arxiv.org/abs/2512.10794</a>\n<br>Related:: <a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2024/yuRepresentationAlignmentGeneration2025\" data-href=\"Overview/Papers/2024/yuRepresentationAlignmentGeneration2025\" href=\"overview/papers/2024/yurepresentationalignmentgeneration2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">yuRepresentationAlignmentGeneration2025</a>\nTheir method iREPA tweaks REPA by replacing the projection MLP with a conv layer and adding spatial normalization to better preserve ‚Äúspatial structure‚Äù during teacher-to-diffusion feature alignment.In experiments across 27 VFMs, they find that linear probing accuracy on the hidden representations correlates weaker () with the gFID than the spatial structure (). Linear probing accuracy is measured via ImageNet-1k accuracy, whereas spatial structure is measured via a local vs. distant self-similarity metric between patch tokens They explain that using larger VFMs for alignment as well as including the CLS token can lead to worse generation quality due to worse spatial structure.\nUsing a convolution layer for the projection of the VFM latents they focus alignment more on spatial structure. Furthermore, they\n<br>Space: <a data-tooltip-position=\"top\" aria-label=\"Overview/Categories/space/flexible\" data-href=\"Overview/Categories/space/flexible\" href=\"overview/categories/space/flexible.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">flexible</a>\n<br>Rep signal: <a data-href=\"external-vfm\" href=\"overview/categories/rep_signal/external-vfm.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">external-vfm</a>\n<br>Injection: <a data-href=\"alignment-loss\" href=\"overview/categories/injection/alignment-loss.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">alignment-loss</a>\n<br>Training: <a data-href=\"two-stage\" href=\"overview/categories/training/two-stage.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">two-stage</a> ImageNet: FID 7.52 @ 256x256 px, w/o cfg, 400K training steps, DINOv2, SiT-XL/2 (see Table 6, Appendix E) <br>Result is grounded in REPA setup, i.e. it relies on the alignment-loss to improve representation quality -&gt; Does that scale to other setups, e.g. RAE <a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/chenDiffusionAutoencodersAre2025\" data-href=\"Overview/Papers/2025/chenDiffusionAutoencodersAre2025\" href=\"overview/papers/2025/chendiffusionautoencodersare2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">chenDiffusionAutoencodersAre2025</a>? No explanation of why the spatial structure is more important At higher resolution of 512x512 px the difference in FID between REPA and iREPA seems to become smaller as training steps increase -&gt; why?\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results","level":2,"id":"Results_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"}],"links":["overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/categories/space/flexible.html","overview/categories/rep_signal/external-vfm.html","overview/categories/injection/alignment-loss.html","overview/categories/training/two-stage.html","overview/papers/2025/chendiffusionautoencodersare2025.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/singhwhatmattersrepresentation2025.html","pathToRoot":"../../..","attachments":[],"createdTime":1768578369766,"modifiedTime":1769075231582,"sourceSize":2764,"sourcePath":"Overview/Papers/2025/singhWhatMattersRepresentation2025.md","exportPath":"overview/papers/2025/singhwhatmattersrepresentation2025.html","showInTree":true,"treeOrder":50,"backlinks":["overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html"],"type":"markdown"},"overview/readme.html":{"title":"README","icon":"","description":"This repository contains a systematic (but non-exhaustive) review of recent developments in generative modeling, specifically focusing on the integration of representation learning within diffusion and flow-matching frameworks in the image domain.The goal of this overview is to understand the key design choices people have used, compare and classify work as well as get a better high level perspective on the field. There is a lot more great works that are still missing in this review and I might include them as we move forward/if we decide it might be helpful. The review is structured into two primary components:High-level tools for comparative analysis: <a data-tooltip-position=\"top\" aria-label=\"VisualOverview.canvas\" data-href=\"VisualOverview.canvas\" href=\"overview/maps/visualoverview.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">VisualOverview</a>: A mindmap showing the central idea of the work, a key figure of the paper, a link to a more in depth summary of the paper as well as FID score and simple categorization. üìçStart here, go to summaries from there\n<br><a data-href=\"DirectionsAndQuestions\" href=\"overview/maps/directionsandquestions.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">DirectionsAndQuestions</a>: Current developments as well as a list of potential theoretical and empirical questions one could explore. I tried to move away from concrete engineering to a more abstract maths perspective here. ‚è≠Ô∏è Continue here -&gt; feedback &amp; ideas highly appreciated\n<br><a data-href=\"All-Papers\" href=\"overview/maps/all-papers.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">All-Papers</a>: A table of all reviewed works. Individual paper analyses organized by publication year (though most were published in 2025) Content Structure: Each entry follows the same structure: One-liner conveys the key idea/improvement proposed in the paper Brief summary goes more in depth and gives more technical details.\nKey design choices categorizes the paper into: <br>Space: Domain in which the model operates (<a data-href=\"pixel\" href=\"overview/categories/space/pixel.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">pixel</a>| <a data-href=\"latent\" href=\"overview/categories/space/latent.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">latent</a> | <a data-tooltip-position=\"top\" aria-label=\"Overview/Categories/space/flexible\" data-href=\"Overview/Categories/space/flexible\" href=\"overview/categories/space/flexible.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">flexible</a>)\n<br>Rep Signal: Signal that is used to guide the model (<a data-href=\"external-vfm\" href=\"overview/categories/rep_signal/external-vfm.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">external-vfm</a> | <a data-href=\"internal\" href=\"overview/categories/rep_signal/internal.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">internal</a> | <a data-tooltip-position=\"top\" aria-label=\"Overview/Categories/rep_signal/none\" data-href=\"Overview/Categories/rep_signal/none\" href=\"overview/categories/rep_signal/none.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">none</a>)\n<br>Injection: How external signal is used for the model (<a data-href=\"alignment-loss\" href=\"overview/categories/injection/alignment-loss.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">alignment-loss</a> | <a data-href=\"joint-modeling\" href=\"overview/categories/injection/joint-modeling.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">joint-modeling</a> | <a data-href=\"tokenizer\" href=\"overview/categories/injection/tokenizer.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">tokenizer</a> | <a data-tooltip-position=\"top\" aria-label=\"Overview/Categories/injection/none\" data-href=\"Overview/Categories/injection/none\" href=\"overview/categories/injection/none.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">none</a>)\n<br>Training: training-regime (<a data-href=\"end-to-end\" href=\"overview/categories/training/end-to-end.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">end-to-end</a> | <a data-href=\"two-stage\" href=\"overview/categories/training/two-stage.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">two-stage</a>) Results FID score and training setting\nif not marked via \"*\" the model was trained on 400k iterations, batchsize 256, on pixels, using SiT-L as backbone, no cfg and DINOv2-B representations if any are used; while I tried to make it as comparable as possible, still some unaligned hyperparameters remain, so\nFID score is not very meaningful Limitations/open questions give an overview over the methods limitations and open questions, sometimes in context with other works Here, one finds the categories mentioned in key design choices. Each note contains backlinks to the papers that use that design choice.<br>\nSo for example, if you're interested in papers that work in pixel-space, go to <a data-href=\"pixel\" href=\"overview/categories/space/pixel.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">pixel</a> and browse through the paper summaries that are backlinked there.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Literature Review: Representation Diffusion (2024 -2025)","level":1,"id":"Literature_Review_Representation_Diffusion_(2024_-2025)_0"},{"heading":"Organization of the Review","level":2,"id":"Organization_of_the_Review_0"},{"heading":"1. Synthesis and Visualization (Overview/Maps)","level":3,"id":"1._Synthesis_and_Visualization_(`Overview/Maps`)_0"},{"heading":"2. Literature Summaries (Overview/Papers)","level":3,"id":"2._Literature_Summaries_(`Overview/Papers`)_0"},{"heading":"Categories (<code>Overview/Categories</code>)","level":4,"id":"Categories_(`Overview/Categories`)_0"}],"links":["overview/maps/visualoverview.html","overview/maps/directionsandquestions.html","overview/maps/all-papers.html","overview/categories/space/pixel.html","overview/categories/space/latent.html","overview/categories/space/flexible.html","overview/categories/rep_signal/external-vfm.html","overview/categories/rep_signal/internal.html","overview/categories/rep_signal/none.html","overview/categories/injection/alignment-loss.html","overview/categories/injection/joint-modeling.html","overview/categories/injection/tokenizer.html","overview/categories/injection/none.html","overview/categories/training/end-to-end.html","overview/categories/training/two-stage.html","overview/categories/space/pixel.html"],"author":"","coverImageURL":"","fullURL":"overview/readme.html","pathToRoot":"..","attachments":[],"createdTime":1768575086055,"modifiedTime":1769100133313,"sourceSize":3183,"sourcePath":"Overview/README.md","exportPath":"overview/readme.html","showInTree":true,"treeOrder":51,"backlinks":[],"type":"markdown"}},"fileInfo":{"overview/categories/injection/alignment-loss.html":{"createdTime":1768817841833,"modifiedTime":1768817841833,"sourceSize":0,"sourcePath":"Overview/Categories/injection/alignment-loss.md","exportPath":"overview/categories/injection/alignment-loss.html","showInTree":true,"treeOrder":3,"backlinks":["overview/readme.html","overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/wangrepaworksit2025a.html","overview/papers/2025/yupixelditpixeldiffusion2025.html"],"type":"markdown","data":null},"overview/categories/injection/joint-modeling.html":{"createdTime":1768817848608,"modifiedTime":1768817848608,"sourceSize":0,"sourcePath":"Overview/Categories/injection/joint-modeling.md","exportPath":"overview/categories/injection/joint-modeling.html","showInTree":true,"treeOrder":4,"backlinks":["overview/readme.html","overview/papers/2025/wurepresentationentanglementgeneration2025.html"],"type":"markdown","data":null},"overview/categories/injection/none.html":{"createdTime":1768839687558,"modifiedTime":1768839687558,"sourceSize":0,"sourcePath":"Overview/Categories/injection/none.md","exportPath":"overview/categories/injection/none.html","showInTree":true,"treeOrder":5,"backlinks":["overview/readme.html","overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","overview/papers/2025/wangdiffusedisperseimage2025.html","overview/papers/2025/libackbasicslet2025.html"],"type":"markdown","data":null},"overview/categories/injection/tokenizer.html":{"createdTime":1768817854941,"modifiedTime":1768817854941,"sourceSize":0,"sourcePath":"Overview/Categories/injection/tokenizer.md","exportPath":"overview/categories/injection/tokenizer.html","showInTree":true,"treeOrder":6,"backlinks":["overview/readme.html","overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html"],"type":"markdown","data":null},"overview/categories/rep_signal/external-vfm.html":{"createdTime":1768817759883,"modifiedTime":1768817759883,"sourceSize":0,"sourcePath":"Overview/Categories/rep_signal/external-vfm.md","exportPath":"overview/categories/rep_signal/external-vfm.html","showInTree":true,"treeOrder":8,"backlinks":["overview/readme.html","overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html","overview/papers/2025/wangrepaworksit2025a.html","overview/papers/2025/yupixelditpixeldiffusion2025.html","overview/papers/2025/wurepresentationentanglementgeneration2025.html"],"type":"markdown","data":null},"overview/categories/rep_signal/internal.html":{"createdTime":1768817808699,"modifiedTime":1768817808699,"sourceSize":0,"sourcePath":"Overview/Categories/rep_signal/internal.md","exportPath":"overview/categories/rep_signal/internal.html","showInTree":true,"treeOrder":9,"backlinks":["overview/readme.html","overview/papers/2025/wangdiffusedisperseimage2025.html"],"type":"markdown","data":null},"overview/categories/rep_signal/none.html":{"createdTime":1768817821509,"modifiedTime":1768817821509,"sourceSize":0,"sourcePath":"Overview/Categories/rep_signal/none.md","exportPath":"overview/categories/rep_signal/none.html","showInTree":true,"treeOrder":10,"backlinks":["overview/readme.html","overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","overview/papers/2025/libackbasicslet2025.html"],"type":"markdown","data":null},"overview/categories/space/flexible.html":{"createdTime":1768817872627,"modifiedTime":1768817872627,"sourceSize":0,"sourcePath":"Overview/Categories/space/flexible.md","exportPath":"overview/categories/space/flexible.html","showInTree":true,"treeOrder":12,"backlinks":["overview/readme.html","overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html","overview/papers/2025/wangrepaworksit2025a.html","overview/papers/2025/wangdiffusedisperseimage2025.html","overview/papers/2025/wurepresentationentanglementgeneration2025.html"],"type":"markdown","data":null},"overview/categories/space/latent.html":{"createdTime":1768577924932,"modifiedTime":1768577924932,"sourceSize":0,"sourcePath":"Overview/Categories/space/latent.md","exportPath":"overview/categories/space/latent.html","showInTree":true,"treeOrder":13,"backlinks":["overview/readme.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html"],"type":"markdown","data":null},"overview/categories/space/pixel.html":{"createdTime":1768577936836,"modifiedTime":1768577936836,"sourceSize":0,"sourcePath":"Overview/Categories/space/pixel.md","exportPath":"overview/categories/space/pixel.html","showInTree":true,"treeOrder":14,"backlinks":["overview/readme.html","overview/papers/2025/libackbasicslet2025.html","overview/papers/2025/yupixelditpixeldiffusion2025.html"],"type":"markdown","data":null},"overview/categories/training/end-to-end.html":{"createdTime":1768818908083,"modifiedTime":1768818908083,"sourceSize":0,"sourcePath":"Overview/Categories/training/end-to-end.md","exportPath":"overview/categories/training/end-to-end.html","showInTree":true,"treeOrder":16,"backlinks":["overview/readme.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/wangdiffusedisperseimage2025.html","overview/papers/2025/libackbasicslet2025.html","overview/papers/2025/yupixelditpixeldiffusion2025.html"],"type":"markdown","data":null},"overview/categories/training/flexible.html":{"createdTime":1769004290137,"modifiedTime":1769004290137,"sourceSize":0,"sourcePath":"Overview/Categories/training/flexible.md","exportPath":"overview/categories/training/flexible.html","showInTree":true,"treeOrder":17,"backlinks":[],"type":"markdown","data":null},"overview/categories/training/two-stage.html":{"createdTime":1768818913606,"modifiedTime":1768818913606,"sourceSize":0,"sourcePath":"Overview/Categories/training/two-stage.md","exportPath":"overview/categories/training/two-stage.html","showInTree":true,"treeOrder":18,"backlinks":["overview/readme.html","overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html","overview/papers/2025/wangrepaworksit2025a.html","overview/papers/2025/wurepresentationentanglementgeneration2025.html"],"type":"markdown","data":null},"overview/data/bildschirmfoto-2026-01-21-um-15.59.33.png":{"createdTime":1769007576952,"modifiedTime":1769007576953,"sourceSize":510254,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 15.59.33.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-15.59.33.png","showInTree":true,"treeOrder":0,"backlinks":[],"type":"media","data":null},"overview/data/bildschirmfoto-2026-01-21-um-15.59.33.html":{"createdTime":1769007576952,"modifiedTime":1769007576953,"sourceSize":510254,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 15.59.33.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-15.59.33.html","showInTree":true,"treeOrder":20,"backlinks":[],"type":"attachment","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.09.13.png":{"createdTime":1769008157392,"modifiedTime":1769008157393,"sourceSize":182049,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.09.13.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.09.13.png","showInTree":true,"treeOrder":0,"backlinks":[],"type":"media","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.09.13.html":{"createdTime":1769008157392,"modifiedTime":1769008157393,"sourceSize":182049,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.09.13.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.09.13.html","showInTree":true,"treeOrder":21,"backlinks":[],"type":"attachment","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.13.27.png":{"createdTime":1769008410327,"modifiedTime":1769008410330,"sourceSize":51226,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.13.27.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.13.27.png","showInTree":true,"treeOrder":0,"backlinks":[],"type":"media","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.13.27.html":{"createdTime":1769008410327,"modifiedTime":1769008410330,"sourceSize":51226,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.13.27.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.13.27.html","showInTree":true,"treeOrder":22,"backlinks":[],"type":"attachment","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.19.47.png":{"createdTime":1769008792834,"modifiedTime":1769008792835,"sourceSize":72150,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.19.47.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.19.47.png","showInTree":true,"treeOrder":0,"backlinks":[],"type":"media","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.19.47.html":{"createdTime":1769008792834,"modifiedTime":1769008792835,"sourceSize":72150,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.19.47.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.19.47.html","showInTree":true,"treeOrder":23,"backlinks":[],"type":"attachment","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.25.46.png":{"createdTime":1769009151595,"modifiedTime":1769009151598,"sourceSize":58217,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.25.46.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.25.46.png","showInTree":true,"treeOrder":0,"backlinks":[],"type":"media","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.25.46.html":{"createdTime":1769009151595,"modifiedTime":1769009151598,"sourceSize":58217,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.25.46.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.25.46.html","showInTree":true,"treeOrder":24,"backlinks":[],"type":"attachment","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.31.07.png":{"createdTime":1769009473904,"modifiedTime":1769009473904,"sourceSize":71462,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.31.07.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.31.07.png","showInTree":true,"treeOrder":0,"backlinks":[],"type":"media","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.31.07.html":{"createdTime":1769009473904,"modifiedTime":1769009473904,"sourceSize":71462,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.31.07.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.31.07.html","showInTree":true,"treeOrder":25,"backlinks":[],"type":"attachment","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.35.48.png":{"createdTime":1769009752892,"modifiedTime":1769009752893,"sourceSize":118668,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.35.48.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.35.48.png","showInTree":true,"treeOrder":0,"backlinks":[],"type":"media","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.35.48.html":{"createdTime":1769009752892,"modifiedTime":1769009752893,"sourceSize":118668,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.35.48.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.35.48.html","showInTree":true,"treeOrder":26,"backlinks":[],"type":"attachment","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.41.45.png":{"createdTime":1769010108424,"modifiedTime":1769010108424,"sourceSize":191958,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.41.45.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.41.45.png","showInTree":true,"treeOrder":0,"backlinks":[],"type":"media","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.41.45.html":{"createdTime":1769010108424,"modifiedTime":1769010108424,"sourceSize":191958,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.41.45.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.41.45.html","showInTree":true,"treeOrder":27,"backlinks":[],"type":"attachment","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.45.44.png":{"createdTime":1769010348759,"modifiedTime":1769010348763,"sourceSize":287460,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.45.44.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.45.44.png","showInTree":true,"treeOrder":0,"backlinks":[],"type":"media","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.45.44.html":{"createdTime":1769010348759,"modifiedTime":1769010348763,"sourceSize":287460,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.45.44.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.45.44.html","showInTree":true,"treeOrder":28,"backlinks":[],"type":"attachment","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.50.44.png":{"createdTime":1769010647407,"modifiedTime":1769010647407,"sourceSize":24009,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.50.44.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.50.44.png","showInTree":true,"treeOrder":0,"backlinks":[],"type":"media","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.50.44.html":{"createdTime":1769010647407,"modifiedTime":1769010647407,"sourceSize":24009,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.50.44.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.50.44.html","showInTree":true,"treeOrder":29,"backlinks":[],"type":"attachment","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.54.26.png":{"createdTime":1769010869864,"modifiedTime":1769010869864,"sourceSize":469338,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.54.26.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.54.26.png","showInTree":true,"treeOrder":0,"backlinks":[],"type":"media","data":null},"overview/data/bildschirmfoto-2026-01-21-um-16.54.26.html":{"createdTime":1769010869864,"modifiedTime":1769010869864,"sourceSize":469338,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 16.54.26.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-16.54.26.html","showInTree":true,"treeOrder":30,"backlinks":[],"type":"attachment","data":null},"overview/data/bildschirmfoto-2026-01-21-um-17.04.23.png":{"createdTime":1769011465911,"modifiedTime":1769011465912,"sourceSize":256193,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 17.04.23.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-17.04.23.png","showInTree":true,"treeOrder":0,"backlinks":[],"type":"media","data":null},"overview/data/bildschirmfoto-2026-01-21-um-17.04.23.html":{"createdTime":1769011465911,"modifiedTime":1769011465912,"sourceSize":256193,"sourcePath":"Overview/Data/Bildschirmfoto 2026-01-21 um 17.04.23.png","exportPath":"overview/data/bildschirmfoto-2026-01-21-um-17.04.23.html","showInTree":true,"treeOrder":31,"backlinks":[],"type":"attachment","data":null},"overview/maps/all-papers.html":{"createdTime":1768577986054,"modifiedTime":1769091767311,"sourceSize":451,"sourcePath":"Overview/Maps/All-Papers.md","exportPath":"overview/maps/all-papers.html","showInTree":true,"treeOrder":33,"backlinks":["overview/readme.html"],"type":"markdown","data":null},"overview/maps/directionsandquestions.html":{"createdTime":1769003617263,"modifiedTime":1769100098513,"sourceSize":4651,"sourcePath":"Overview/Maps/DirectionsAndQuestions.md","exportPath":"overview/maps/directionsandquestions.html","showInTree":true,"treeOrder":34,"backlinks":["overview/readme.html"],"type":"markdown","data":null},"overview/maps/visualoverview.html":{"createdTime":1769002725757,"modifiedTime":1769074479155,"sourceSize":10534,"sourcePath":"Overview/Maps/VisualOverview.canvas","exportPath":"overview/maps/visualoverview.html","showInTree":true,"treeOrder":35,"backlinks":["overview/readme.html"],"type":"canvas","data":null},"overview/papers/2024/yurepresentationalignmentgeneration2025.html":{"createdTime":1768820192430,"modifiedTime":1769075125359,"sourceSize":2778,"sourcePath":"Overview/Papers/2024/yuRepresentationAlignmentGeneration2025.md","exportPath":"overview/papers/2024/yurepresentationalignmentgeneration2025.html","showInTree":true,"treeOrder":38,"backlinks":["overview/papers/2025/singhwhatmattersrepresentation2025.html"],"type":"markdown","data":null},"overview/papers/2025/libackbasicslet2025.html":{"createdTime":1768927936821,"modifiedTime":1769075240846,"sourceSize":2778,"sourcePath":"Overview/Papers/2025/liBackBasicsLet2025.md","exportPath":"overview/papers/2025/libackbasicslet2025.html","showInTree":true,"treeOrder":40,"backlinks":[],"type":"markdown","data":null},"overview/papers/2025/kouzelisboostinggenerativeimage2025.html":{"createdTime":1768908441822,"modifiedTime":1769075252694,"sourceSize":2712,"sourcePath":"Overview/Papers/2025/kouzelisBoostingGenerativeImage2025.md","exportPath":"overview/papers/2025/kouzelisboostinggenerativeimage2025.html","showInTree":true,"treeOrder":41,"backlinks":["overview/papers/2025/wurepresentationentanglementgeneration2025.html"],"type":"markdown","data":null},"overview/papers/2025/wangdiffusedisperseimage2025.html":{"createdTime":1768923759178,"modifiedTime":1769075156037,"sourceSize":2365,"sourcePath":"Overview/Papers/2025/wangDiffuseDisperseImage2025.md","exportPath":"overview/papers/2025/wangdiffusedisperseimage2025.html","showInTree":true,"treeOrder":42,"backlinks":[],"type":"markdown","data":null},"overview/papers/2025/chendiffusionautoencodersare2025.html":{"createdTime":1768838020230,"modifiedTime":1769075264844,"sourceSize":1937,"sourcePath":"Overview/Papers/2025/chenDiffusionAutoencodersAre2025.md","exportPath":"overview/papers/2025/chendiffusionautoencodersare2025.html","showInTree":true,"treeOrder":43,"backlinks":["overview/papers/2025/singhwhatmattersrepresentation2025.html"],"type":"markdown","data":null},"overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html":{"createdTime":1768840091876,"modifiedTime":1769075186930,"sourceSize":3679,"sourcePath":"Overview/Papers/2025/zhengDiffusionTransformersRepresentation2025.md","exportPath":"overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","showInTree":true,"treeOrder":44,"backlinks":["overview/maps/directionsandquestions.html"],"type":"markdown","data":null},"overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html":{"createdTime":1768901150386,"modifiedTime":1769075226334,"sourceSize":3221,"sourcePath":"Overview/Papers/2025/skorokhodovImprovingDiffusabilityAutoencoders2025.md","exportPath":"overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","showInTree":true,"treeOrder":45,"backlinks":["overview/maps/directionsandquestions.html"],"type":"markdown","data":null},"overview/papers/2025/yupixelditpixeldiffusion2025.html":{"createdTime":1768988319732,"modifiedTime":1769075195861,"sourceSize":4059,"sourcePath":"Overview/Papers/2025/yuPixelDiTPixelDiffusion2025.md","exportPath":"overview/papers/2025/yupixelditpixeldiffusion2025.html","showInTree":true,"treeOrder":46,"backlinks":[],"type":"markdown","data":null},"overview/papers/2025/wangrepaworksit2025a.html":{"createdTime":1768916177239,"modifiedTime":1769075213626,"sourceSize":3463,"sourcePath":"Overview/Papers/2025/wangREPAWorksIt2025a.md","exportPath":"overview/papers/2025/wangrepaworksit2025a.html","showInTree":true,"treeOrder":47,"backlinks":["overview/maps/directionsandquestions.html"],"type":"markdown","data":null},"overview/papers/2025/lengrepaeunlockingvae2025a.html":{"createdTime":1768828626381,"modifiedTime":1769075245293,"sourceSize":2138,"sourcePath":"Overview/Papers/2025/lengREPAEUnlockingVAE2025a.md","exportPath":"overview/papers/2025/lengrepaeunlockingvae2025a.html","showInTree":true,"treeOrder":48,"backlinks":["overview/maps/directionsandquestions.html"],"type":"markdown","data":null},"overview/papers/2025/wurepresentationentanglementgeneration2025.html":{"createdTime":1768993065907,"modifiedTime":1769075203943,"sourceSize":2201,"sourcePath":"Overview/Papers/2025/wuRepresentationEntanglementGeneration2025.md","exportPath":"overview/papers/2025/wurepresentationentanglementgeneration2025.html","showInTree":true,"treeOrder":49,"backlinks":["overview/papers/2025/kouzelisboostinggenerativeimage2025.html"],"type":"markdown","data":null},"overview/papers/2025/singhwhatmattersrepresentation2025.html":{"createdTime":1768578369766,"modifiedTime":1769075231582,"sourceSize":2764,"sourcePath":"Overview/Papers/2025/singhWhatMattersRepresentation2025.md","exportPath":"overview/papers/2025/singhwhatmattersrepresentation2025.html","showInTree":true,"treeOrder":50,"backlinks":["overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html"],"type":"markdown","data":null},"overview/readme.html":{"createdTime":1768575086055,"modifiedTime":1769100133313,"sourceSize":3183,"sourcePath":"Overview/README.md","exportPath":"overview/readme.html","showInTree":true,"treeOrder":51,"backlinks":[],"type":"markdown","data":null},"site-lib/scripts/graph-wasm.wasm":{"createdTime":1768575105303,"modifiedTime":1768575009474.6726,"sourceSize":23655,"sourcePath":"","exportPath":"site-lib/scripts/graph-wasm.wasm","showInTree":false,"treeOrder":0,"backlinks":[],"type":"other","data":null},"site-lib/fonts/94f2f163d4b698242fef.otf":{"createdTime":1769100154964,"modifiedTime":1769100154964,"sourceSize":66800,"sourcePath":"","exportPath":"site-lib/fonts/94f2f163d4b698242fef.otf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/72505e6a122c6acd5471.woff2":{"createdTime":1769100154965,"modifiedTime":1769100154965,"sourceSize":104232,"sourcePath":"","exportPath":"site-lib/fonts/72505e6a122c6acd5471.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/2d5198822ab091ce4305.woff2":{"createdTime":1769100154967,"modifiedTime":1769100154967,"sourceSize":104332,"sourcePath":"","exportPath":"site-lib/fonts/2d5198822ab091ce4305.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/c8ba52b05a9ef10f4758.woff2":{"createdTime":1769100154965,"modifiedTime":1769100154965,"sourceSize":98868,"sourcePath":"","exportPath":"site-lib/fonts/c8ba52b05a9ef10f4758.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/cb10ffd7684cd9836a05.woff2":{"createdTime":1769100154966,"modifiedTime":1769100154966,"sourceSize":106876,"sourcePath":"","exportPath":"site-lib/fonts/cb10ffd7684cd9836a05.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/293fd13dbca5a3e450ef.woff2":{"createdTime":1769100154965,"modifiedTime":1769100154965,"sourceSize":105924,"sourcePath":"","exportPath":"site-lib/fonts/293fd13dbca5a3e450ef.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/085cb93e613ba3d40d2b.woff2":{"createdTime":1769100154966,"modifiedTime":1769100154966,"sourceSize":112184,"sourcePath":"","exportPath":"site-lib/fonts/085cb93e613ba3d40d2b.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/b5f0f109bc88052d4000.woff2":{"createdTime":1769100154965,"modifiedTime":1769100154965,"sourceSize":105804,"sourcePath":"","exportPath":"site-lib/fonts/b5f0f109bc88052d4000.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/cbe0ae49c52c920fd563.woff2":{"createdTime":1769100154966,"modifiedTime":1769100154966,"sourceSize":106108,"sourcePath":"","exportPath":"site-lib/fonts/cbe0ae49c52c920fd563.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/535a6cf662596b3bd6a6.woff2":{"createdTime":1769100154966,"modifiedTime":1769100154966,"sourceSize":111708,"sourcePath":"","exportPath":"site-lib/fonts/535a6cf662596b3bd6a6.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/70cc7ff27245e82ad414.ttf":{"createdTime":1769100154967,"modifiedTime":1769100154967,"sourceSize":192740,"sourcePath":"","exportPath":"site-lib/fonts/70cc7ff27245e82ad414.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/454577c22304619db035.ttf":{"createdTime":1769100154967,"modifiedTime":1769100154967,"sourceSize":161376,"sourcePath":"","exportPath":"site-lib/fonts/454577c22304619db035.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/52ac8f3034507f1d9e53.ttf":{"createdTime":1769100154967,"modifiedTime":1769100154967,"sourceSize":191568,"sourcePath":"","exportPath":"site-lib/fonts/52ac8f3034507f1d9e53.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/05b618077343fbbd92b7.ttf":{"createdTime":1769100154967,"modifiedTime":1769100154967,"sourceSize":155288,"sourcePath":"","exportPath":"site-lib/fonts/05b618077343fbbd92b7.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/4bb6ac751d1c5478ff3a.woff2":{"createdTime":1769100154964,"modifiedTime":1769100154964,"sourceSize":7876,"sourcePath":"","exportPath":"site-lib/fonts/4bb6ac751d1c5478ff3a.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/media/6155340132a851f6089e.svg":{"createdTime":1769100154964,"modifiedTime":1769100154964,"sourceSize":315,"sourcePath":"","exportPath":"site-lib/media/6155340132a851f6089e.svg","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"site-lib/media/2308ab1944a6bfa5c5b8.svg":{"createdTime":1769100154964,"modifiedTime":1769100154964,"sourceSize":278,"sourcePath":"","exportPath":"site-lib/media/2308ab1944a6bfa5c5b8.svg","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"site-lib/fonts/mathjax_zero.woff":{"createdTime":1769099497161,"modifiedTime":1769099497161,"sourceSize":1368,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_zero.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_main-regular.woff":{"createdTime":1769099497161,"modifiedTime":1769099497161,"sourceSize":34160,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_main-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_main-bold.woff":{"createdTime":1769099497161,"modifiedTime":1769099497161,"sourceSize":34464,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_main-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_math-italic.woff":{"createdTime":1769099497161,"modifiedTime":1769099497161,"sourceSize":19360,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_math-italic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_main-italic.woff":{"createdTime":1769099497161,"modifiedTime":1769099497161,"sourceSize":20832,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_main-italic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_math-bolditalic.woff":{"createdTime":1769099497161,"modifiedTime":1769099497161,"sourceSize":19776,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_math-bolditalic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size1-regular.woff":{"createdTime":1769099497161,"modifiedTime":1769099497161,"sourceSize":5792,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size1-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size2-regular.woff":{"createdTime":1769099497161,"modifiedTime":1769099497161,"sourceSize":5464,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size2-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size3-regular.woff":{"createdTime":1769099497161,"modifiedTime":1769099497161,"sourceSize":3244,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size3-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size4-regular.woff":{"createdTime":1769099497162,"modifiedTime":1769099497162,"sourceSize":5148,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size4-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_ams-regular.woff":{"createdTime":1769099497162,"modifiedTime":1769099497162,"sourceSize":40808,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_ams-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_calligraphic-regular.woff":{"createdTime":1769099497162,"modifiedTime":1769099497162,"sourceSize":9600,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_calligraphic-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_calligraphic-bold.woff":{"createdTime":1769099497162,"modifiedTime":1769099497162,"sourceSize":9908,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_calligraphic-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_fraktur-regular.woff":{"createdTime":1769099497162,"modifiedTime":1769099497162,"sourceSize":21480,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_fraktur-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_fraktur-bold.woff":{"createdTime":1769099497163,"modifiedTime":1769099497163,"sourceSize":22340,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_fraktur-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_sansserif-regular.woff":{"createdTime":1769099497163,"modifiedTime":1769099497163,"sourceSize":12660,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_sansserif-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_sansserif-bold.woff":{"createdTime":1769099497163,"modifiedTime":1769099497163,"sourceSize":15944,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_sansserif-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_sansserif-italic.woff":{"createdTime":1769099497163,"modifiedTime":1769099497163,"sourceSize":14628,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_sansserif-italic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_script-regular.woff":{"createdTime":1769099497163,"modifiedTime":1769099497163,"sourceSize":11852,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_script-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_typewriter-regular.woff":{"createdTime":1769099497163,"modifiedTime":1769099497163,"sourceSize":17604,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_typewriter-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_vector-regular.woff":{"createdTime":1769099497163,"modifiedTime":1769099497163,"sourceSize":1136,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_vector-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_vector-bold.woff":{"createdTime":1769099497163,"modifiedTime":1769099497163,"sourceSize":1116,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_vector-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/html/file-tree-content.html":{"createdTime":1769100155164,"modifiedTime":1769100155164,"sourceSize":24603,"sourcePath":"","exportPath":"site-lib/html/file-tree-content.html","showInTree":false,"treeOrder":0,"backlinks":[],"type":"html","data":null},"site-lib/scripts/webpage.js":{"createdTime":1768575105527,"modifiedTime":1768575105527,"sourceSize":110729,"sourcePath":"","exportPath":"site-lib/scripts/webpage.js","showInTree":false,"treeOrder":0,"backlinks":[],"type":"script","data":null},"site-lib/scripts/graph-wasm.js":{"createdTime":1768575105527,"modifiedTime":1768575105528,"sourceSize":12885,"sourcePath":"","exportPath":"site-lib/scripts/graph-wasm.js","showInTree":false,"treeOrder":0,"backlinks":[],"type":"script","data":null},"site-lib/scripts/graph-render-worker.js":{"createdTime":1768575105528,"modifiedTime":1768575105528,"sourceSize":5681,"sourcePath":"","exportPath":"site-lib/scripts/graph-render-worker.js","showInTree":false,"treeOrder":0,"backlinks":[],"type":"script","data":null},"site-lib/media/favicon.png":{"createdTime":1769100154926,"modifiedTime":1769100154926,"sourceSize":1105,"sourcePath":"","exportPath":"site-lib/media/favicon.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"site-lib/styles/obsidian.css":{"createdTime":1769100154987,"modifiedTime":1769100154987,"sourceSize":206058,"sourcePath":"","exportPath":"site-lib/styles/obsidian.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/styles/other-plugins.css":{"createdTime":1769012699228,"modifiedTime":1769012699228,"sourceSize":1947,"sourcePath":"","exportPath":"site-lib/styles/other-plugins.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/styles/global-variable-styles.css":{"createdTime":1769100154955,"modifiedTime":1769100154955,"sourceSize":322,"sourcePath":"","exportPath":"site-lib/styles/global-variable-styles.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/styles/main-styles.css":{"createdTime":1768575105539,"modifiedTime":1768575105539,"sourceSize":19521,"sourcePath":"","exportPath":"site-lib/styles/main-styles.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/rss.xml":{"createdTime":1769100163500,"modifiedTime":1769100163500,"sourceSize":67484,"sourcePath":"","exportPath":"site-lib/rss.xml","showInTree":false,"treeOrder":0,"backlinks":[],"type":"other","data":null}},"sourceToTarget":{"Overview/Categories/injection/alignment-loss.md":"overview/categories/injection/alignment-loss.html","Overview/Categories/injection/joint-modeling.md":"overview/categories/injection/joint-modeling.html","Overview/Categories/injection/none.md":"overview/categories/injection/none.html","Overview/Categories/injection/tokenizer.md":"overview/categories/injection/tokenizer.html","Overview/Categories/rep_signal/external-vfm.md":"overview/categories/rep_signal/external-vfm.html","Overview/Categories/rep_signal/internal.md":"overview/categories/rep_signal/internal.html","Overview/Categories/rep_signal/none.md":"overview/categories/rep_signal/none.html","Overview/Categories/space/flexible.md":"overview/categories/space/flexible.html","Overview/Categories/space/latent.md":"overview/categories/space/latent.html","Overview/Categories/space/pixel.md":"overview/categories/space/pixel.html","Overview/Categories/training/end-to-end.md":"overview/categories/training/end-to-end.html","Overview/Categories/training/flexible.md":"overview/categories/training/flexible.html","Overview/Categories/training/two-stage.md":"overview/categories/training/two-stage.html","Overview/Data/Bildschirmfoto 2026-01-21 um 15.59.33.png":"overview/data/bildschirmfoto-2026-01-21-um-15.59.33.html","Overview/Data/Bildschirmfoto 2026-01-21 um 16.09.13.png":"overview/data/bildschirmfoto-2026-01-21-um-16.09.13.html","Overview/Data/Bildschirmfoto 2026-01-21 um 16.13.27.png":"overview/data/bildschirmfoto-2026-01-21-um-16.13.27.html","Overview/Data/Bildschirmfoto 2026-01-21 um 16.19.47.png":"overview/data/bildschirmfoto-2026-01-21-um-16.19.47.html","Overview/Data/Bildschirmfoto 2026-01-21 um 16.25.46.png":"overview/data/bildschirmfoto-2026-01-21-um-16.25.46.html","Overview/Data/Bildschirmfoto 2026-01-21 um 16.31.07.png":"overview/data/bildschirmfoto-2026-01-21-um-16.31.07.html","Overview/Data/Bildschirmfoto 2026-01-21 um 16.35.48.png":"overview/data/bildschirmfoto-2026-01-21-um-16.35.48.html","Overview/Data/Bildschirmfoto 2026-01-21 um 16.41.45.png":"overview/data/bildschirmfoto-2026-01-21-um-16.41.45.html","Overview/Data/Bildschirmfoto 2026-01-21 um 16.45.44.png":"overview/data/bildschirmfoto-2026-01-21-um-16.45.44.html","Overview/Data/Bildschirmfoto 2026-01-21 um 16.50.44.png":"overview/data/bildschirmfoto-2026-01-21-um-16.50.44.html","Overview/Data/Bildschirmfoto 2026-01-21 um 16.54.26.png":"overview/data/bildschirmfoto-2026-01-21-um-16.54.26.html","Overview/Data/Bildschirmfoto 2026-01-21 um 17.04.23.png":"overview/data/bildschirmfoto-2026-01-21-um-17.04.23.html","Overview/Maps/All-Papers.md":"overview/maps/all-papers.html","Overview/Maps/DirectionsAndQuestions.md":"overview/maps/directionsandquestions.html","Overview/Maps/VisualOverview.canvas":"overview/maps/visualoverview.html","Overview/Papers/2024/yuRepresentationAlignmentGeneration2025.md":"overview/papers/2024/yurepresentationalignmentgeneration2025.html","Overview/Papers/2025/liBackBasicsLet2025.md":"overview/papers/2025/libackbasicslet2025.html","Overview/Papers/2025/kouzelisBoostingGenerativeImage2025.md":"overview/papers/2025/kouzelisboostinggenerativeimage2025.html","Overview/Papers/2025/wangDiffuseDisperseImage2025.md":"overview/papers/2025/wangdiffusedisperseimage2025.html","Overview/Papers/2025/chenDiffusionAutoencodersAre2025.md":"overview/papers/2025/chendiffusionautoencodersare2025.html","Overview/Papers/2025/zhengDiffusionTransformersRepresentation2025.md":"overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","Overview/Papers/2025/skorokhodovImprovingDiffusabilityAutoencoders2025.md":"overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","Overview/Papers/2025/yuPixelDiTPixelDiffusion2025.md":"overview/papers/2025/yupixelditpixeldiffusion2025.html","Overview/Papers/2025/wangREPAWorksIt2025a.md":"overview/papers/2025/wangrepaworksit2025a.html","Overview/Papers/2025/lengREPAEUnlockingVAE2025a.md":"overview/papers/2025/lengrepaeunlockingvae2025a.html","Overview/Papers/2025/wuRepresentationEntanglementGeneration2025.md":"overview/papers/2025/wurepresentationentanglementgeneration2025.html","Overview/Papers/2025/singhWhatMattersRepresentation2025.md":"overview/papers/2025/singhwhatmattersrepresentation2025.html","Overview/README.md":"overview/readme.html","":"site-lib/rss.xml"},"featureOptions":{"backlinks":{"featureId":"backlinks","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":".footer","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Backlinks","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""}},"tags":{"featureId":"tags","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":".header .data-bar","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"showInlineTags":true,"showFrontmatterTags":true,"info_showInlineTags":{"show":true,"name":"","description":"Show tags defined inside the document at the top of the page.","placeholder":""},"info_showFrontmatterTags":{"show":true,"name":"","description":"Show tags defined in the frontmatter of the document at the top of the page.","placeholder":""}},"alias":{"featureId":"aliases","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":".header .data-bar","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Aliases","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""}},"properties":{"featureId":"properties","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":".header","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Properties","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"info_hideProperties":{"show":true,"name":"","description":"A list of properties to hide from the properties view","placeholder":""}},"fileNavigation":{"featureId":"file-navigation","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#left-sidebar-content","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"info_includePath":{"show":false,"name":"","description":"","placeholder":""},"showCustomIcons":false,"showDefaultFolderIcons":false,"showDefaultFileIcons":false,"defaultFolderIcon":"lucide//folder","defaultFileIcon":"lucide//file","defaultMediaIcon":"lucide//file-image","exposeStartingPath":true,"info_showCustomIcons":{"show":true,"name":"","description":"Show custom icons for files and folders","placeholder":""},"info_showDefaultFolderIcons":{"show":true,"name":"","description":"Show a default icon of a folder for every folder in the tree","placeholder":""},"info_showDefaultFileIcons":{"show":true,"name":"","description":"Show a default icon of a file for every file in the tree","placeholder":""},"info_defaultFolderIcon":{"show":true,"name":"","description":"The icon to use for folders. Prefix with 'lucide//' to use a Lucide icon","placeholder":""},"info_defaultFileIcon":{"show":true,"name":"","description":"The icon to use for files. Prefix with 'lucide//' to use a Lucide icon","placeholder":""},"info_defaultMediaIcon":{"show":true,"name":"","description":"The icon to use for media files. Prefix with 'lucide//' to use a Lucide icon","placeholder":""},"info_exposeStartingPath":{"show":true,"name":"","description":"Whether or not to show the current file in the file tree when the page is first loaded","placeholder":""},"includePath":"site-lib/html/file-tree.html"},"search":{"featureId":"search","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#left-sidebar .topbar-content","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Search...","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""}},"outline":{"featureId":"outline","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#right-sidebar-content","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Outline","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"startCollapsed":false,"minCollapseDepth":0,"info_startCollapsed":{"show":true,"name":"","description":"Should the outline start collapsed?","placeholder":""},"info_minCollapseDepth":{"show":true,"name":"","description":"Only allow outline items to be collapsed if they are at least this many levels deep in the tree.","placeholder":"","dropdownOptions":{"1":1,"2":2,"No Collapse":100}}},"themeToggle":{"featureId":"theme-toggle","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#right-sidebar .topbar-content","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""}},"graphView":{"featureId":"graph-view","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#right-sidebar-content","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Graph View","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"showOrphanNodes":true,"showAttachments":false,"allowGlobalGraph":true,"allowExpand":true,"attractionForce":1,"linkLength":15,"repulsionForce":80,"centralForce":2,"edgePruning":100,"minNodeRadius":3,"maxNodeRadius":7,"info_showOrphanNodes":{"show":true,"name":"","description":"Show nodes that are not connected to any other nodes.","placeholder":""},"info_showAttachments":{"show":true,"name":"","description":"Show attachments like images and PDFs as nodes in the graph.","placeholder":""},"info_allowGlobalGraph":{"show":true,"name":"","description":"Allow the user to view the global graph of all nodes.","placeholder":""},"info_allowExpand":{"show":true,"name":"","description":"Allow the user to pop-out the graph view to take up the whole screen","placeholder":""},"info_attractionForce":{"show":true,"name":"","description":"How much should linked nodes attract each other? This will make the graph appear more clustered.","placeholder":""},"info_linkLength":{"show":true,"name":"","description":"How long should the links between nodes be? The shorter the links the more connected nodes will cluster together.","placeholder":""},"info_repulsionForce":{"show":true,"name":"","description":"How much should nodes repel each other? This will make disconnected parts more spread out.","placeholder":""},"info_centralForce":{"show":true,"name":"","description":"How much should nodes be attracted to the center? This will make the graph appear more dense and circular.","placeholder":""},"info_edgePruning":{"show":true,"name":"","description":"Edges with a length above this threshold will not be rendered, however they will still contribute to the simulation. This can help large tangled graphs look more organised. Hovering over a node will still display these links.","placeholder":""},"info_minNodeRadius":{"show":true,"name":"","description":"How small should the smallest nodes be? The smaller a node is the less it will attract other nodes.","placeholder":""},"info_maxNodeRadius":{"show":true,"name":"","description":"How large should the largest nodes be? Nodes are sized by how many links they have. The larger a node is the more it will attract other nodes. This can be used to create a good grouping around the most important nodes.","placeholder":""}},"sidebar":{"featureId":"sidebar","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"allowResizing":true,"allowCollapsing":true,"rightDefaultWidth":"20em","leftDefaultWidth":"20em","info_allowResizing":{"show":true,"name":"","description":"Whether or not to allow the sidebars to be resized","placeholder":""},"info_allowCollapsing":{"show":true,"name":"","description":"Whether or not to allow the sidebars to be collapsed","placeholder":""},"info_rightDefaultWidth":{"show":true,"name":"","description":"The default width of the right sidebar","placeholder":""},"info_leftDefaultWidth":{"show":true,"name":"","description":"The default width of the left sidebar","placeholder":""}},"customHead":{"featureId":"custom-head","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"head","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"info_includePath":{"show":false,"name":"","description":"","placeholder":""},"sourcePath":"","info_sourcePath":{"show":true,"name":"","description":"The local path to the source .html file which will be included.","placeholder":"","fileInputOptions":{"makeRelativeToVault":true,"browseButton":true}},"includePath":"site-lib/html/custom-head.html"},"document":{"featureId":"obsidian-document","enabled":true,"unavailable":false,"alwaysEnabled":true,"hideSettingsButton":false,"allowFoldingLists":true,"allowFoldingHeadings":true,"documentWidth":"40em","info_allowFoldingLists":{"show":true,"name":"","description":"Whether or not to allow lists to be folded","placeholder":""},"info_allowFoldingHeadings":{"show":true,"name":"","description":"Whether or not to allow headings to be folded","placeholder":""},"info_documentWidth":{"show":true,"name":"","description":"The width of the document","placeholder":""}},"rss":{"featureId":"rss","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"siteUrl":"","authorName":"","info_siteUrl":{"show":true,"name":"","description":"The url that this site will be hosted at","placeholder":"https://example.com/mysite"},"info_authorName":{"show":true,"name":"","description":"The name of the author of the site","placeholder":""}},"linkPreview":{"featureId":"link-preview","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":true}},"modifiedTime":1769100154988,"siteName":"RepresentationDiffusion","vaultName":"BachelorThesis","exportRoot":"","baseURL":"","pluginVersion":"1.9.2","themeName":"","bodyClasses":"publish css-settings-manager show-inline-title show-ribbon is-focused","hasFavicon":false}