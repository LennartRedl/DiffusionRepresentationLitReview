{"createdTime":1768918285150,"shownInTree":["overview/categories/injection/alignment-loss.html","overview/categories/injection/joint-modeling.html","overview/categories/injection/none.html","overview/categories/injection/tokenizer.html","overview/categories/objective/flexible.html","overview/categories/objective/flow-matching.html","overview/categories/rep_signal/external-vfm.html","overview/categories/rep_signal/internal.html","overview/categories/rep_signal/none.html","overview/categories/space/flexible.html","overview/categories/space/latent.html","overview/categories/space/pixel.html","overview/categories/training/end-to-end.html","overview/categories/training/two-stage.html","overview/maps/all-papers.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html","overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","overview/papers/2025/wangrepaworksit2025a.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/readme.html"],"attachments":["site-lib/scripts/graph-wasm.wasm","site-lib/fonts/94f2f163d4b698242fef.otf","site-lib/fonts/72505e6a122c6acd5471.woff2","site-lib/fonts/2d5198822ab091ce4305.woff2","site-lib/fonts/c8ba52b05a9ef10f4758.woff2","site-lib/fonts/cb10ffd7684cd9836a05.woff2","site-lib/fonts/293fd13dbca5a3e450ef.woff2","site-lib/fonts/085cb93e613ba3d40d2b.woff2","site-lib/fonts/b5f0f109bc88052d4000.woff2","site-lib/fonts/cbe0ae49c52c920fd563.woff2","site-lib/fonts/535a6cf662596b3bd6a6.woff2","site-lib/fonts/70cc7ff27245e82ad414.ttf","site-lib/fonts/454577c22304619db035.ttf","site-lib/fonts/52ac8f3034507f1d9e53.ttf","site-lib/fonts/05b618077343fbbd92b7.ttf","site-lib/fonts/4bb6ac751d1c5478ff3a.woff2","site-lib/media/6155340132a851f6089e.svg","site-lib/media/2308ab1944a6bfa5c5b8.svg","site-lib/fonts/mathjax_zero.woff","site-lib/fonts/mathjax_main-regular.woff","site-lib/fonts/mathjax_main-bold.woff","site-lib/fonts/mathjax_math-italic.woff","site-lib/fonts/mathjax_main-italic.woff","site-lib/fonts/mathjax_math-bolditalic.woff","site-lib/fonts/mathjax_size1-regular.woff","site-lib/fonts/mathjax_size2-regular.woff","site-lib/fonts/mathjax_size3-regular.woff","site-lib/fonts/mathjax_size4-regular.woff","site-lib/fonts/mathjax_ams-regular.woff","site-lib/fonts/mathjax_calligraphic-regular.woff","site-lib/fonts/mathjax_calligraphic-bold.woff","site-lib/fonts/mathjax_fraktur-regular.woff","site-lib/fonts/mathjax_fraktur-bold.woff","site-lib/fonts/mathjax_sansserif-regular.woff","site-lib/fonts/mathjax_sansserif-bold.woff","site-lib/fonts/mathjax_sansserif-italic.woff","site-lib/fonts/mathjax_script-regular.woff","site-lib/fonts/mathjax_typewriter-regular.woff","site-lib/fonts/mathjax_vector-regular.woff","site-lib/fonts/mathjax_vector-bold.woff","site-lib/html/file-tree-content.html","site-lib/scripts/webpage.js","site-lib/scripts/graph-wasm.js","site-lib/scripts/graph-render-worker.js","site-lib/media/favicon.png","site-lib/styles/obsidian.css","site-lib/styles/global-variable-styles.css","site-lib/styles/main-styles.css","site-lib/rss.xml"],"allFiles":["overview/papers/2025/wangrepaworksit2025a.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html","overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","overview/papers/2025/chendiffusionautoencodersare2025.html","overview/maps/all-papers.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/categories/injection/none.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/categories/objective/flexible.html","overview/categories/objective/flow-matching.html","overview/categories/training/two-stage.html","overview/categories/training/end-to-end.html","overview/categories/space/flexible.html","overview/categories/injection/tokenizer.html","overview/categories/injection/joint-modeling.html","overview/categories/injection/alignment-loss.html","overview/categories/rep_signal/none.html","overview/categories/rep_signal/internal.html","overview/categories/rep_signal/external-vfm.html","overview/categories/space/pixel.html","overview/categories/space/latent.html","overview/readme.html","site-lib/scripts/graph-wasm.wasm","site-lib/fonts/94f2f163d4b698242fef.otf","site-lib/fonts/72505e6a122c6acd5471.woff2","site-lib/fonts/2d5198822ab091ce4305.woff2","site-lib/fonts/c8ba52b05a9ef10f4758.woff2","site-lib/fonts/cb10ffd7684cd9836a05.woff2","site-lib/fonts/293fd13dbca5a3e450ef.woff2","site-lib/fonts/085cb93e613ba3d40d2b.woff2","site-lib/fonts/b5f0f109bc88052d4000.woff2","site-lib/fonts/cbe0ae49c52c920fd563.woff2","site-lib/fonts/535a6cf662596b3bd6a6.woff2","site-lib/fonts/70cc7ff27245e82ad414.ttf","site-lib/fonts/454577c22304619db035.ttf","site-lib/fonts/52ac8f3034507f1d9e53.ttf","site-lib/fonts/05b618077343fbbd92b7.ttf","site-lib/fonts/4bb6ac751d1c5478ff3a.woff2","site-lib/media/6155340132a851f6089e.svg","site-lib/media/2308ab1944a6bfa5c5b8.svg","site-lib/fonts/mathjax_zero.woff","site-lib/fonts/mathjax_main-regular.woff","site-lib/fonts/mathjax_main-bold.woff","site-lib/fonts/mathjax_math-italic.woff","site-lib/fonts/mathjax_main-italic.woff","site-lib/fonts/mathjax_math-bolditalic.woff","site-lib/fonts/mathjax_size1-regular.woff","site-lib/fonts/mathjax_size2-regular.woff","site-lib/fonts/mathjax_size3-regular.woff","site-lib/fonts/mathjax_size4-regular.woff","site-lib/fonts/mathjax_ams-regular.woff","site-lib/fonts/mathjax_calligraphic-regular.woff","site-lib/fonts/mathjax_calligraphic-bold.woff","site-lib/fonts/mathjax_fraktur-regular.woff","site-lib/fonts/mathjax_fraktur-bold.woff","site-lib/fonts/mathjax_sansserif-regular.woff","site-lib/fonts/mathjax_sansserif-bold.woff","site-lib/fonts/mathjax_sansserif-italic.woff","site-lib/fonts/mathjax_script-regular.woff","site-lib/fonts/mathjax_typewriter-regular.woff","site-lib/fonts/mathjax_vector-regular.woff","site-lib/fonts/mathjax_vector-bold.woff","site-lib/html/file-tree-content.html","site-lib/scripts/webpage.js","site-lib/scripts/graph-wasm.js","site-lib/scripts/graph-render-worker.js","site-lib/media/favicon.png","site-lib/styles/obsidian.css","site-lib/styles/global-variable-styles.css","site-lib/styles/main-styles.css"],"webpages":{"overview/categories/injection/alignment-loss.html":{"title":"alignment-loss","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/injection/alignment-loss.html","pathToRoot":"../../..","attachments":[],"createdTime":1768817841833,"modifiedTime":1768817841833,"sourceSize":0,"sourcePath":"Overview/Categories/injection/alignment-loss.md","exportPath":"overview/categories/injection/alignment-loss.html","showInTree":true,"treeOrder":3,"backlinks":["overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/wangrepaworksit2025a.html"],"type":"markdown"},"overview/categories/injection/joint-modeling.html":{"title":"joint-modeling","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/injection/joint-modeling.html","pathToRoot":"../../..","attachments":[],"createdTime":1768817848608,"modifiedTime":1768817848608,"sourceSize":0,"sourcePath":"Overview/Categories/injection/joint-modeling.md","exportPath":"overview/categories/injection/joint-modeling.html","showInTree":true,"treeOrder":4,"backlinks":[],"type":"markdown"},"overview/categories/injection/none.html":{"title":"none","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/injection/none.html","pathToRoot":"../../..","attachments":[],"createdTime":1768839687558,"modifiedTime":1768839687558,"sourceSize":0,"sourcePath":"Overview/Categories/injection/none.md","exportPath":"overview/categories/injection/none.html","showInTree":true,"treeOrder":5,"backlinks":["overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html"],"type":"markdown"},"overview/categories/injection/tokenizer.html":{"title":"tokenizer","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/injection/tokenizer.html","pathToRoot":"../../..","attachments":[],"createdTime":1768817854941,"modifiedTime":1768817854941,"sourceSize":0,"sourcePath":"Overview/Categories/injection/tokenizer.md","exportPath":"overview/categories/injection/tokenizer.html","showInTree":true,"treeOrder":6,"backlinks":["overview/papers/2025/kouzelisboostinggenerativeimage2025.html"],"type":"markdown"},"overview/categories/objective/flexible.html":{"title":"flexible","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/objective/flexible.html","pathToRoot":"../../..","attachments":[],"createdTime":1768839211366,"modifiedTime":1768839211366,"sourceSize":0,"sourcePath":"Overview/Categories/objective/flexible.md","exportPath":"overview/categories/objective/flexible.html","showInTree":true,"treeOrder":8,"backlinks":[],"type":"markdown"},"overview/categories/objective/flow-matching.html":{"title":"flow-matching","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/objective/flow-matching.html","pathToRoot":"../../..","attachments":[],"createdTime":1768838844240,"modifiedTime":1768838844240,"sourceSize":0,"sourcePath":"Overview/Categories/objective/flow-matching.md","exportPath":"overview/categories/objective/flow-matching.html","showInTree":true,"treeOrder":9,"backlinks":[],"type":"markdown"},"overview/categories/rep_signal/external-vfm.html":{"title":"external-vfm","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/rep_signal/external-vfm.html","pathToRoot":"../../..","attachments":[],"createdTime":1768817759883,"modifiedTime":1768817759883,"sourceSize":0,"sourcePath":"Overview/Categories/rep_signal/external-vfm.md","exportPath":"overview/categories/rep_signal/external-vfm.html","showInTree":true,"treeOrder":11,"backlinks":["overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html","overview/papers/2025/wangrepaworksit2025a.html"],"type":"markdown"},"overview/categories/rep_signal/internal.html":{"title":"internal","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/rep_signal/internal.html","pathToRoot":"../../..","attachments":[],"createdTime":1768817808699,"modifiedTime":1768817808699,"sourceSize":0,"sourcePath":"Overview/Categories/rep_signal/internal.md","exportPath":"overview/categories/rep_signal/internal.html","showInTree":true,"treeOrder":12,"backlinks":[],"type":"markdown"},"overview/categories/rep_signal/none.html":{"title":"none","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/rep_signal/none.html","pathToRoot":"../../..","attachments":[],"createdTime":1768817821509,"modifiedTime":1768817821509,"sourceSize":0,"sourcePath":"Overview/Categories/rep_signal/none.md","exportPath":"overview/categories/rep_signal/none.html","showInTree":true,"treeOrder":13,"backlinks":["overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html"],"type":"markdown"},"overview/categories/space/flexible.html":{"title":"flexible","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/space/flexible.html","pathToRoot":"../../..","attachments":[],"createdTime":1768817872627,"modifiedTime":1768817872627,"sourceSize":0,"sourcePath":"Overview/Categories/space/flexible.md","exportPath":"overview/categories/space/flexible.html","showInTree":true,"treeOrder":15,"backlinks":["overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html"],"type":"markdown"},"overview/categories/space/latent.html":{"title":"latent","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/space/latent.html","pathToRoot":"../../..","attachments":[],"createdTime":1768577924932,"modifiedTime":1768577924932,"sourceSize":0,"sourcePath":"Overview/Categories/space/latent.md","exportPath":"overview/categories/space/latent.html","showInTree":true,"treeOrder":16,"backlinks":["overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html","overview/papers/2025/wangrepaworksit2025a.html"],"type":"markdown"},"overview/categories/space/pixel.html":{"title":"pixel","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/space/pixel.html","pathToRoot":"../../..","attachments":[],"createdTime":1768577936836,"modifiedTime":1768577936836,"sourceSize":0,"sourcePath":"Overview/Categories/space/pixel.md","exportPath":"overview/categories/space/pixel.html","showInTree":true,"treeOrder":17,"backlinks":[],"type":"markdown"},"overview/categories/training/end-to-end.html":{"title":"end-to-end","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/training/end-to-end.html","pathToRoot":"../../..","attachments":[],"createdTime":1768818908083,"modifiedTime":1768818908083,"sourceSize":0,"sourcePath":"Overview/Categories/training/end-to-end.md","exportPath":"overview/categories/training/end-to-end.html","showInTree":true,"treeOrder":19,"backlinks":[],"type":"markdown"},"overview/categories/training/two-stage.html":{"title":"two-stage","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/categories/training/two-stage.html","pathToRoot":"../../..","attachments":[],"createdTime":1768818913606,"modifiedTime":1768818913606,"sourceSize":0,"sourcePath":"Overview/Categories/training/two-stage.md","exportPath":"overview/categories/training/two-stage.html","showInTree":true,"treeOrder":20,"backlinks":[],"type":"markdown"},"overview/maps/all-papers.html":{"title":"All-Papers","icon":"","description":"Note that the imagenet_fid score is computed on ImageNet 256x256 resolution, w/o classifier free guidance, after 400K training steps, using a SiT, if not marked with a \"\" at the end of the score. For details on the setup for the FID with \"\" click onto the respective summary page.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":["overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/wangrepaworksit2025a.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html"],"author":"","coverImageURL":"","fullURL":"overview/maps/all-papers.html","pathToRoot":"../..","attachments":[],"createdTime":1768577986054,"modifiedTime":1768897330353,"sourceSize":449,"sourcePath":"Overview/Maps/All-Papers.md","exportPath":"overview/maps/all-papers.html","showInTree":true,"treeOrder":22,"backlinks":[],"type":"markdown"},"overview/papers/2024/yurepresentationalignmentgeneration2025.html":{"title":"Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2410.06940\" target=\"_self\">http://arxiv.org/abs/2410.06940</a>\nRelated:: They add a representation alignment loss that aligns the diffusion models hidden representations to those of a vision foundation model (VFM) to speed up DiT/SiT training.They motivate their work by previous findings that better diffusion models learn hidden representations are semantically more meaningful (higher linear probing accuracy). Representations are already weakly aligned (measured via CKNNA) with those of DINOv2. To improve alignment they add an alignment loss to their overall lossHere denotes the patch index, the hidden representation at a fixed layer, the VFMs representation and a similarity function, e.g. cosine similarity. is a 3-layer MLP that is learned during training. They find that alignment at earlier layers (e.g. 8 out of 24 layers) yields best results.\nThe overall loss becomes where is a hyperparameter used to control the amount of alignment.\n<br>Space: <a data-href=\"Overview/Categories/space/flexible\" href=\"overview/categories/space/flexible.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Overview/Categories/space/flexible</a>\n<br>Rep signal: <a data-href=\"external-vfm\" href=\"overview/categories/rep_signal/external-vfm.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">external-vfm</a>\n<br>Injection: <a data-href=\"alignment-loss\" href=\"overview/categories/injection/alignment-loss.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">alignment-loss</a> <br>ImageNet: FID 7.9 @ 256x256 px, w/o cfg, 400K training steps, DINOv2-B, SiT-XL/2 (see Table 8 in <a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/singhWhatMattersRepresentation2025\" data-href=\"Overview/Papers/2025/singhWhatMattersRepresentation2025\" href=\"overview/papers/2025/singhwhatmattersrepresentation2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">singhWhatMattersRepresentation2025</a>) <br>Why are earlier layers more suitable for alignment? They argue that later layers focus on high-frequency details, but <a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/singhWhatMattersRepresentation2025\" data-href=\"Overview/Papers/2025/singhWhatMattersRepresentation2025\" href=\"overview/papers/2025/singhwhatmattersrepresentation2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">singhWhatMattersRepresentation2025</a> use early layers (e.g. 4, 6 or 8, see Table 1c) as well, who don't focus on semantic/high-level structure.\nThey focus mainly on LDM (SiT is their base backbone model) - more extensive results in pixel diffusion might be interesting. Conditioning / representations:\nLosses:\nArchitecture notes:\nTraining notes:\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results (headline)","level":2,"id":"Results_(headline)_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"},{"heading":"Details","level":1,"id":"Details_0"},{"heading":"Method sketch","level":2,"id":"Method_sketch_0"}],"links":["overview/categories/space/flexible.html","overview/categories/rep_signal/external-vfm.html","overview/categories/injection/alignment-loss.html","overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2025/singhwhatmattersrepresentation2025.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2024/yurepresentationalignmentgeneration2025.html","pathToRoot":"../../..","attachments":[],"createdTime":1768820192430,"modifiedTime":1768839239356,"sourceSize":2892,"sourcePath":"Overview/Papers/2024/yuRepresentationAlignmentGeneration2025.md","exportPath":"overview/papers/2024/yurepresentationalignmentgeneration2025.html","showInTree":true,"treeOrder":25,"backlinks":["overview/papers/2025/singhwhatmattersrepresentation2025.html"],"type":"markdown"},"overview/papers/2025/kouzelisboostinggenerativeimage2025.html":{"title":"Boosting Generative Image Modeling via Joint Image-Feature Synthesis","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2504.16064\" target=\"_self\">http://arxiv.org/abs/2504.16064</a>\nRelated:: They propose ReDi, a generative framework, in which the diffusion model jointly learns the latents of a VAE and the latents of a VFM.With ReDi, they jointly model (i) VAE latents and (ii) the latents of a pretrained VFM (in their experiments, they use DINOv2-B).\nThey consider two approaches to feed the VAE latents and the visual representation tokens to the SiT/DiT:\nMerged Tokens: Here both token are transformed separately to the same dimension and then summed channel-wise to obtain Separate Tokens: Tokens are just concatenated along the sequence dimension $$\n\\mathbf{h}t=\\left[\\mathbf{x}_t \\mathbf{W}{\\mathrm{emb}}^x, \\mathbf{z}t \\mathbf{W}{\\mathrm{emb}}^z\\right] \\in \\mathbb{R}^{2 L \\times C_d}. <br>Space: <a data-href=\"latent\" href=\"overview/categories/space/latent.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">latent</a>\n<br>Rep signal: <a data-href=\"external-vfm\" href=\"overview/categories/rep_signal/external-vfm.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">external-vfm</a>\n<br>Injection: <a data-href=\"tokenizer\" href=\"overview/categories/injection/tokenizer.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">tokenizer</a> ImageNet: FID 9.4 @ 256x256 px, w/o cfg, 400K training steps, SiT-L as diffusion backbone and DINOv2-B for external representations (see Table 1)\nNotes: (optional) They don't provide explanations of why\n(i) this approach works well (/better than REPA)\n(ii) they choose DINOv2 and why other VFMs perform worse (see openreview W2 in rebuttal to Reviewer 8DbC20)\nHow does it scale to higher resolutions (e.g. )?\nMissing understanding for why REPA and ReDi is complementary Conditioning / representations:\nLosses:\nArchitecture notes:\nTraining notes:\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results (headline)","level":2,"id":"Results_(headline)_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"},{"heading":"Details","level":1,"id":"Details_0"},{"heading":"Method sketch","level":2,"id":"Method_sketch_0"}],"links":["overview/categories/space/latent.html","overview/categories/rep_signal/external-vfm.html","overview/categories/injection/tokenizer.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/kouzelisboostinggenerativeimage2025.html","pathToRoot":"../../..","attachments":[],"createdTime":1768908441822,"modifiedTime":1768916843534,"sourceSize":2762,"sourcePath":"Overview/Papers/2025/kouzelisBoostingGenerativeImage2025.md","exportPath":"overview/papers/2025/kouzelisboostinggenerativeimage2025.html","showInTree":true,"treeOrder":27,"backlinks":[],"type":"markdown"},"overview/papers/2025/chendiffusionautoencodersare2025.html":{"title":"Diffusion Autoencoders are Scalable Image Tokenizers","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2501.18593\" target=\"_self\">http://arxiv.org/abs/2501.18593</a>\nRelated:: They replace the VAE component with a diffusion model that is conditioned on the latent representation of an encoder to simplify the learning procedure and not rely on external models (GAN, LPIPS loss in standard VAE).They motivate their work by noting that the VAE component used in modern latent diffusion models is based on several \"heuristic\" losses (LPIPS, GAN and reconstruction loss). They propose to instead use a diffusion loss to learn both the encoder and decoder jointly. They condition their diffusion decoder (U-Net) by simply concatenating it to the input, i.e. where is the noised input and is the output of the encoder (during inference the output of the latent diffusion model; transformed to correct resolution via nearest neighbors if latent resolution differs).\n<br>Space: <a data-href=\"latent\" href=\"overview/categories/space/latent.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">latent</a>\n<br>Rep signal: <a data-href=\"Overview/Categories/rep_signal/none\" href=\"overview/categories/rep_signal/none.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Overview/Categories/rep_signal/none</a>\n<br>Injection: <a data-href=\"injection/none\" href=\"overview/categories/injection/none.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">injection/none</a> ImageNet: FID 6.29 @ 256x256 px, w cfg of 2, DiT-XL/2 as diffusion backbone, !number of training steps is not reported\nNotes: (optional) They have very limited quantitative results, particularly in the generative domain Conditioning / representations:\nLosses:\nArchitecture notes:\nTraining notes:\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results (headline)","level":2,"id":"Results_(headline)_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"},{"heading":"Details","level":1,"id":"Details_0"},{"heading":"Method sketch","level":2,"id":"Method_sketch_0"}],"links":["overview/categories/space/latent.html","overview/categories/rep_signal/none.html","overview/categories/injection/none.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/chendiffusionautoencodersare2025.html","pathToRoot":"../../..","attachments":[],"createdTime":1768838020230,"modifiedTime":1768900189121,"sourceSize":2086,"sourcePath":"Overview/Papers/2025/chenDiffusionAutoencodersAre2025.md","exportPath":"overview/papers/2025/chendiffusionautoencodersare2025.html","showInTree":true,"treeOrder":28,"backlinks":["overview/papers/2025/singhwhatmattersrepresentation2025.html"],"type":"markdown"},"overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html":{"title":"Diffusion Transformers with Representation Autoencoders","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2510.11690\" target=\"_self\">http://arxiv.org/abs/2510.11690</a>\nRelated:: This paper proposes to train the diffusion model in the latent space of a VFM instead of a VAE for improved generation quality.They want to simplify the training procedure, not by guiding the diffusion model via an external model (e.g. REPA) but by learning the latent of a strong VFM. This reduces hyperparameters and simplifies the architecture.\nTo decode the latents, they train a ViT with their objective as where and denote the encoder (VFM, precisely DINOv2-B/SigLIP2-B and MAE-B) and the decoder (ViT-XL) respectively. They get competitive/better reconstruction FID scores with RAEs compared to typical SD-VAE (see Table 1). To make the RAE decoder more robust, they train it on a smoothed distribution (noise-augmented decoding), effectively learning to decode where denotes the training set processed by the RAE encoder. This enables the decoder to perform well during the generative part.\nThey further show that the unique minimizer of the flow matching loss is representable via a stack of standard DiT-Block iff , where is the dimension of the tokens. They argue that this is due to the full rank of the noise (see also Back To Basics).<br>\nInstead of scaling the width of all the DiT blocks, which is computationally prohibitive, they use a wide DDT (<a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://arxiv.org/pdf/2504.05741v1\" target=\"_self\">https://arxiv.org/pdf/2504.05741v1</a>) head after the standard DiT.<br>\nBuilding on (<a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://arxiv.org/pdf/2403.03206\" target=\"_self\">https://arxiv.org/pdf/2403.03206</a>) they propose a schedule shift but using the effective data dimension computed as \"number of tokens times their dimensionality\".\n<br>Space: <a data-href=\"space/\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">space/</a>\n<br>Rep signal: <a data-href=\"rep_signal/\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">rep_signal/</a>\n<br>Injection: <a data-href=\"injection/\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">injection/</a> ImageNet: FID 2.16 @ 256x256 px, w/o cfg, 80 epochs (should be 400K training steps), LightningDiT-XL as diffusion backbone and train on the latents of DINOv2-B Notes: (optional) How does the autoencoder part RAE work on large scale (e.g. LAION dataset)?\nThey use DINOv2 as it works best in the generative part, however they don't provide any explanation for that. What makes their features good for representation? (compare this to Diffusability, MAETok Theorem 1 and What matter) Conditioning / representations:\nLosses:\nArchitecture notes:\nTraining notes:\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results (headline)","level":2,"id":"Results_(headline)_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"},{"heading":"Details","level":1,"id":"Details_0"},{"heading":"Method sketch","level":2,"id":"Method_sketch_0"}],"links":[".html",".html",".html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","pathToRoot":"../../..","attachments":[],"createdTime":1768840091876,"modifiedTime":1768901541954,"sourceSize":3449,"sourcePath":"Overview/Papers/2025/zhengDiffusionTransformersRepresentation2025.md","exportPath":"overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","showInTree":true,"treeOrder":29,"backlinks":[],"type":"markdown"},"overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html":{"title":"Improving the Diffusability of Autoencoders","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2502.14831\" target=\"_self\">http://arxiv.org/abs/2502.14831</a>\nRelated:: They fine tune the VAE using scale equivariance, such that the latent space has less high-frequency signals, simplifying the training task for the diffusion model.Their key assumption is that diffusion models \"synthesize low-frequency components first and add high-frequency ones on top\" later, which results in pictures that are perceived as realistic by humans, as we focus more on structure and composition than on high-level details. Consider the per-frequency observation model where denotes an orthonormal transform (e.g. discrete cosine transform (DCT)). The per-frequency SNR now becomesSo in order for the diffusion model to focus on low-frequencies in the high noise regime one needs to be very large for .\nIn experiments they find that the frequency spectrum in the RGB domain resembles that structure, whereas in modern VAEs (e.g. FluxAE) a lot more energy is in the high-frequency domain (see Figure 4).\nThey argue that downsampling removes high-frequency components and thus propose to regularize the model via where and are the downsampled versions of the input image and the latent respectively and is a distance metric ( + LPIPS). They refer to this regularization as Scale Equivariance Regularization.\n<br>Space: <a data-href=\"latent\" href=\"overview/categories/space/latent.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">latent</a>\n<br>Rep signal: <a data-tooltip-position=\"top\" aria-label=\"Overview/Categories/rep_signal/none\" data-href=\"Overview/Categories/rep_signal/none\" href=\"overview/categories/rep_signal/none.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">none</a>\n<br>Injection: <a data-tooltip-position=\"top\" aria-label=\"Overview/Categories/injection/none\" data-href=\"Overview/Categories/injection/none\" href=\"overview/categories/injection/none.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">none</a> ImageNet: FID 9.61 @ 256x256 px, w/o cfg, 400K training steps, DiT-L as diffusion backbone They don't provide rigorous results that characterize good latent spaces based on spectral decomposition They have a non-negligible amount of FLOP overhead ( of the total FLOPs for regularization (see openreview Rebuttal to Reviewer JqrQ)) and it makes training more complex with additional hyperparameters Conditioning / representations:\nLosses:\nArchitecture notes:\nTraining notes:\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results (headline)","level":2,"id":"Results_(headline)_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"},{"heading":"Details","level":1,"id":"Details_0"},{"heading":"Method sketch","level":2,"id":"Method_sketch_0"}],"links":["overview/categories/space/latent.html","overview/categories/rep_signal/none.html","overview/categories/injection/none.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","pathToRoot":"../../..","attachments":[],"createdTime":1768901150386,"modifiedTime":1768916884025,"sourceSize":3351,"sourcePath":"Overview/Papers/2025/skorokhodovImprovingDiffusabilityAutoencoders2025.md","exportPath":"overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","showInTree":true,"treeOrder":30,"backlinks":[],"type":"markdown"},"overview/papers/2025/wangrepaworksit2025a.html":{"title":"REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2505.16792\" target=\"_self\">http://arxiv.org/abs/2505.16792</a>\nRelated:: The authors argue that representation alignment in diffusion training accelerates convergence in early epochs but degrades performance later, so they propose to stop the alignment at a fixed number of iterations.\n<br>Space: <a data-href=\"latent\" href=\"overview/categories/space/latent.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">latent</a>\n<br>Rep signal: <a data-href=\"external-vfm\" href=\"overview/categories/rep_signal/external-vfm.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">external-vfm</a>\n<br>Injection: <a data-href=\"alignment-loss\" href=\"overview/categories/injection/alignment-loss.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">alignment-loss</a> ImageNet: FID 8.9 @ 256x256 px, w/o cfg, 400K training steps, SiT-L as diffusion backbone, DINOv2-B for external representations (see Table 9)\nNotes: (optional) Conditioning / representations:\nLosses:\nArchitecture notes:\nTraining notes:\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results (headline)","level":2,"id":"Results_(headline)_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"},{"heading":"Details","level":1,"id":"Details_0"},{"heading":"Method sketch","level":2,"id":"Method_sketch_0"}],"links":["overview/categories/space/latent.html","overview/categories/rep_signal/external-vfm.html","overview/categories/injection/alignment-loss.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/wangrepaworksit2025a.html","pathToRoot":"../../..","attachments":[],"createdTime":1768916177239,"modifiedTime":1768917043567,"sourceSize":1360,"sourcePath":"Overview/Papers/2025/wangREPAWorksIt2025a.md","exportPath":"overview/papers/2025/wangrepaworksit2025a.html","showInTree":true,"treeOrder":31,"backlinks":[],"type":"markdown"},"overview/papers/2025/lengrepaeunlockingvae2025a.html":{"title":"REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2504.10483\" target=\"_self\">http://arxiv.org/abs/2504.10483</a>\nRelated:: Building on REPA, the authors propose an end-to-end training procedure, where the VAE component is optimized during training by backpropagating the REPA loss. REPA-E aims to improve and simplify the LDM training procedure, by – instead of having a two stage procedure – using an end-to-end pipeline. They find that simply backpropagating the diffusion loss to the VAE is ineffective and leads to a simple latent space of the VAE (see Table 1). In this case the space is easier to denoise, but results in bad generation performance.\nTo mitigate this collapse, they use a VFM as an anchor and backpropagate the REPA loss. Their overall loss is of the form where refer to the parameters for the LDM, VAE and trainable REPA projection layer, respectively. They propose to normalize the latent space using a BatchNorm layer before passing the VAEs output to the SiT component.\nIn their standard procedure they leverage weights of a pretrained VAE, however they show that training the VAE and the Diffusion Model in an end-to-end fashion manner performs on a very similar level (see Table 8).\n<br>Space: <a data-href=\"latent\" href=\"overview/categories/space/latent.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">latent</a>\n<br>Rep signal: <a data-href=\"external-vfm\" href=\"overview/categories/rep_signal/external-vfm.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">external-vfm</a>\n<br>Injection: <a data-href=\"alignment-loss\" href=\"overview/categories/injection/alignment-loss.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">alignment-loss</a> ImageNet: FID 4.07 @ 256x256 px, w/o cfg, 400K training steps, DINOv2-B if external, SiT-XL (see Figure 2b) Conditioning / representations:\nLosses:\nArchitecture notes:\nTraining notes:\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Brief summary","level":2,"id":"Brief_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results (headline)","level":2,"id":"Results_(headline)_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"},{"heading":"Details","level":1,"id":"Details_0"},{"heading":"Method sketch","level":2,"id":"Method_sketch_0"}],"links":["overview/categories/space/latent.html","overview/categories/rep_signal/external-vfm.html","overview/categories/injection/alignment-loss.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/lengrepaeunlockingvae2025a.html","pathToRoot":"../../..","attachments":[],"createdTime":1768828626381,"modifiedTime":1768840483507,"sourceSize":2267,"sourcePath":"Overview/Papers/2025/lengREPAEUnlockingVAE2025a.md","exportPath":"overview/papers/2025/lengrepaeunlockingvae2025a.html","showInTree":true,"treeOrder":32,"backlinks":[],"type":"markdown"},"overview/papers/2025/singhwhatmattersrepresentation2025.html":{"title":"What matters for Representation Alignment: Global Information or Spatial Structure?","icon":"","description":"\nPaper: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://arxiv.org/abs/2512.10794\" target=\"_self\">http://arxiv.org/abs/2512.10794</a>\n<br>Related:: <a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2024/yuRepresentationAlignmentGeneration2025\" data-href=\"Overview/Papers/2024/yuRepresentationAlignmentGeneration2025\" href=\"overview/papers/2024/yurepresentationalignmentgeneration2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">yuRepresentationAlignmentGeneration2025</a>\nTheir method iREPA tweaks REPA by replacing the projection MLP with a conv layer and adding spatial normalization to better preserve “spatial structure” during teacher-to-diffusion feature alignment.In experiments across 27 VFMs, they find that linear probing accuracy on the hidden representations correlates weaker () with the gFID than the spatial structure (). Linear probing accuracy is measured via ImageNet-1k accuracy, whereas spatial structure is measured via a local vs. distant self-similarity metric between patch tokens They explain that using larger VFMs for alignment as well as including the CLS token can lead to worse generation quality due to worse spatial structure.\nUsing a convolution layer for the projection of the VFM latents they focus alignment more on spatial structure. Furthermore, they\n<br>Space: <a data-href=\"Overview/Categories/space/flexible\" href=\"overview/categories/space/flexible.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Overview/Categories/space/flexible</a>\n<br>Rep signal: <a data-href=\"external-vfm\" href=\"overview/categories/rep_signal/external-vfm.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">external-vfm</a>\n<br>Injection: <a data-href=\"alignment-loss\" href=\"overview/categories/injection/alignment-loss.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">alignment-loss</a> ImageNet: FID 7.52 @ 256x256 px, w/o cfg, 400K training steps, DINOv2, SiT-XL/2 (see Table 6, Appendix E) <br>Result is grounded in REPA setup, i.e. it relies on the alignment-loss to improve representation quality -&gt; Does that scale to other setups, e.g. RAE <a data-tooltip-position=\"top\" aria-label=\"Overview/Papers/2025/chenDiffusionAutoencodersAre2025\" data-href=\"Overview/Papers/2025/chenDiffusionAutoencodersAre2025\" href=\"overview/papers/2025/chendiffusionautoencodersare2025.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">chenDiffusionAutoencodersAre2025</a>? No explanation of why the spatial structure is more important At higher resolution of 512x512 px the difference in FID between REPA and iREPA seems to become smaller as training steps increase -&gt; why? Conditioning / representations:\nLosses:\nArchitecture notes:\nTraining notes:\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Links","level":2,"id":"Links_0"},{"heading":"Summary","level":1,"id":"Summary_0"},{"heading":"One-liner (what changed vs baseline?)","level":2,"id":"One-liner_(what_changed_vs_baseline?)_0"},{"heading":"Short summary","level":2,"id":"Short_summary_0"},{"heading":"Key design choices","level":2,"id":"Key_design_choices_0"},{"heading":"Results (headline)","level":2,"id":"Results_(headline)_0"},{"heading":"Limitations / open questions","level":2,"id":"Limitations_/_open_questions_0"},{"heading":"Details (for the team)","level":1,"id":"Details_(for_the_team)_0"},{"heading":"Method sketch","level":2,"id":"Method_sketch_0"}],"links":["overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/categories/space/flexible.html","overview/categories/rep_signal/external-vfm.html","overview/categories/injection/alignment-loss.html","overview/papers/2025/chendiffusionautoencodersare2025.html"],"author":"","coverImageURL":"","fullURL":"overview/papers/2025/singhwhatmattersrepresentation2025.html","pathToRoot":"../../..","attachments":[],"createdTime":1768578369766,"modifiedTime":1768913648920,"sourceSize":2897,"sourcePath":"Overview/Papers/2025/singhWhatMattersRepresentation2025.md","exportPath":"overview/papers/2025/singhwhatmattersrepresentation2025.html","showInTree":true,"treeOrder":33,"backlinks":["overview/papers/2024/yurepresentationalignmentgeneration2025.html"],"type":"markdown"},"overview/readme.html":{"title":"README","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"overview/readme.html","pathToRoot":"..","attachments":[],"createdTime":1768575086055,"modifiedTime":1768575086055,"sourceSize":0,"sourcePath":"Overview/README.md","exportPath":"overview/readme.html","showInTree":true,"treeOrder":34,"backlinks":[],"type":"markdown"}},"fileInfo":{"overview/categories/injection/alignment-loss.html":{"createdTime":1768817841833,"modifiedTime":1768817841833,"sourceSize":0,"sourcePath":"Overview/Categories/injection/alignment-loss.md","exportPath":"overview/categories/injection/alignment-loss.html","showInTree":true,"treeOrder":3,"backlinks":["overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/wangrepaworksit2025a.html"],"type":"markdown","data":null},"overview/categories/injection/joint-modeling.html":{"createdTime":1768817848608,"modifiedTime":1768817848608,"sourceSize":0,"sourcePath":"Overview/Categories/injection/joint-modeling.md","exportPath":"overview/categories/injection/joint-modeling.html","showInTree":true,"treeOrder":4,"backlinks":[],"type":"markdown","data":null},"overview/categories/injection/none.html":{"createdTime":1768839687558,"modifiedTime":1768839687558,"sourceSize":0,"sourcePath":"Overview/Categories/injection/none.md","exportPath":"overview/categories/injection/none.html","showInTree":true,"treeOrder":5,"backlinks":["overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html"],"type":"markdown","data":null},"overview/categories/injection/tokenizer.html":{"createdTime":1768817854941,"modifiedTime":1768817854941,"sourceSize":0,"sourcePath":"Overview/Categories/injection/tokenizer.md","exportPath":"overview/categories/injection/tokenizer.html","showInTree":true,"treeOrder":6,"backlinks":["overview/papers/2025/kouzelisboostinggenerativeimage2025.html"],"type":"markdown","data":null},"overview/categories/objective/flexible.html":{"createdTime":1768839211366,"modifiedTime":1768839211366,"sourceSize":0,"sourcePath":"Overview/Categories/objective/flexible.md","exportPath":"overview/categories/objective/flexible.html","showInTree":true,"treeOrder":8,"backlinks":[],"type":"markdown","data":null},"overview/categories/objective/flow-matching.html":{"createdTime":1768838844240,"modifiedTime":1768838844240,"sourceSize":0,"sourcePath":"Overview/Categories/objective/flow-matching.md","exportPath":"overview/categories/objective/flow-matching.html","showInTree":true,"treeOrder":9,"backlinks":[],"type":"markdown","data":null},"overview/categories/rep_signal/external-vfm.html":{"createdTime":1768817759883,"modifiedTime":1768817759883,"sourceSize":0,"sourcePath":"Overview/Categories/rep_signal/external-vfm.md","exportPath":"overview/categories/rep_signal/external-vfm.html","showInTree":true,"treeOrder":11,"backlinks":["overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html","overview/papers/2025/wangrepaworksit2025a.html"],"type":"markdown","data":null},"overview/categories/rep_signal/internal.html":{"createdTime":1768817808699,"modifiedTime":1768817808699,"sourceSize":0,"sourcePath":"Overview/Categories/rep_signal/internal.md","exportPath":"overview/categories/rep_signal/internal.html","showInTree":true,"treeOrder":12,"backlinks":[],"type":"markdown","data":null},"overview/categories/rep_signal/none.html":{"createdTime":1768817821509,"modifiedTime":1768817821509,"sourceSize":0,"sourcePath":"Overview/Categories/rep_signal/none.md","exportPath":"overview/categories/rep_signal/none.html","showInTree":true,"treeOrder":13,"backlinks":["overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html"],"type":"markdown","data":null},"overview/categories/space/flexible.html":{"createdTime":1768817872627,"modifiedTime":1768817872627,"sourceSize":0,"sourcePath":"Overview/Categories/space/flexible.md","exportPath":"overview/categories/space/flexible.html","showInTree":true,"treeOrder":15,"backlinks":["overview/papers/2025/singhwhatmattersrepresentation2025.html","overview/papers/2024/yurepresentationalignmentgeneration2025.html"],"type":"markdown","data":null},"overview/categories/space/latent.html":{"createdTime":1768577924932,"modifiedTime":1768577924932,"sourceSize":0,"sourcePath":"Overview/Categories/space/latent.md","exportPath":"overview/categories/space/latent.html","showInTree":true,"treeOrder":16,"backlinks":["overview/papers/2025/lengrepaeunlockingvae2025a.html","overview/papers/2025/chendiffusionautoencodersare2025.html","overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","overview/papers/2025/kouzelisboostinggenerativeimage2025.html","overview/papers/2025/wangrepaworksit2025a.html"],"type":"markdown","data":null},"overview/categories/space/pixel.html":{"createdTime":1768577936836,"modifiedTime":1768577936836,"sourceSize":0,"sourcePath":"Overview/Categories/space/pixel.md","exportPath":"overview/categories/space/pixel.html","showInTree":true,"treeOrder":17,"backlinks":[],"type":"markdown","data":null},"overview/categories/training/end-to-end.html":{"createdTime":1768818908083,"modifiedTime":1768818908083,"sourceSize":0,"sourcePath":"Overview/Categories/training/end-to-end.md","exportPath":"overview/categories/training/end-to-end.html","showInTree":true,"treeOrder":19,"backlinks":[],"type":"markdown","data":null},"overview/categories/training/two-stage.html":{"createdTime":1768818913606,"modifiedTime":1768818913606,"sourceSize":0,"sourcePath":"Overview/Categories/training/two-stage.md","exportPath":"overview/categories/training/two-stage.html","showInTree":true,"treeOrder":20,"backlinks":[],"type":"markdown","data":null},"overview/maps/all-papers.html":{"createdTime":1768577986054,"modifiedTime":1768897330353,"sourceSize":449,"sourcePath":"Overview/Maps/All-Papers.md","exportPath":"overview/maps/all-papers.html","showInTree":true,"treeOrder":22,"backlinks":[],"type":"markdown","data":null},"overview/papers/2024/yurepresentationalignmentgeneration2025.html":{"createdTime":1768820192430,"modifiedTime":1768839239356,"sourceSize":2892,"sourcePath":"Overview/Papers/2024/yuRepresentationAlignmentGeneration2025.md","exportPath":"overview/papers/2024/yurepresentationalignmentgeneration2025.html","showInTree":true,"treeOrder":25,"backlinks":["overview/papers/2025/singhwhatmattersrepresentation2025.html"],"type":"markdown","data":null},"overview/papers/2025/kouzelisboostinggenerativeimage2025.html":{"createdTime":1768908441822,"modifiedTime":1768916843534,"sourceSize":2762,"sourcePath":"Overview/Papers/2025/kouzelisBoostingGenerativeImage2025.md","exportPath":"overview/papers/2025/kouzelisboostinggenerativeimage2025.html","showInTree":true,"treeOrder":27,"backlinks":[],"type":"markdown","data":null},"overview/papers/2025/chendiffusionautoencodersare2025.html":{"createdTime":1768838020230,"modifiedTime":1768900189121,"sourceSize":2086,"sourcePath":"Overview/Papers/2025/chenDiffusionAutoencodersAre2025.md","exportPath":"overview/papers/2025/chendiffusionautoencodersare2025.html","showInTree":true,"treeOrder":28,"backlinks":["overview/papers/2025/singhwhatmattersrepresentation2025.html"],"type":"markdown","data":null},"overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html":{"createdTime":1768840091876,"modifiedTime":1768901541954,"sourceSize":3449,"sourcePath":"Overview/Papers/2025/zhengDiffusionTransformersRepresentation2025.md","exportPath":"overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","showInTree":true,"treeOrder":29,"backlinks":[],"type":"markdown","data":null},"overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html":{"createdTime":1768901150386,"modifiedTime":1768916884025,"sourceSize":3351,"sourcePath":"Overview/Papers/2025/skorokhodovImprovingDiffusabilityAutoencoders2025.md","exportPath":"overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","showInTree":true,"treeOrder":30,"backlinks":[],"type":"markdown","data":null},"overview/papers/2025/wangrepaworksit2025a.html":{"createdTime":1768916177239,"modifiedTime":1768917043567,"sourceSize":1360,"sourcePath":"Overview/Papers/2025/wangREPAWorksIt2025a.md","exportPath":"overview/papers/2025/wangrepaworksit2025a.html","showInTree":true,"treeOrder":31,"backlinks":[],"type":"markdown","data":null},"overview/papers/2025/lengrepaeunlockingvae2025a.html":{"createdTime":1768828626381,"modifiedTime":1768840483507,"sourceSize":2267,"sourcePath":"Overview/Papers/2025/lengREPAEUnlockingVAE2025a.md","exportPath":"overview/papers/2025/lengrepaeunlockingvae2025a.html","showInTree":true,"treeOrder":32,"backlinks":[],"type":"markdown","data":null},"overview/papers/2025/singhwhatmattersrepresentation2025.html":{"createdTime":1768578369766,"modifiedTime":1768913648920,"sourceSize":2897,"sourcePath":"Overview/Papers/2025/singhWhatMattersRepresentation2025.md","exportPath":"overview/papers/2025/singhwhatmattersrepresentation2025.html","showInTree":true,"treeOrder":33,"backlinks":["overview/papers/2024/yurepresentationalignmentgeneration2025.html"],"type":"markdown","data":null},"overview/readme.html":{"createdTime":1768575086055,"modifiedTime":1768575086055,"sourceSize":0,"sourcePath":"Overview/README.md","exportPath":"overview/readme.html","showInTree":true,"treeOrder":34,"backlinks":[],"type":"markdown","data":null},"site-lib/scripts/graph-wasm.wasm":{"createdTime":1768575105303,"modifiedTime":1768575009474.6726,"sourceSize":23655,"sourcePath":"","exportPath":"site-lib/scripts/graph-wasm.wasm","showInTree":false,"treeOrder":0,"backlinks":[],"type":"other","data":null},"site-lib/fonts/94f2f163d4b698242fef.otf":{"createdTime":1768918285127,"modifiedTime":1768918285127,"sourceSize":66800,"sourcePath":"","exportPath":"site-lib/fonts/94f2f163d4b698242fef.otf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/72505e6a122c6acd5471.woff2":{"createdTime":1768918285128,"modifiedTime":1768918285128,"sourceSize":104232,"sourcePath":"","exportPath":"site-lib/fonts/72505e6a122c6acd5471.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/2d5198822ab091ce4305.woff2":{"createdTime":1768918285127,"modifiedTime":1768918285127,"sourceSize":104332,"sourcePath":"","exportPath":"site-lib/fonts/2d5198822ab091ce4305.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/c8ba52b05a9ef10f4758.woff2":{"createdTime":1768918285129,"modifiedTime":1768918285129,"sourceSize":98868,"sourcePath":"","exportPath":"site-lib/fonts/c8ba52b05a9ef10f4758.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/cb10ffd7684cd9836a05.woff2":{"createdTime":1768918285129,"modifiedTime":1768918285129,"sourceSize":106876,"sourcePath":"","exportPath":"site-lib/fonts/cb10ffd7684cd9836a05.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/293fd13dbca5a3e450ef.woff2":{"createdTime":1768918285128,"modifiedTime":1768918285128,"sourceSize":105924,"sourcePath":"","exportPath":"site-lib/fonts/293fd13dbca5a3e450ef.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/085cb93e613ba3d40d2b.woff2":{"createdTime":1768918285129,"modifiedTime":1768918285129,"sourceSize":112184,"sourcePath":"","exportPath":"site-lib/fonts/085cb93e613ba3d40d2b.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/b5f0f109bc88052d4000.woff2":{"createdTime":1768918285128,"modifiedTime":1768918285128,"sourceSize":105804,"sourcePath":"","exportPath":"site-lib/fonts/b5f0f109bc88052d4000.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/cbe0ae49c52c920fd563.woff2":{"createdTime":1768918285129,"modifiedTime":1768918285129,"sourceSize":106108,"sourcePath":"","exportPath":"site-lib/fonts/cbe0ae49c52c920fd563.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/535a6cf662596b3bd6a6.woff2":{"createdTime":1768918285128,"modifiedTime":1768918285128,"sourceSize":111708,"sourcePath":"","exportPath":"site-lib/fonts/535a6cf662596b3bd6a6.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/70cc7ff27245e82ad414.ttf":{"createdTime":1768918285130,"modifiedTime":1768918285130,"sourceSize":192740,"sourcePath":"","exportPath":"site-lib/fonts/70cc7ff27245e82ad414.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/454577c22304619db035.ttf":{"createdTime":1768918285130,"modifiedTime":1768918285130,"sourceSize":161376,"sourcePath":"","exportPath":"site-lib/fonts/454577c22304619db035.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/52ac8f3034507f1d9e53.ttf":{"createdTime":1768918285130,"modifiedTime":1768918285130,"sourceSize":191568,"sourcePath":"","exportPath":"site-lib/fonts/52ac8f3034507f1d9e53.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/05b618077343fbbd92b7.ttf":{"createdTime":1768918285130,"modifiedTime":1768918285130,"sourceSize":155288,"sourcePath":"","exportPath":"site-lib/fonts/05b618077343fbbd92b7.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/4bb6ac751d1c5478ff3a.woff2":{"createdTime":1768918285126,"modifiedTime":1768918285126,"sourceSize":7876,"sourcePath":"","exportPath":"site-lib/fonts/4bb6ac751d1c5478ff3a.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/media/6155340132a851f6089e.svg":{"createdTime":1768918285126,"modifiedTime":1768918285126,"sourceSize":315,"sourcePath":"","exportPath":"site-lib/media/6155340132a851f6089e.svg","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"site-lib/media/2308ab1944a6bfa5c5b8.svg":{"createdTime":1768918285126,"modifiedTime":1768918285126,"sourceSize":278,"sourcePath":"","exportPath":"site-lib/media/2308ab1944a6bfa5c5b8.svg","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"site-lib/fonts/mathjax_zero.woff":{"createdTime":1768918223691,"modifiedTime":1768918223691,"sourceSize":1368,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_zero.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_main-regular.woff":{"createdTime":1768918223691,"modifiedTime":1768918223691,"sourceSize":34160,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_main-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_main-bold.woff":{"createdTime":1768918223691,"modifiedTime":1768918223691,"sourceSize":34464,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_main-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_math-italic.woff":{"createdTime":1768918223691,"modifiedTime":1768918223691,"sourceSize":19360,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_math-italic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_main-italic.woff":{"createdTime":1768918223691,"modifiedTime":1768918223691,"sourceSize":20832,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_main-italic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_math-bolditalic.woff":{"createdTime":1768918223691,"modifiedTime":1768918223691,"sourceSize":19776,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_math-bolditalic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size1-regular.woff":{"createdTime":1768918223691,"modifiedTime":1768918223691,"sourceSize":5792,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size1-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size2-regular.woff":{"createdTime":1768918223691,"modifiedTime":1768918223691,"sourceSize":5464,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size2-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size3-regular.woff":{"createdTime":1768918223691,"modifiedTime":1768918223691,"sourceSize":3244,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size3-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size4-regular.woff":{"createdTime":1768918223691,"modifiedTime":1768918223691,"sourceSize":5148,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size4-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_ams-regular.woff":{"createdTime":1768918223691,"modifiedTime":1768918223691,"sourceSize":40808,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_ams-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_calligraphic-regular.woff":{"createdTime":1768918223692,"modifiedTime":1768918223692,"sourceSize":9600,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_calligraphic-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_calligraphic-bold.woff":{"createdTime":1768918223692,"modifiedTime":1768918223692,"sourceSize":9908,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_calligraphic-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_fraktur-regular.woff":{"createdTime":1768918223692,"modifiedTime":1768918223692,"sourceSize":21480,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_fraktur-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_fraktur-bold.woff":{"createdTime":1768918223692,"modifiedTime":1768918223692,"sourceSize":22340,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_fraktur-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_sansserif-regular.woff":{"createdTime":1768918223692,"modifiedTime":1768918223692,"sourceSize":12660,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_sansserif-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_sansserif-bold.woff":{"createdTime":1768918223692,"modifiedTime":1768918223692,"sourceSize":15944,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_sansserif-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_sansserif-italic.woff":{"createdTime":1768918223692,"modifiedTime":1768918223692,"sourceSize":14628,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_sansserif-italic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_script-regular.woff":{"createdTime":1768918223692,"modifiedTime":1768918223692,"sourceSize":11852,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_script-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_typewriter-regular.woff":{"createdTime":1768918223693,"modifiedTime":1768918223693,"sourceSize":17604,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_typewriter-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_vector-regular.woff":{"createdTime":1768918223693,"modifiedTime":1768918223693,"sourceSize":1136,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_vector-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_vector-bold.woff":{"createdTime":1768918223693,"modifiedTime":1768918223693,"sourceSize":1116,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_vector-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/html/file-tree-content.html":{"createdTime":1768918285306,"modifiedTime":1768918285306,"sourceSize":17177,"sourcePath":"","exportPath":"site-lib/html/file-tree-content.html","showInTree":false,"treeOrder":0,"backlinks":[],"type":"html","data":null},"site-lib/scripts/webpage.js":{"createdTime":1768575105527,"modifiedTime":1768575105527,"sourceSize":110729,"sourcePath":"","exportPath":"site-lib/scripts/webpage.js","showInTree":false,"treeOrder":0,"backlinks":[],"type":"script","data":null},"site-lib/scripts/graph-wasm.js":{"createdTime":1768575105527,"modifiedTime":1768575105528,"sourceSize":12885,"sourcePath":"","exportPath":"site-lib/scripts/graph-wasm.js","showInTree":false,"treeOrder":0,"backlinks":[],"type":"script","data":null},"site-lib/scripts/graph-render-worker.js":{"createdTime":1768575105528,"modifiedTime":1768575105528,"sourceSize":5681,"sourcePath":"","exportPath":"site-lib/scripts/graph-render-worker.js","showInTree":false,"treeOrder":0,"backlinks":[],"type":"script","data":null},"site-lib/media/favicon.png":{"createdTime":1768918285081,"modifiedTime":1768918285081,"sourceSize":1105,"sourcePath":"","exportPath":"site-lib/media/favicon.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"site-lib/styles/obsidian.css":{"createdTime":1768918285149,"modifiedTime":1768918285149,"sourceSize":206058,"sourcePath":"","exportPath":"site-lib/styles/obsidian.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/styles/global-variable-styles.css":{"createdTime":1768918285117,"modifiedTime":1768918285117,"sourceSize":305,"sourcePath":"","exportPath":"site-lib/styles/global-variable-styles.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/styles/main-styles.css":{"createdTime":1768575105539,"modifiedTime":1768575105539,"sourceSize":19521,"sourcePath":"","exportPath":"site-lib/styles/main-styles.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/rss.xml":{"createdTime":1768918289336,"modifiedTime":1768918289336,"sourceSize":25330,"sourcePath":"","exportPath":"site-lib/rss.xml","showInTree":false,"treeOrder":0,"backlinks":[],"type":"other","data":null}},"sourceToTarget":{"Overview/Categories/injection/alignment-loss.md":"overview/categories/injection/alignment-loss.html","Overview/Categories/injection/joint-modeling.md":"overview/categories/injection/joint-modeling.html","Overview/Categories/injection/none.md":"overview/categories/injection/none.html","Overview/Categories/injection/tokenizer.md":"overview/categories/injection/tokenizer.html","Overview/Categories/objective/flexible.md":"overview/categories/objective/flexible.html","Overview/Categories/objective/flow-matching.md":"overview/categories/objective/flow-matching.html","Overview/Categories/rep_signal/external-vfm.md":"overview/categories/rep_signal/external-vfm.html","Overview/Categories/rep_signal/internal.md":"overview/categories/rep_signal/internal.html","Overview/Categories/rep_signal/none.md":"overview/categories/rep_signal/none.html","Overview/Categories/space/flexible.md":"overview/categories/space/flexible.html","Overview/Categories/space/latent.md":"overview/categories/space/latent.html","Overview/Categories/space/pixel.md":"overview/categories/space/pixel.html","Overview/Categories/training/end-to-end.md":"overview/categories/training/end-to-end.html","Overview/Categories/training/two-stage.md":"overview/categories/training/two-stage.html","Overview/Maps/All-Papers.md":"overview/maps/all-papers.html","Overview/Papers/2024/yuRepresentationAlignmentGeneration2025.md":"overview/papers/2024/yurepresentationalignmentgeneration2025.html","Overview/Papers/2025/kouzelisBoostingGenerativeImage2025.md":"overview/papers/2025/kouzelisboostinggenerativeimage2025.html","Overview/Papers/2025/chenDiffusionAutoencodersAre2025.md":"overview/papers/2025/chendiffusionautoencodersare2025.html","Overview/Papers/2025/zhengDiffusionTransformersRepresentation2025.md":"overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html","Overview/Papers/2025/skorokhodovImprovingDiffusabilityAutoencoders2025.md":"overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html","Overview/Papers/2025/wangREPAWorksIt2025a.md":"overview/papers/2025/wangrepaworksit2025a.html","Overview/Papers/2025/lengREPAEUnlockingVAE2025a.md":"overview/papers/2025/lengrepaeunlockingvae2025a.html","Overview/Papers/2025/singhWhatMattersRepresentation2025.md":"overview/papers/2025/singhwhatmattersrepresentation2025.html","Overview/README.md":"overview/readme.html","":"site-lib/rss.xml"},"featureOptions":{"backlinks":{"featureId":"backlinks","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":".footer","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Backlinks","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""}},"tags":{"featureId":"tags","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":".header .data-bar","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"showInlineTags":true,"showFrontmatterTags":true,"info_showInlineTags":{"show":true,"name":"","description":"Show tags defined inside the document at the top of the page.","placeholder":""},"info_showFrontmatterTags":{"show":true,"name":"","description":"Show tags defined in the frontmatter of the document at the top of the page.","placeholder":""}},"alias":{"featureId":"aliases","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":".header .data-bar","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Aliases","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""}},"properties":{"featureId":"properties","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":".header","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Properties","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"info_hideProperties":{"show":true,"name":"","description":"A list of properties to hide from the properties view","placeholder":""}},"fileNavigation":{"featureId":"file-navigation","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#left-sidebar-content","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"info_includePath":{"show":false,"name":"","description":"","placeholder":""},"showCustomIcons":false,"showDefaultFolderIcons":false,"showDefaultFileIcons":false,"defaultFolderIcon":"lucide//folder","defaultFileIcon":"lucide//file","defaultMediaIcon":"lucide//file-image","exposeStartingPath":true,"info_showCustomIcons":{"show":true,"name":"","description":"Show custom icons for files and folders","placeholder":""},"info_showDefaultFolderIcons":{"show":true,"name":"","description":"Show a default icon of a folder for every folder in the tree","placeholder":""},"info_showDefaultFileIcons":{"show":true,"name":"","description":"Show a default icon of a file for every file in the tree","placeholder":""},"info_defaultFolderIcon":{"show":true,"name":"","description":"The icon to use for folders. Prefix with 'lucide//' to use a Lucide icon","placeholder":""},"info_defaultFileIcon":{"show":true,"name":"","description":"The icon to use for files. Prefix with 'lucide//' to use a Lucide icon","placeholder":""},"info_defaultMediaIcon":{"show":true,"name":"","description":"The icon to use for media files. Prefix with 'lucide//' to use a Lucide icon","placeholder":""},"info_exposeStartingPath":{"show":true,"name":"","description":"Whether or not to show the current file in the file tree when the page is first loaded","placeholder":""},"includePath":"site-lib/html/file-tree.html"},"search":{"featureId":"search","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#left-sidebar .topbar-content","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Search...","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""}},"outline":{"featureId":"outline","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#right-sidebar-content","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Outline","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"startCollapsed":false,"minCollapseDepth":0,"info_startCollapsed":{"show":true,"name":"","description":"Should the outline start collapsed?","placeholder":""},"info_minCollapseDepth":{"show":true,"name":"","description":"Only allow outline items to be collapsed if they are at least this many levels deep in the tree.","placeholder":"","dropdownOptions":{"1":1,"2":2,"No Collapse":100}}},"themeToggle":{"featureId":"theme-toggle","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#right-sidebar .topbar-content","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""}},"graphView":{"featureId":"graph-view","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#right-sidebar-content","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Graph View","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"showOrphanNodes":true,"showAttachments":false,"allowGlobalGraph":true,"allowExpand":true,"attractionForce":1,"linkLength":15,"repulsionForce":80,"centralForce":2,"edgePruning":100,"minNodeRadius":3,"maxNodeRadius":7,"info_showOrphanNodes":{"show":true,"name":"","description":"Show nodes that are not connected to any other nodes.","placeholder":""},"info_showAttachments":{"show":true,"name":"","description":"Show attachments like images and PDFs as nodes in the graph.","placeholder":""},"info_allowGlobalGraph":{"show":true,"name":"","description":"Allow the user to view the global graph of all nodes.","placeholder":""},"info_allowExpand":{"show":true,"name":"","description":"Allow the user to pop-out the graph view to take up the whole screen","placeholder":""},"info_attractionForce":{"show":true,"name":"","description":"How much should linked nodes attract each other? This will make the graph appear more clustered.","placeholder":""},"info_linkLength":{"show":true,"name":"","description":"How long should the links between nodes be? The shorter the links the more connected nodes will cluster together.","placeholder":""},"info_repulsionForce":{"show":true,"name":"","description":"How much should nodes repel each other? This will make disconnected parts more spread out.","placeholder":""},"info_centralForce":{"show":true,"name":"","description":"How much should nodes be attracted to the center? This will make the graph appear more dense and circular.","placeholder":""},"info_edgePruning":{"show":true,"name":"","description":"Edges with a length above this threshold will not be rendered, however they will still contribute to the simulation. This can help large tangled graphs look more organised. Hovering over a node will still display these links.","placeholder":""},"info_minNodeRadius":{"show":true,"name":"","description":"How small should the smallest nodes be? The smaller a node is the less it will attract other nodes.","placeholder":""},"info_maxNodeRadius":{"show":true,"name":"","description":"How large should the largest nodes be? Nodes are sized by how many links they have. The larger a node is the more it will attract other nodes. This can be used to create a good grouping around the most important nodes.","placeholder":""}},"sidebar":{"featureId":"sidebar","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"allowResizing":true,"allowCollapsing":true,"rightDefaultWidth":"20em","leftDefaultWidth":"20em","info_allowResizing":{"show":true,"name":"","description":"Whether or not to allow the sidebars to be resized","placeholder":""},"info_allowCollapsing":{"show":true,"name":"","description":"Whether or not to allow the sidebars to be collapsed","placeholder":""},"info_rightDefaultWidth":{"show":true,"name":"","description":"The default width of the right sidebar","placeholder":""},"info_leftDefaultWidth":{"show":true,"name":"","description":"The default width of the left sidebar","placeholder":""}},"customHead":{"featureId":"custom-head","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"head","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"info_includePath":{"show":false,"name":"","description":"","placeholder":""},"sourcePath":"","info_sourcePath":{"show":true,"name":"","description":"The local path to the source .html file which will be included.","placeholder":"","fileInputOptions":{"makeRelativeToVault":true,"browseButton":true}},"includePath":"site-lib/html/custom-head.html"},"document":{"featureId":"obsidian-document","enabled":true,"unavailable":false,"alwaysEnabled":true,"hideSettingsButton":false,"allowFoldingLists":true,"allowFoldingHeadings":true,"documentWidth":"40em","info_allowFoldingLists":{"show":true,"name":"","description":"Whether or not to allow lists to be folded","placeholder":""},"info_allowFoldingHeadings":{"show":true,"name":"","description":"Whether or not to allow headings to be folded","placeholder":""},"info_documentWidth":{"show":true,"name":"","description":"The width of the document","placeholder":""}},"rss":{"featureId":"rss","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"siteUrl":"","authorName":"","info_siteUrl":{"show":true,"name":"","description":"The url that this site will be hosted at","placeholder":"https://example.com/mysite"},"info_authorName":{"show":true,"name":"","description":"The name of the author of the site","placeholder":""}},"linkPreview":{"featureId":"link-preview","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":true}},"modifiedTime":1768918285150,"siteName":"BachelorThesis","vaultName":"BachelorThesis","exportRoot":"","baseURL":"","pluginVersion":"1.9.2","themeName":"","bodyClasses":"publish css-settings-manager show-inline-title show-ribbon is-focused","hasFavicon":false}