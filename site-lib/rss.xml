<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[BachelorThesis]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>site-lib/media/favicon.png</url><title>BachelorThesis</title><link></link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Tue, 20 Jan 2026 14:11:29 GMT</lastBuildDate><atom:link href="site-lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Tue, 20 Jan 2026 14:11:25 GMT</pubDate><ttl>60</ttl><dc:creator></dc:creator><item><title><![CDATA[REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2505.16792" target="_self">http://arxiv.org/abs/2505.16792</a>
Related:: The authors argue that representation alignment in diffusion training accelerates convergence in early epochs but degrades performance later, so they propose to stop the alignment at a fixed number of iterations.
<br>Space: <a data-href="latent" href="overview/categories/space/latent.html" class="internal-link" target="_self" rel="noopener nofollow">latent</a>
<br>Rep signal: <a data-href="external-vfm" href="overview/categories/rep_signal/external-vfm.html" class="internal-link" target="_self" rel="noopener nofollow">external-vfm</a>
<br>Injection: <a data-href="alignment-loss" href="overview/categories/injection/alignment-loss.html" class="internal-link" target="_self" rel="noopener nofollow">alignment-loss</a> ImageNet: FID 8.9 @ 256x256 px, w/o cfg, 400K training steps, SiT-L as diffusion backbone, DINOv2-B for external representations (see Table 9)
Notes: (optional) Conditioning / representations:
Losses:
Architecture notes:
Training notes:
]]></description><link>overview/papers/2025/wangrepaworksit2025a.html</link><guid isPermaLink="false">Overview/Papers/2025/wangREPAWorksIt2025a.md</guid><pubDate>Tue, 20 Jan 2026 13:50:43 GMT</pubDate></item><item><title><![CDATA[Improving the Diffusability of Autoencoders]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2502.14831" target="_self">http://arxiv.org/abs/2502.14831</a>
Related:: They fine tune the VAE using scale equivariance, such that the latent space has less high-frequency signals, simplifying the training task for the diffusion model.Their key assumption is that diffusion models "synthesize low-frequency components first and add high-frequency ones on top" later, which results in pictures that are perceived as realistic by humans, as we focus more on structure and composition than on high-level details. Consider the per-frequency observation model where denotes an orthonormal transform (e.g. discrete cosine transform (DCT)). The per-frequency SNR now becomesSo in order for the diffusion model to focus on low-frequencies in the high noise regime one needs to be very large for .
In experiments they find that the frequency spectrum in the RGB domain resembles that structure, whereas in modern VAEs (e.g. FluxAE) a lot more energy is in the high-frequency domain (see Figure 4).
They argue that downsampling removes high-frequency components and thus propose to regularize the model via where and are the downsampled versions of the input image and the latent respectively and is a distance metric ( + LPIPS). They refer to this regularization as Scale Equivariance Regularization.
<br>Space: <a data-href="latent" href="overview/categories/space/latent.html" class="internal-link" target="_self" rel="noopener nofollow">latent</a>
<br>Rep signal: <a data-tooltip-position="top" aria-label="Overview/Categories/rep_signal/none" data-href="Overview/Categories/rep_signal/none" href="overview/categories/rep_signal/none.html" class="internal-link" target="_self" rel="noopener nofollow">none</a>
<br>Injection: <a data-tooltip-position="top" aria-label="Overview/Categories/injection/none" data-href="Overview/Categories/injection/none" href="overview/categories/injection/none.html" class="internal-link" target="_self" rel="noopener nofollow">none</a> ImageNet: FID 9.61 @ 256x256 px, w/o cfg, 400K training steps, DiT-L as diffusion backbone They don't provide rigorous results that characterize good latent spaces based on spectral decomposition They have a non-negligible amount of FLOP overhead ( of the total FLOPs for regularization (see openreview Rebuttal to Reviewer JqrQ)) and it makes training more complex with additional hyperparameters Conditioning / representations:
Losses:
Architecture notes:
Training notes:
]]></description><link>overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html</link><guid isPermaLink="false">Overview/Papers/2025/skorokhodovImprovingDiffusabilityAutoencoders2025.md</guid><pubDate>Tue, 20 Jan 2026 13:48:04 GMT</pubDate></item><item><title><![CDATA[Boosting Generative Image Modeling via Joint Image-Feature Synthesis]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2504.16064" target="_self">http://arxiv.org/abs/2504.16064</a>
Related:: They propose ReDi, a generative framework, in which the diffusion model jointly learns the latents of a VAE and the latents of a VFM.With ReDi, they jointly model (i) VAE latents and (ii) the latents of a pretrained VFM (in their experiments, they use DINOv2-B).
They consider two approaches to feed the VAE latents and the visual representation tokens to the SiT/DiT:
Merged Tokens: Here both token are transformed separately to the same dimension and then summed channel-wise to obtain Separate Tokens: Tokens are just concatenated along the sequence dimension $$
\mathbf{h}t=\left[\mathbf{x}_t \mathbf{W}{\mathrm{emb}}^x, \mathbf{z}t \mathbf{W}{\mathrm{emb}}^z\right] \in \mathbb{R}^{2 L \times C_d}. <br>Space: <a data-href="latent" href="overview/categories/space/latent.html" class="internal-link" target="_self" rel="noopener nofollow">latent</a>
<br>Rep signal: <a data-href="external-vfm" href="overview/categories/rep_signal/external-vfm.html" class="internal-link" target="_self" rel="noopener nofollow">external-vfm</a>
<br>Injection: <a data-href="tokenizer" href="overview/categories/injection/tokenizer.html" class="internal-link" target="_self" rel="noopener nofollow">tokenizer</a> ImageNet: FID 9.4 @ 256x256 px, w/o cfg, 400K training steps, SiT-L as diffusion backbone and DINOv2-B for external representations (see Table 1)
Notes: (optional) They don't provide explanations of why
(i) this approach works well (/better than REPA)
(ii) they choose DINOv2 and why other VFMs perform worse (see openreview W2 in rebuttal to Reviewer 8DbC20)
How does it scale to higher resolutions (e.g. )?
Missing understanding for why REPA and ReDi is complementary Conditioning / representations:
Losses:
Architecture notes:
Training notes:
]]></description><link>overview/papers/2025/kouzelisboostinggenerativeimage2025.html</link><guid isPermaLink="false">Overview/Papers/2025/kouzelisBoostingGenerativeImage2025.md</guid><pubDate>Tue, 20 Jan 2026 13:47:23 GMT</pubDate></item><item><title><![CDATA[What matters for Representation Alignment: Global Information or Spatial Structure?]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2512.10794" target="_self">http://arxiv.org/abs/2512.10794</a>
<br>Related:: <a data-tooltip-position="top" aria-label="Overview/Papers/2024/yuRepresentationAlignmentGeneration2025" data-href="Overview/Papers/2024/yuRepresentationAlignmentGeneration2025" href="overview/papers/2024/yurepresentationalignmentgeneration2025.html" class="internal-link" target="_self" rel="noopener nofollow">yuRepresentationAlignmentGeneration2025</a>
Their method iREPA tweaks REPA by replacing the projection MLP with a conv layer and adding spatial normalization to better preserve “spatial structure” during teacher-to-diffusion feature alignment.In experiments across 27 VFMs, they find that linear probing accuracy on the hidden representations correlates weaker () with the gFID than the spatial structure (). Linear probing accuracy is measured via ImageNet-1k accuracy, whereas spatial structure is measured via a local vs. distant self-similarity metric between patch tokens They explain that using larger VFMs for alignment as well as including the CLS token can lead to worse generation quality due to worse spatial structure.
Using a convolution layer for the projection of the VFM latents they focus alignment more on spatial structure. Furthermore, they
<br>Space: <a data-href="Overview/Categories/space/flexible" href="overview/categories/space/flexible.html" class="internal-link" target="_self" rel="noopener nofollow">Overview/Categories/space/flexible</a>
<br>Rep signal: <a data-href="external-vfm" href="overview/categories/rep_signal/external-vfm.html" class="internal-link" target="_self" rel="noopener nofollow">external-vfm</a>
<br>Injection: <a data-href="alignment-loss" href="overview/categories/injection/alignment-loss.html" class="internal-link" target="_self" rel="noopener nofollow">alignment-loss</a> ImageNet: FID 7.52 @ 256x256 px, w/o cfg, 400K training steps, DINOv2, SiT-XL/2 (see Table 6, Appendix E) <br>Result is grounded in REPA setup, i.e. it relies on the alignment-loss to improve representation quality -&gt; Does that scale to other setups, e.g. RAE <a data-tooltip-position="top" aria-label="Overview/Papers/2025/chenDiffusionAutoencodersAre2025" data-href="Overview/Papers/2025/chenDiffusionAutoencodersAre2025" href="overview/papers/2025/chendiffusionautoencodersare2025.html" class="internal-link" target="_self" rel="noopener nofollow">chenDiffusionAutoencodersAre2025</a>? No explanation of why the spatial structure is more important At higher resolution of 512x512 px the difference in FID between REPA and iREPA seems to become smaller as training steps increase -&gt; why? Conditioning / representations:
Losses:
Architecture notes:
Training notes:
]]></description><link>overview/papers/2025/singhwhatmattersrepresentation2025.html</link><guid isPermaLink="false">Overview/Papers/2025/singhWhatMattersRepresentation2025.md</guid><pubDate>Tue, 20 Jan 2026 12:54:08 GMT</pubDate></item><item><title><![CDATA[Diffusion Transformers with Representation Autoencoders]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2510.11690" target="_self">http://arxiv.org/abs/2510.11690</a>
Related:: This paper proposes to train the diffusion model in the latent space of a VFM instead of a VAE for improved generation quality.They want to simplify the training procedure, not by guiding the diffusion model via an external model (e.g. REPA) but by learning the latent of a strong VFM. This reduces hyperparameters and simplifies the architecture.
To decode the latents, they train a ViT with their objective as where and denote the encoder (VFM, precisely DINOv2-B/SigLIP2-B and MAE-B) and the decoder (ViT-XL) respectively. They get competitive/better reconstruction FID scores with RAEs compared to typical SD-VAE (see Table 1). To make the RAE decoder more robust, they train it on a smoothed distribution (noise-augmented decoding), effectively learning to decode where denotes the training set processed by the RAE encoder. This enables the decoder to perform well during the generative part.
They further show that the unique minimizer of the flow matching loss is representable via a stack of standard DiT-Block iff , where is the dimension of the tokens. They argue that this is due to the full rank of the noise (see also Back To Basics).<br>
Instead of scaling the width of all the DiT blocks, which is computationally prohibitive, they use a wide DDT (<a rel="noopener nofollow" class="external-link is-unresolved" href="https://arxiv.org/pdf/2504.05741v1" target="_self">https://arxiv.org/pdf/2504.05741v1</a>) head after the standard DiT.<br>
Building on (<a rel="noopener nofollow" class="external-link is-unresolved" href="https://arxiv.org/pdf/2403.03206" target="_self">https://arxiv.org/pdf/2403.03206</a>) they propose a schedule shift but using the effective data dimension computed as "number of tokens times their dimensionality".
<br>Space: <a data-href="space/" href=".html" class="internal-link" target="_self" rel="noopener nofollow">space/</a>
<br>Rep signal: <a data-href="rep_signal/" href=".html" class="internal-link" target="_self" rel="noopener nofollow">rep_signal/</a>
<br>Injection: <a data-href="injection/" href=".html" class="internal-link" target="_self" rel="noopener nofollow">injection/</a> ImageNet: FID 2.16 @ 256x256 px, w/o cfg, 80 epochs (should be 400K training steps), LightningDiT-XL as diffusion backbone and train on the latents of DINOv2-B Notes: (optional) How does the autoencoder part RAE work on large scale (e.g. LAION dataset)?
They use DINOv2 as it works best in the generative part, however they don't provide any explanation for that. What makes their features good for representation? (compare this to Diffusability, MAETok Theorem 1 and What matter) Conditioning / representations:
Losses:
Architecture notes:
Training notes:
]]></description><link>overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html</link><guid isPermaLink="false">Overview/Papers/2025/zhengDiffusionTransformersRepresentation2025.md</guid><pubDate>Tue, 20 Jan 2026 09:32:21 GMT</pubDate></item><item><title><![CDATA[Diffusion Autoencoders are Scalable Image Tokenizers]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2501.18593" target="_self">http://arxiv.org/abs/2501.18593</a>
Related:: They replace the VAE component with a diffusion model that is conditioned on the latent representation of an encoder to simplify the learning procedure and not rely on external models (GAN, LPIPS loss in standard VAE).They motivate their work by noting that the VAE component used in modern latent diffusion models is based on several "heuristic" losses (LPIPS, GAN and reconstruction loss). They propose to instead use a diffusion loss to learn both the encoder and decoder jointly. They condition their diffusion decoder (U-Net) by simply concatenating it to the input, i.e. where is the noised input and is the output of the encoder (during inference the output of the latent diffusion model; transformed to correct resolution via nearest neighbors if latent resolution differs).
<br>Space: <a data-href="latent" href="overview/categories/space/latent.html" class="internal-link" target="_self" rel="noopener nofollow">latent</a>
<br>Rep signal: <a data-href="Overview/Categories/rep_signal/none" href="overview/categories/rep_signal/none.html" class="internal-link" target="_self" rel="noopener nofollow">Overview/Categories/rep_signal/none</a>
<br>Injection: <a data-href="injection/none" href="overview/categories/injection/none.html" class="internal-link" target="_self" rel="noopener nofollow">injection/none</a> ImageNet: FID 6.29 @ 256x256 px, w cfg of 2, DiT-XL/2 as diffusion backbone, !number of training steps is not reported
Notes: (optional) They have very limited quantitative results, particularly in the generative domain Conditioning / representations:
Losses:
Architecture notes:
Training notes:
]]></description><link>overview/papers/2025/chendiffusionautoencodersare2025.html</link><guid isPermaLink="false">Overview/Papers/2025/chenDiffusionAutoencodersAre2025.md</guid><pubDate>Tue, 20 Jan 2026 09:09:49 GMT</pubDate></item><item><title><![CDATA[All-Papers]]></title><description><![CDATA[Note that the imagenet_fid score is computed on ImageNet 256x256 resolution, w/o classifier free guidance, after 400K training steps, using a SiT, if not marked with a "" at the end of the score. For details on the setup for the FID with "" click onto the respective summary page.]]></description><link>overview/maps/all-papers.html</link><guid isPermaLink="false">Overview/Maps/All-Papers.md</guid><pubDate>Tue, 20 Jan 2026 08:22:10 GMT</pubDate></item><item><title><![CDATA[REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2504.10483" target="_self">http://arxiv.org/abs/2504.10483</a>
Related:: Building on REPA, the authors propose an end-to-end training procedure, where the VAE component is optimized during training by backpropagating the REPA loss. REPA-E aims to improve and simplify the LDM training procedure, by – instead of having a two stage procedure – using an end-to-end pipeline. They find that simply backpropagating the diffusion loss to the VAE is ineffective and leads to a simple latent space of the VAE (see Table 1). In this case the space is easier to denoise, but results in bad generation performance.
To mitigate this collapse, they use a VFM as an anchor and backpropagate the REPA loss. Their overall loss is of the form where refer to the parameters for the LDM, VAE and trainable REPA projection layer, respectively. They propose to normalize the latent space using a BatchNorm layer before passing the VAEs output to the SiT component.
In their standard procedure they leverage weights of a pretrained VAE, however they show that training the VAE and the Diffusion Model in an end-to-end fashion manner performs on a very similar level (see Table 8).
<br>Space: <a data-href="latent" href="overview/categories/space/latent.html" class="internal-link" target="_self" rel="noopener nofollow">latent</a>
<br>Rep signal: <a data-href="external-vfm" href="overview/categories/rep_signal/external-vfm.html" class="internal-link" target="_self" rel="noopener nofollow">external-vfm</a>
<br>Injection: <a data-href="alignment-loss" href="overview/categories/injection/alignment-loss.html" class="internal-link" target="_self" rel="noopener nofollow">alignment-loss</a> ImageNet: FID 4.07 @ 256x256 px, w/o cfg, 400K training steps, DINOv2-B if external, SiT-XL (see Figure 2b) Conditioning / representations:
Losses:
Architecture notes:
Training notes:
]]></description><link>overview/papers/2025/lengrepaeunlockingvae2025a.html</link><guid isPermaLink="false">Overview/Papers/2025/lengREPAEUnlockingVAE2025a.md</guid><pubDate>Mon, 19 Jan 2026 16:34:43 GMT</pubDate></item><item><title><![CDATA[none]]></title><link>overview/categories/injection/none.html</link><guid isPermaLink="false">Overview/Categories/injection/none.md</guid><pubDate>Mon, 19 Jan 2026 16:21:27 GMT</pubDate></item><item><title><![CDATA[Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2410.06940" target="_self">http://arxiv.org/abs/2410.06940</a>
Related:: They add a representation alignment loss that aligns the diffusion models hidden representations to those of a vision foundation model (VFM) to speed up DiT/SiT training.They motivate their work by previous findings that better diffusion models learn hidden representations are semantically more meaningful (higher linear probing accuracy). Representations are already weakly aligned (measured via CKNNA) with those of DINOv2. To improve alignment they add an alignment loss to their overall lossHere denotes the patch index, the hidden representation at a fixed layer, the VFMs representation and a similarity function, e.g. cosine similarity. is a 3-layer MLP that is learned during training. They find that alignment at earlier layers (e.g. 8 out of 24 layers) yields best results.
The overall loss becomes where is a hyperparameter used to control the amount of alignment.
<br>Space: <a data-href="Overview/Categories/space/flexible" href="overview/categories/space/flexible.html" class="internal-link" target="_self" rel="noopener nofollow">Overview/Categories/space/flexible</a>
<br>Rep signal: <a data-href="external-vfm" href="overview/categories/rep_signal/external-vfm.html" class="internal-link" target="_self" rel="noopener nofollow">external-vfm</a>
<br>Injection: <a data-href="alignment-loss" href="overview/categories/injection/alignment-loss.html" class="internal-link" target="_self" rel="noopener nofollow">alignment-loss</a> <br>ImageNet: FID 7.9 @ 256x256 px, w/o cfg, 400K training steps, DINOv2-B, SiT-XL/2 (see Table 8 in <a data-tooltip-position="top" aria-label="Overview/Papers/2025/singhWhatMattersRepresentation2025" data-href="Overview/Papers/2025/singhWhatMattersRepresentation2025" href="overview/papers/2025/singhwhatmattersrepresentation2025.html" class="internal-link" target="_self" rel="noopener nofollow">singhWhatMattersRepresentation2025</a>) <br>Why are earlier layers more suitable for alignment? They argue that later layers focus on high-frequency details, but <a data-tooltip-position="top" aria-label="Overview/Papers/2025/singhWhatMattersRepresentation2025" data-href="Overview/Papers/2025/singhWhatMattersRepresentation2025" href="overview/papers/2025/singhwhatmattersrepresentation2025.html" class="internal-link" target="_self" rel="noopener nofollow">singhWhatMattersRepresentation2025</a> use early layers (e.g. 4, 6 or 8, see Table 1c) as well, who don't focus on semantic/high-level structure.
They focus mainly on LDM (SiT is their base backbone model) - more extensive results in pixel diffusion might be interesting. Conditioning / representations:
Losses:
Architecture notes:
Training notes:
]]></description><link>overview/papers/2024/yurepresentationalignmentgeneration2025.html</link><guid isPermaLink="false">Overview/Papers/2024/yuRepresentationAlignmentGeneration2025.md</guid><pubDate>Mon, 19 Jan 2026 16:13:59 GMT</pubDate></item><item><title><![CDATA[flexible]]></title><link>overview/categories/objective/flexible.html</link><guid isPermaLink="false">Overview/Categories/objective/flexible.md</guid><pubDate>Mon, 19 Jan 2026 16:13:31 GMT</pubDate></item><item><title><![CDATA[flow-matching]]></title><link>overview/categories/objective/flow-matching.html</link><guid isPermaLink="false">Overview/Categories/objective/flow-matching.md</guid><pubDate>Mon, 19 Jan 2026 16:07:24 GMT</pubDate></item><item><title><![CDATA[two-stage]]></title><link>overview/categories/training/two-stage.html</link><guid isPermaLink="false">Overview/Categories/training/two-stage.md</guid><pubDate>Mon, 19 Jan 2026 10:35:13 GMT</pubDate></item><item><title><![CDATA[end-to-end]]></title><link>overview/categories/training/end-to-end.html</link><guid isPermaLink="false">Overview/Categories/training/end-to-end.md</guid><pubDate>Mon, 19 Jan 2026 10:35:08 GMT</pubDate></item><item><title><![CDATA[flexible]]></title><link>overview/categories/space/flexible.html</link><guid isPermaLink="false">Overview/Categories/space/flexible.md</guid><pubDate>Mon, 19 Jan 2026 10:17:52 GMT</pubDate></item><item><title><![CDATA[tokenizer]]></title><link>overview/categories/injection/tokenizer.html</link><guid isPermaLink="false">Overview/Categories/injection/tokenizer.md</guid><pubDate>Mon, 19 Jan 2026 10:17:34 GMT</pubDate></item><item><title><![CDATA[joint-modeling]]></title><link>overview/categories/injection/joint-modeling.html</link><guid isPermaLink="false">Overview/Categories/injection/joint-modeling.md</guid><pubDate>Mon, 19 Jan 2026 10:17:28 GMT</pubDate></item><item><title><![CDATA[alignment-loss]]></title><link>overview/categories/injection/alignment-loss.html</link><guid isPermaLink="false">Overview/Categories/injection/alignment-loss.md</guid><pubDate>Mon, 19 Jan 2026 10:17:21 GMT</pubDate></item><item><title><![CDATA[none]]></title><link>overview/categories/rep_signal/none.html</link><guid isPermaLink="false">Overview/Categories/rep_signal/none.md</guid><pubDate>Mon, 19 Jan 2026 10:17:01 GMT</pubDate></item><item><title><![CDATA[internal]]></title><link>overview/categories/rep_signal/internal.html</link><guid isPermaLink="false">Overview/Categories/rep_signal/internal.md</guid><pubDate>Mon, 19 Jan 2026 10:16:48 GMT</pubDate></item><item><title><![CDATA[external-vfm]]></title><link>overview/categories/rep_signal/external-vfm.html</link><guid isPermaLink="false">Overview/Categories/rep_signal/external-vfm.md</guid><pubDate>Mon, 19 Jan 2026 10:15:59 GMT</pubDate></item><item><title><![CDATA[pixel]]></title><link>overview/categories/space/pixel.html</link><guid isPermaLink="false">Overview/Categories/space/pixel.md</guid><pubDate>Fri, 16 Jan 2026 15:38:56 GMT</pubDate></item><item><title><![CDATA[latent]]></title><link>overview/categories/space/latent.html</link><guid isPermaLink="false">Overview/Categories/space/latent.md</guid><pubDate>Fri, 16 Jan 2026 15:38:44 GMT</pubDate></item><item><title><![CDATA[README]]></title><link>overview/readme.html</link><guid isPermaLink="false">Overview/README.md</guid><pubDate>Fri, 16 Jan 2026 14:51:26 GMT</pubDate></item></channel></rss>