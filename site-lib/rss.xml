<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[RepresentationDiffusion]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>site-lib/media/favicon.png</url><title>RepresentationDiffusion</title><link></link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Thu, 22 Jan 2026 16:42:43 GMT</lastBuildDate><atom:link href="site-lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Thu, 22 Jan 2026 16:42:34 GMT</pubDate><ttl>60</ttl><dc:creator></dc:creator><item><title><![CDATA[README]]></title><description><![CDATA[This repository contains a systematic (but non-exhaustive) review of recent developments in generative modeling, specifically focusing on the integration of representation learning within diffusion and flow-matching frameworks in the image domain.The goal of this overview is to understand the key design choices people have used, compare and classify work as well as get a better high level perspective on the field. There is a lot more great works that are still missing in this review and I might include them as we move forward/if we decide it might be helpful. The review is structured into two primary components:High-level tools for comparative analysis: <a data-tooltip-position="top" aria-label="VisualOverview.canvas" data-href="VisualOverview.canvas" href="overview/maps/visualoverview.html" class="internal-link" target="_self" rel="noopener nofollow">VisualOverview</a>: A mindmap showing the central idea of the work, a key figure of the paper, a link to a more in depth summary of the paper as well as FID score and simple categorization. üìçStart here, go to summaries from there
<br><a data-href="DirectionsAndQuestions" href="overview/maps/directionsandquestions.html" class="internal-link" target="_self" rel="noopener nofollow">DirectionsAndQuestions</a>: Current developments as well as a list of potential theoretical and empirical questions one could explore. I tried to move away from concrete engineering to a more abstract maths perspective here. ‚è≠Ô∏è Continue here -&gt; feedback &amp; ideas highly appreciated
<br><a data-href="All-Papers" href="overview/maps/all-papers.html" class="internal-link" target="_self" rel="noopener nofollow">All-Papers</a>: A table of all reviewed works. Individual paper analyses organized by publication year (though most were published in 2025) Content Structure: Each entry follows the same structure: One-liner conveys the key idea/improvement proposed in the paper Brief summary goes more in depth and gives more technical details.
Key design choices categorizes the paper into: <br>Space: Domain in which the model operates (<a data-href="pixel" href="overview/categories/space/pixel.html" class="internal-link" target="_self" rel="noopener nofollow">pixel</a>| <a data-href="latent" href="overview/categories/space/latent.html" class="internal-link" target="_self" rel="noopener nofollow">latent</a> | <a data-tooltip-position="top" aria-label="Overview/Categories/space/flexible" data-href="Overview/Categories/space/flexible" href="overview/categories/space/flexible.html" class="internal-link" target="_self" rel="noopener nofollow">flexible</a>)
<br>Rep Signal: Signal that is used to guide the model (<a data-href="external-vfm" href="overview/categories/rep_signal/external-vfm.html" class="internal-link" target="_self" rel="noopener nofollow">external-vfm</a> | <a data-href="internal" href="overview/categories/rep_signal/internal.html" class="internal-link" target="_self" rel="noopener nofollow">internal</a> | <a data-tooltip-position="top" aria-label="Overview/Categories/rep_signal/none" data-href="Overview/Categories/rep_signal/none" href="overview/categories/rep_signal/none.html" class="internal-link" target="_self" rel="noopener nofollow">none</a>)
<br>Injection: How external signal is used for the model (<a data-href="alignment-loss" href="overview/categories/injection/alignment-loss.html" class="internal-link" target="_self" rel="noopener nofollow">alignment-loss</a> | <a data-href="joint-modeling" href="overview/categories/injection/joint-modeling.html" class="internal-link" target="_self" rel="noopener nofollow">joint-modeling</a> | <a data-href="tokenizer" href="overview/categories/injection/tokenizer.html" class="internal-link" target="_self" rel="noopener nofollow">tokenizer</a> | <a data-tooltip-position="top" aria-label="Overview/Categories/injection/none" data-href="Overview/Categories/injection/none" href="overview/categories/injection/none.html" class="internal-link" target="_self" rel="noopener nofollow">none</a>)
<br>Training: training-regime (<a data-href="end-to-end" href="overview/categories/training/end-to-end.html" class="internal-link" target="_self" rel="noopener nofollow">end-to-end</a> | <a data-href="two-stage" href="overview/categories/training/two-stage.html" class="internal-link" target="_self" rel="noopener nofollow">two-stage</a>) Results FID score and training setting
if not marked via "*" the model was trained on 400k iterations, batchsize 256, on pixels, using SiT-L as backbone, no cfg and DINOv2-B representations if any are used; while I tried to make it as comparable as possible, still some unaligned hyperparameters remain, so
FID score is not very meaningful Limitations/open questions give an overview over the methods limitations and open questions, sometimes in context with other works Here, one finds the categories mentioned in key design choices. Each note contains backlinks to the papers that use that design choice.<br>
So for example, if you're interested in papers that work in pixel-space, go to <a data-href="pixel" href="overview/categories/space/pixel.html" class="internal-link" target="_self" rel="noopener nofollow">pixel</a> and browse through the paper summaries that are backlinked there.]]></description><link>overview/readme.html</link><guid isPermaLink="false">Overview/README.md</guid><pubDate>Thu, 22 Jan 2026 16:42:13 GMT</pubDate></item><item><title><![CDATA[DirectionsAndQuestions]]></title><description><![CDATA[In the following I try to structure my perspective on what approaches for improving the way diffusion models learn can try. I aimed to put it into a more high level/mathematical perspective which is further away from the actual implementation design choices used.
I also formulated some ideas and questions: I hope this gives us a good reference point to discuss where this project is headed more precisely. I would be very happy on feedback and potential ideas from your side ü§ó Structured to be easily referenced :)I am aware that some of the motivations/goals are hard to frame mathematically, e.g. pixel-space diffusion might be preferable over latent-space diffusion, simplify since this simplifies the architecture and reduces the number of external models required etc.I am most comfortable with the flow-matching framework, so everything is written from that perspective.A: Problem: What properties of the target distribution (or a coupling with the base (gaussian) distribution) allow for a easily learnable velocity field? (simplify training)
smooth conditional velocity field (e.g. small Lipschitz constant)? (simplify inference, since less NFEs needed later on)
B: Theoretical questions/ideas:
Can we obtain sample complexity bounds for the diffusion training as a function of the target distribution? -&gt; simplified result (target distribution is GMM with uniform weights and identity covariance matrices) that exists here is Theorem 2.1 in MAETok paper (see <a rel="noopener nofollow" class="external-link is-unresolved" href="https://arxiv.org/pdf/2502.03444#page=2.60" target="_self">https://arxiv.org/pdf/2502.03444#page=2.60</a>)
Can we get smoothness guarantees (Lipschitz bounds) for the optimal velocity field based on properties on or a coupling ?
C: Empirical questions/ideas:
<br>Measure variance of the conditional velocity field , where randomness comes from , to understand how hard the learning problem is (model essentially has to average out directions if variance is high (idea is similar to flow-matching vs rectified flows trajectories, see Paths Crossed at the Wrong Time part in <a rel="noopener nofollow" class="external-link is-unresolved" href="https://alechelbling.com/blog/rectified-flow/#:~:text=with%20fewer%20crossings.-,Paths%20Crossed%20at%20the%20Wrong%20Time,-Our%20learned%20flow" target="_self">https://alechelbling.com/blog/rectified-flow/#:~:text=with%20fewer%20crossings.-,Paths%20Crossed%20at%20the%20Wrong%20Time,-Our%20learned%20flow</a>))
<br>Compute empirical statistics on curvature of solution trajectories when diffusion model is learned on specialized latents (e.g. <a data-tooltip-position="top" aria-label="Overview/Papers/2025/lengREPAEUnlockingVAE2025a" data-href="Overview/Papers/2025/lengREPAEUnlockingVAE2025a" href="overview/papers/2025/lengrepaeunlockingvae2025a.html" class="internal-link" target="_self" rel="noopener nofollow">REPA-E</a>, <a data-tooltip-position="top" aria-label="Overview/Papers/2025/zhengDiffusionTransformersRepresentation2025" data-href="Overview/Papers/2025/zhengDiffusionTransformersRepresentation2025" href="overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html" class="internal-link" target="_self" rel="noopener nofollow">RAE</a> or <a data-tooltip-position="top" aria-label="Overview/Papers/2025/skorokhodovImprovingDiffusabilityAutoencoders2025" data-href="Overview/Papers/2025/skorokhodovImprovingDiffusabilityAutoencoders2025" href="overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html" class="internal-link" target="_self" rel="noopener nofollow">FT-SE</a>) vs classic SD-VAE
A: Problem: How does modeling the joint velocity field of images and representations reduce the complexity of learning the marginal dynamics?B: Theoretical question/idea:
If the marginal velocity field (just learning to denoise the (semantic) VFM-based token) is simple to learn and mutual information between and is large, does that make it easier (to be quantified) to learn than just the marginal velocity field ?
C: Empirical question/idea:
Compare isolated joint-modeling (e.g. REG/ReDi w/o REPA, using different approaches to obtain the additional token) with baseline SiT on convergence speed, A: Problem: How does an additional alignment loss affect the learning problem?B: Theoretical questions/ideas:
Can we get bounds on the distance between learned distribution and target distribution that are dependent on the quality of the representations?<br>
-&gt; <a rel="noopener nofollow" class="external-link is-unresolved" href="https://arxiv.org/abs/2507.08980" target="_self">https://arxiv.org/abs/2507.08980</a> provides a TV bound on learned distribution and target distribution derived from the DDPM framework (see Theorem 1 in REED <a rel="noopener nofollow" class="external-link is-unresolved" href="https://arxiv.org/pdf/2507.08980)" target="_self">https://arxiv.org/pdf/2507.08980)</a>; could we make the bound more explicit on the quality of the representation (e.g. bound their by a function of )?
How does the alignment loss modify the loss landscape, e.g. improve smoothness?
C: Empirical ideas:
<br>Investigate gradient similarity between and during training -&gt; essentially this idea was covered in <a data-tooltip-position="top" aria-label="wangREPAWorksIt2025a" data-href="wangREPAWorksIt2025a" href="overview/papers/2025/wangrepaworksit2025a.html" class="internal-link" target="_self" rel="noopener nofollow">HASTE</a> but could be investigated more thoroughly for different types of alignment losses
Compute statistics of Hessians (e.g. condition number) of and and compare them -&gt; information on loss landscape
]]></description><link>overview/maps/directionsandquestions.html</link><guid isPermaLink="false">Overview/Maps/DirectionsAndQuestions.md</guid><pubDate>Thu, 22 Jan 2026 16:41:38 GMT</pubDate></item><item><title><![CDATA[All-Papers]]></title><description><![CDATA[Note that the imagenet_fid score is computed on ImageNet 256x256 resolution, w/o classifier free guidance, after 400K training steps, using a SiT, if not marked with a "*" at the end of the score. For details on the setup for the FID with "*" click onto the respective summary page and go to Results.]]></description><link>overview/maps/all-papers.html</link><guid isPermaLink="false">Overview/Maps/All-Papers.md</guid><pubDate>Thu, 22 Jan 2026 14:22:47 GMT</pubDate></item><item><title><![CDATA[Diffusion Autoencoders are Scalable Image Tokenizers]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2501.18593" target="_self">http://arxiv.org/abs/2501.18593</a>
Related:: They replace the VAE component with a diffusion model that is conditioned on the latent representation of an encoder to simplify the learning procedure and not rely on external models (GAN, LPIPS loss in standard VAE).They motivate their work by noting that the VAE component used in modern latent diffusion models is based on several "heuristic" losses (LPIPS, GAN and reconstruction loss). They propose to instead use a diffusion loss to learn both the encoder and decoder jointly. They condition their diffusion decoder (U-Net) by simply concatenating it to the input, i.e. where is the noised input and is the output of the encoder (during inference the output of the latent diffusion model; transformed to correct resolution via nearest neighbors if latent resolution differs).
<br>Space: <a data-href="latent" href="overview/categories/space/latent.html" class="internal-link" target="_self" rel="noopener nofollow">latent</a>
<br>Rep signal: <a data-href="Overview/Categories/rep_signal/none" href="overview/categories/rep_signal/none.html" class="internal-link" target="_self" rel="noopener nofollow">Overview/Categories/rep_signal/none</a>
<br>Injection: <a data-href="injection/none" href="overview/categories/injection/none.html" class="internal-link" target="_self" rel="noopener nofollow">injection/none</a>
<br>Training: <a data-href="end-to-end" href="overview/categories/training/end-to-end.html" class="internal-link" target="_self" rel="noopener nofollow">end-to-end</a> ImageNet: FID 6.29 @ 256x256 px, w cfg of 2, DiT-XL/2 as diffusion backbone, !number of training steps is not reported They have very limited quantitative results, particularly in the generative domain
]]></description><link>overview/papers/2025/chendiffusionautoencodersare2025.html</link><guid isPermaLink="false">Overview/Papers/2025/chenDiffusionAutoencodersAre2025.md</guid><pubDate>Thu, 22 Jan 2026 09:47:44 GMT</pubDate></item><item><title><![CDATA[Boosting Generative Image Modeling via Joint Image-Feature Synthesis]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2504.16064" target="_self">http://arxiv.org/abs/2504.16064</a>
<br>Related:: <a data-href="wuRepresentationEntanglementGeneration2025" href="overview/papers/2025/wurepresentationentanglementgeneration2025.html" class="internal-link" target="_self" rel="noopener nofollow">wuRepresentationEntanglementGeneration2025</a> They propose ReDi, a generative framework, in which the diffusion model jointly learns the latents of a VAE and the latents of a VFM.With ReDi, they jointly model (i) VAE latents and (ii) the latents of a pretrained VFM (in their experiments, they use DINOv2-B).
They consider two approaches to feed the VAE latents and the visual representation tokens to the SiT/DiT:
Merged Tokens: Here both token are transformed separately to the same dimension and then summed channel-wise to obtain Separate Tokens: Tokens are just concatenated along the sequence dimension They use the merged token approach by default to reduce computational complexity.
To bring VAE latents and VFM latents to similar channel sizes, they apply PCA on the VFM latents. Furthermore, they apply representation guidance during inference to control the influence of the VFMs representations, which is similarly designed to CFG.
By combining ReDi with REPA, they are able to improve convergence speed and achievable FID.
<br>Space: <a data-tooltip-position="top" aria-label="Overview/Categories/space/flexible" data-href="Overview/Categories/space/flexible" href="overview/categories/space/flexible.html" class="internal-link" target="_self" rel="noopener nofollow">flexible</a>
<br>Rep signal: <a data-href="external-vfm" href="overview/categories/rep_signal/external-vfm.html" class="internal-link" target="_self" rel="noopener nofollow">external-vfm</a>
<br>Injection: <a data-href="tokenizer" href="overview/categories/injection/tokenizer.html" class="internal-link" target="_self" rel="noopener nofollow">tokenizer</a>
<br>Training: <a data-href="two-stage" href="overview/categories/training/two-stage.html" class="internal-link" target="_self" rel="noopener nofollow">two-stage</a> ImageNet: FID 9.4 @ 256x256 px, w/o cfg, 400K training steps, SiT-L as diffusion backbone and DINOv2-B for external representations (see Table 1) They don't provide explanations of why
(i) this approach works well (/better than REPA)
(ii) they choose DINOv2 and why other VFMs perform worse (see openreview W2 in rebuttal to Reviewer 8DbC20)
How does it scale to higher resolutions (e.g. )?
Missing understanding for why REPA and ReDi is complementary
]]></description><link>overview/papers/2025/kouzelisboostinggenerativeimage2025.html</link><guid isPermaLink="false">Overview/Papers/2025/kouzelisBoostingGenerativeImage2025.md</guid><pubDate>Thu, 22 Jan 2026 09:47:32 GMT</pubDate></item><item><title><![CDATA[REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2504.10483" target="_self">http://arxiv.org/abs/2504.10483</a>
Related:: Building on REPA, the authors propose an end-to-end training procedure, where the VAE component is optimized during training by backpropagating the REPA loss. REPA-E aims to improve and simplify the LDM training procedure, by ‚Äì instead of having a two stage procedure ‚Äì using an end-to-end pipeline. They find that simply backpropagating the diffusion loss to the VAE is ineffective and leads to a simple latent space of the VAE (see Table 1). In this case the space is easier to denoise, but results in bad generation performance.
To mitigate this collapse, they use a VFM as an anchor and backpropagate the REPA loss. Their overall loss is of the form where refer to the parameters for the LDM, VAE and trainable REPA projection layer, respectively. They propose to normalize the latent space using a BatchNorm layer before passing the VAEs output to the SiT component.
In their standard procedure they leverage weights of a pretrained VAE, however they show that training the VAE and the Diffusion Model in an end-to-end fashion manner performs on a very similar level (see Table 8).
<br>Space: <a data-href="latent" href="overview/categories/space/latent.html" class="internal-link" target="_self" rel="noopener nofollow">latent</a>
<br>Rep signal: <a data-href="external-vfm" href="overview/categories/rep_signal/external-vfm.html" class="internal-link" target="_self" rel="noopener nofollow">external-vfm</a>
<br>Injection: <a data-href="alignment-loss" href="overview/categories/injection/alignment-loss.html" class="internal-link" target="_self" rel="noopener nofollow">alignment-loss</a>
<br>Training: <a data-href="end-to-end" href="overview/categories/training/end-to-end.html" class="internal-link" target="_self" rel="noopener nofollow">end-to-end</a> ImageNet: FID 4.07 @ 256x256 px, w/o cfg, 400K training steps, DINOv2-B if external, SiT-XL (see Figure 2b) ]]></description><link>overview/papers/2025/lengrepaeunlockingvae2025a.html</link><guid isPermaLink="false">Overview/Papers/2025/lengREPAEUnlockingVAE2025a.md</guid><pubDate>Thu, 22 Jan 2026 09:47:25 GMT</pubDate></item><item><title><![CDATA[Back to Basics: Let Denoising Generative Models Denoise]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2511.13720" target="_self">http://arxiv.org/abs/2511.13720</a>
Related:: The minimizer of and prediction losses have the (high) ambient dimension and hence the diffusion model has a difficult training target. They propose to let the model predict the ground truth during denoising to leverage low dimensionality of the support of the data dimension.Originally diffusion models where trained to predict the clean image from a noised version . The community has shifted to using noise (-prediction) or velocity (-prediction) instead, which the authors explain as likely being "legacy reasons".
From the flow-matching framework they present the relations between and prediction as well as the , and losses. The nine possible combinations (see Table 1) are all not equivalent (pointwise equal everywhere). However, a scalar reweighting can be applied to obtain equivalence.
Motivated by this the Li and He investigate the effect of both loss reweighting and noise-level shifts. While the latter can lead to FID improvement (see Table 3), "catastrophic failure" of and prediction in higher resolution regimes (e.g. px; see Table 2) can not be prevented.
They use aggressive patch sizes (image_size/ 16) and are still able to train their model effectively, which they explain by the low dimensionality of the data. Overall, their architecture is just a DiT with standard timestep and class conditioning (AdaLN-Zero). They only let the model predict and use a velocity-loss formulation.
<br>Space: <a data-href="pixel" href="overview/categories/space/pixel.html" class="internal-link" target="_self" rel="noopener nofollow">pixel</a>
<br>Rep signal: <a data-tooltip-position="top" aria-label="Overview/Categories/rep_signal/none" data-href="Overview/Categories/rep_signal/none" href="overview/categories/rep_signal/none.html" class="internal-link" target="_self" rel="noopener nofollow">none</a>
<br>Injection: <a data-tooltip-position="top" aria-label="Overview/Categories/injection/none" data-href="Overview/Categories/injection/none" href="overview/categories/injection/none.html" class="internal-link" target="_self" rel="noopener nofollow">none</a>
<br>Training: <a data-href="end-to-end" href="overview/categories/training/end-to-end.html" class="internal-link" target="_self" rel="noopener nofollow">end-to-end</a> ImageNet: FID 2.79 @ 256x256 px, w/ cfg, 200 epochs (lowest epoch number they show), JiT-L as diffusion backbone (similar param number as SiT-L) More ablations on training loss convergence against iteration steps would be interesting to compare against other methods (they only report after 200 and 600 epochs) They use several "engineering hacks" such as bottleneck embedding, RoPE/qk-norm, in-context class tokens, dropout/early stop (for H- and G-models) but don't ablate the contribution of those components
]]></description><link>overview/papers/2025/libackbasicslet2025.html</link><guid isPermaLink="false">Overview/Papers/2025/liBackBasicsLet2025.md</guid><pubDate>Thu, 22 Jan 2026 09:47:20 GMT</pubDate></item><item><title><![CDATA[What matters for Representation Alignment: Global Information or Spatial Structure?]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2512.10794" target="_self">http://arxiv.org/abs/2512.10794</a>
<br>Related:: <a data-tooltip-position="top" aria-label="Overview/Papers/2024/yuRepresentationAlignmentGeneration2025" data-href="Overview/Papers/2024/yuRepresentationAlignmentGeneration2025" href="overview/papers/2024/yurepresentationalignmentgeneration2025.html" class="internal-link" target="_self" rel="noopener nofollow">yuRepresentationAlignmentGeneration2025</a>
Their method iREPA tweaks REPA by replacing the projection MLP with a conv layer and adding spatial normalization to better preserve ‚Äúspatial structure‚Äù during teacher-to-diffusion feature alignment.In experiments across 27 VFMs, they find that linear probing accuracy on the hidden representations correlates weaker () with the gFID than the spatial structure (). Linear probing accuracy is measured via ImageNet-1k accuracy, whereas spatial structure is measured via a local vs. distant self-similarity metric between patch tokens They explain that using larger VFMs for alignment as well as including the CLS token can lead to worse generation quality due to worse spatial structure.
Using a convolution layer for the projection of the VFM latents they focus alignment more on spatial structure. Furthermore, they
<br>Space: <a data-tooltip-position="top" aria-label="Overview/Categories/space/flexible" data-href="Overview/Categories/space/flexible" href="overview/categories/space/flexible.html" class="internal-link" target="_self" rel="noopener nofollow">flexible</a>
<br>Rep signal: <a data-href="external-vfm" href="overview/categories/rep_signal/external-vfm.html" class="internal-link" target="_self" rel="noopener nofollow">external-vfm</a>
<br>Injection: <a data-href="alignment-loss" href="overview/categories/injection/alignment-loss.html" class="internal-link" target="_self" rel="noopener nofollow">alignment-loss</a>
<br>Training: <a data-href="two-stage" href="overview/categories/training/two-stage.html" class="internal-link" target="_self" rel="noopener nofollow">two-stage</a> ImageNet: FID 7.52 @ 256x256 px, w/o cfg, 400K training steps, DINOv2, SiT-XL/2 (see Table 6, Appendix E) <br>Result is grounded in REPA setup, i.e. it relies on the alignment-loss to improve representation quality -&gt; Does that scale to other setups, e.g. RAE <a data-tooltip-position="top" aria-label="Overview/Papers/2025/chenDiffusionAutoencodersAre2025" data-href="Overview/Papers/2025/chenDiffusionAutoencodersAre2025" href="overview/papers/2025/chendiffusionautoencodersare2025.html" class="internal-link" target="_self" rel="noopener nofollow">chenDiffusionAutoencodersAre2025</a>? No explanation of why the spatial structure is more important At higher resolution of 512x512 px the difference in FID between REPA and iREPA seems to become smaller as training steps increase -&gt; why?
]]></description><link>overview/papers/2025/singhwhatmattersrepresentation2025.html</link><guid isPermaLink="false">Overview/Papers/2025/singhWhatMattersRepresentation2025.md</guid><pubDate>Thu, 22 Jan 2026 09:47:11 GMT</pubDate></item><item><title><![CDATA[Improving the Diffusability of Autoencoders]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2502.14831" target="_self">http://arxiv.org/abs/2502.14831</a>
Related:: They fine tune the VAE using scale equivariance, such that the latent space has less high-frequency signals, simplifying the training task for the diffusion model.Their key assumption is that diffusion models "synthesize low-frequency components first and add high-frequency ones on top" later, which results in pictures that are perceived as realistic by humans, as we focus more on structure and composition than on high-level details. Consider the per-frequency observation model where denotes an orthonormal transform (e.g. discrete cosine transform (DCT)). The per-frequency SNR now becomesSo in order for the diffusion model to focus on low-frequencies in the high noise regime one needs to be very large for .
In experiments they find that the frequency spectrum in the RGB domain resembles that structure, whereas in modern VAEs (e.g. FluxAE) a lot more energy is in the high-frequency domain (see Figure 4).
They argue that downsampling removes high-frequency components and thus propose to regularize the model via where and are the downsampled versions of the input image and the latent respectively and is a distance metric ( + LPIPS). They refer to this regularization as Scale Equivariance Regularization.
<br>Space: <a data-href="latent" href="overview/categories/space/latent.html" class="internal-link" target="_self" rel="noopener nofollow">latent</a>
<br>Rep signal: <a data-tooltip-position="top" aria-label="Overview/Categories/rep_signal/none" data-href="Overview/Categories/rep_signal/none" href="overview/categories/rep_signal/none.html" class="internal-link" target="_self" rel="noopener nofollow">none</a>
<br>Injection: <a data-tooltip-position="top" aria-label="Overview/Categories/injection/none" data-href="Overview/Categories/injection/none" href="overview/categories/injection/none.html" class="internal-link" target="_self" rel="noopener nofollow">none</a>
<br>Training: <a data-href="two-stage" href="overview/categories/training/two-stage.html" class="internal-link" target="_self" rel="noopener nofollow">two-stage</a> ImageNet: FID 9.61 @ 256x256 px, w/o cfg, 400K training steps, DiT-L as diffusion backbone They don't provide rigorous results that characterize good latent spaces based on spectral decomposition They have a non-negligible amount of FLOP overhead ( of the total FLOPs for regularization (see openreview Rebuttal to Reviewer JqrQ)) and it makes training more complex with additional hyperparameters
]]></description><link>overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html</link><guid isPermaLink="false">Overview/Papers/2025/skorokhodovImprovingDiffusabilityAutoencoders2025.md</guid><pubDate>Thu, 22 Jan 2026 09:47:06 GMT</pubDate></item><item><title><![CDATA[REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2505.16792" target="_self">http://arxiv.org/abs/2505.16792</a>
Related:: Compared to always-on REPA feature alignment for SiT/DiT, their proposed method HASTE adds teacher attention-map distillation and turns alignment off mid-training via a stage-wise termination switch.This paper investigates why REPA improves convergence early in training but makes the diffusion training harder at later epochs. They do a gradient based analysis, where they empirically measure the cosine similarity They observe, that Based on this observation the authors argue that in the beginning the REPA helps the diffusion model, then becomes irrelevant and later conflicts with the diffusion objective. Hence, they propose to switch off the alignment loss. Two methods are considered: (i) Switching off once or simply (ii) switching off once fixed number of iterations is reached.<br>
Additionally, they compose their alignment loss as where is the common REPA-loss and is so called Attention alignment, recently proposed in <a rel="noopener nofollow" class="external-link is-unresolved" href="https://arxiv.org/pdf/2411.09702" target="_self">https://arxiv.org/pdf/2411.09702</a>.
Precisely, where the sum is taken over chosen layers and attention heads and is the cross entropy loss. and denote the query and key matrices of the student (DiT) and the teacher (VFM) respectively. The authors choose mid layers (e.g. 4-7) for ATTA. <br>Space: <a data-tooltip-position="top" aria-label="Overview/Categories/space/flexible" data-href="Overview/Categories/space/flexible" href="overview/categories/space/flexible.html" class="internal-link" target="_self" rel="noopener nofollow">flexible</a>
<br>Rep signal: <a data-href="external-vfm" href="overview/categories/rep_signal/external-vfm.html" class="internal-link" target="_self" rel="noopener nofollow">external-vfm</a>
<br>Injection: <a data-href="alignment-loss" href="overview/categories/injection/alignment-loss.html" class="internal-link" target="_self" rel="noopener nofollow">alignment-loss</a>
<br>Training: <a data-href="two-stage" href="overview/categories/training/two-stage.html" class="internal-link" target="_self" rel="noopener nofollow">two-stage</a> ImageNet: FID 8.9 @ 256x256 px, w/o cfg, 400K training steps, SiT-L as diffusion backbone, DINOv2-B for external representations (see Table 9) Missing rigorous explanation of why mid layer attention alignment works best
No detailed ablations on the effect of the choice of the VFM on performance No theoretical details that give intuition why the gradient angle misalignment occurs, only informal explanations
]]></description><link>overview/papers/2025/wangrepaworksit2025a.html</link><guid isPermaLink="false">Overview/Papers/2025/wangREPAWorksIt2025a.md</guid><pubDate>Thu, 22 Jan 2026 09:46:53 GMT</pubDate></item><item><title><![CDATA[Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2507.01467" target="_self">http://arxiv.org/abs/2507.01467</a>
<br>Related:: <a data-href="kouzelisBoostingGenerativeImage2025" href="overview/papers/2025/kouzelisboostinggenerativeimage2025.html" class="internal-link" target="_self" rel="noopener nofollow">kouzelisBoostingGenerativeImage2025</a>
In *Representation Entanglement Generation* (REG) the diffusion transformer learns to denoise both VAE latents of the image, as well as one additional semantic token from a VFM to also provide representation guidance during inference.REG argues that REPA misses out on generative gains from discriminative representation, as it is leveraged only explicitly during training. They thus propose to jointly learn VAE latents and a single addition token that is obtained through the VFM. In practice they use the CLS token of the VFM, as it yields the best empirical results compared to other approaches (e.g. learnable token, avg(latent_features), avg(DINOv2 features); see Table 5).
During training they use both (i) representation alignment in REPA style, and (ii) the joint modeling.
At inference, they also generate the semantic token to "actively guide and enhance the generation quality".
<br>Space: <a data-tooltip-position="top" aria-label="Overview/Categories/space/flexible" data-href="Overview/Categories/space/flexible" href="overview/categories/space/flexible.html" class="internal-link" target="_self" rel="noopener nofollow">flexible</a>
<br>Rep signal: <a data-href="external-vfm" href="overview/categories/rep_signal/external-vfm.html" class="internal-link" target="_self" rel="noopener nofollow">external-vfm</a>
<br>Injection: <a data-href="joint-modeling" href="overview/categories/injection/joint-modeling.html" class="internal-link" target="_self" rel="noopener nofollow">joint-modeling</a>
<br>Training: <a data-href="two-stage" href="overview/categories/training/two-stage.html" class="internal-link" target="_self" rel="noopener nofollow">two-stage</a> ImageNet: FID 4.6 @ 256x256 px, w/o cfg, 400K training steps, SiT-L as diffusion backbone and DINOv2-B for external representations Their main motivation for the joint modeling is to have representation guidance during inference, however they don't provide any explanation of why the joint modeling gives any advantages during inference
]]></description><link>overview/papers/2025/wurepresentationentanglementgeneration2025.html</link><guid isPermaLink="false">Overview/Papers/2025/wuRepresentationEntanglementGeneration2025.md</guid><pubDate>Thu, 22 Jan 2026 09:46:43 GMT</pubDate></item><item><title><![CDATA[PixelDiT: Pixel Diffusion Transformers for Image Generation]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2511.20645" target="_self">http://arxiv.org/abs/2511.20645</a>
Related:: PixelDiT is a end-to-end pixel space diffusion transformer that splits up the low- and high-level generation task by combining a patch-level and a pixel-level DiT in one architecture.The authors argue that latent diffusion‚Äôs two-stage setup (pretrained autoencoder + diffusion in latent space) introduces a reconstruction bottleneck and can accumulate errors, especially in high-frequency details, motivating end-to-end pixel-space training.
PixelDiT‚Äôs core design is a dual-level transformer: a patch-level DiT handles global semantics with an aggressive patch size (short token sequence), while a pixel-level DiT refines local texture via dense per-pixel token modeling.
Two mechanisms make the pixel pathway efficient:
(i) pixel-wise AdaLN produces per-pixel modulation parameters from semantic tokens (so pixel updates are context-aligned), and
(ii) pixel token compaction, which compresses pixel tokens within each patch before attention and expands them back after, making self-attention computationally feasible while keeping per-pixel updates.
Their training uses the Rectified Flow formulation with a velocity-matching loss, and additionally a REPA loss on mid-level patch tokens to match features from a frozen VFM (DINOv2).
<br>Space: <a data-href="pixel" href="overview/categories/space/pixel.html" class="internal-link" target="_self" rel="noopener nofollow">pixel</a>
<br>Rep signal: <a data-href="external-vfm" href="overview/categories/rep_signal/external-vfm.html" class="internal-link" target="_self" rel="noopener nofollow">external-vfm</a>
<br>Injection: <a data-href="alignment-loss" href="overview/categories/injection/alignment-loss.html" class="internal-link" target="_self" rel="noopener nofollow">alignment-loss</a>
<br>Training: <a data-href="end-to-end" href="overview/categories/training/end-to-end.html" class="internal-link" target="_self" rel="noopener nofollow">end-to-end</a> ImageNet: FID 2.36* @ 256x256 px, with cfg, 80 epochs - since batch_size = 256 corresponds to ~400k training steps, PixelDiT-XL (797M params), DINOv2-B for external representations No ablation that shows REPA vs architectural design contribution to FID
They only provide results with cfg, making it less comparable Conditioning / representations:
Losses:
Architecture notes:
Patch-level modulation input image gets patchified to where is the number of tokens is now projected to hidden size as , where In the DiT blocks, that follows the simple Transformer Block structure, i.e. input -&gt; skip-connection -&gt; normalization (they use RMSNorm) -&gt; Adaptive LayerNorm -&gt; multi-head attention -&gt; scaling -&gt; MLP-block also with RMSNorm, AdaLN and Gating
They use the outputs of the patch-pathway tokens together with timestep embedding as conditioning via for the AdaLN modulation parameters on pixel level diffusion
Pixel-Level Modulation
they then use pixel-level pathway in a reduced hidden dimension to make per-pixel computation efficient
they reshape their input to they project their semantic conditioning token to be in via a linear projection and reshaping to get new modulation parameters which are distinct at every pixel; this pixel-level preciseness allows them do just do self-attention on patch-level in the pixel-level modulation to make it computationally feasible Training notes:
]]></description><link>overview/papers/2025/yupixelditpixeldiffusion2025.html</link><guid isPermaLink="false">Overview/Papers/2025/yuPixelDiTPixelDiffusion2025.md</guid><pubDate>Thu, 22 Jan 2026 09:46:35 GMT</pubDate></item><item><title><![CDATA[Diffusion Transformers with Representation Autoencoders]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2510.11690" target="_self">http://arxiv.org/abs/2510.11690</a>
Related:: This paper proposes to train the diffusion model in the latent space of a VFM instead of a VAE for improved generation quality.They want to simplify the training procedure, not by guiding the diffusion model via an external model (e.g. REPA) but by learning the latent of a VFM. This reduces hyperparameters and simplifies the architecture.
To decode the latents, they train a ViT with their objective as where and denote the encoder (VFM, precisely DINOv2-B/SigLIP2-B and MAE-B) and the decoder (ViT-XL) respectively. They get competitive/better reconstruction FID scores with RAEs compared to typical SD-VAE (see Table 1). To make the RAE decoder more robust, they train it on a smoothed distribution (noise-augmented decoding), effectively learning to decode where denotes the training set processed by the RAE encoder. This enables the decoder to perform well during the generative part.
They further show that the unique minimizer of the flow matching loss is representable via a stack of standard DiT-Block iff , where is the dimension of the tokens. They argue that this is due to the full rank of the noise (see also Back To Basics).<br>
Instead of scaling the width of all the DiT blocks, which is computationally prohibitive, they use a wide DDT (<a rel="noopener nofollow" class="external-link is-unresolved" href="https://arxiv.org/pdf/2504.05741v1" target="_self">https://arxiv.org/pdf/2504.05741v1</a>) head after the standard DiT.<br>
Building on (<a rel="noopener nofollow" class="external-link is-unresolved" href="https://arxiv.org/pdf/2403.03206" target="_self">https://arxiv.org/pdf/2403.03206</a>) they propose a schedule shift but using the effective data dimension computed as "number of tokens times their dimensionality".<br>
They don't include CLS tokens that contain strong semantic information in the training target (cf. arguments in <a data-tooltip-position="top" aria-label="Overview/Papers/2025/singhWhatMattersRepresentation2025" data-href="Overview/Papers/2025/singhWhatMattersRepresentation2025" href="overview/papers/2025/singhwhatmattersrepresentation2025.html" class="internal-link" target="_self" rel="noopener nofollow">singhWhatMattersRepresentation2025</a>)
<br>Space: <a data-href="latent" href="overview/categories/space/latent.html" class="internal-link" target="_self" rel="noopener nofollow">latent</a>
<br>Rep signal: <a data-href="external-vfm" href="overview/categories/rep_signal/external-vfm.html" class="internal-link" target="_self" rel="noopener nofollow">external-vfm</a>
<br>Injection: <a data-href="tokenizer" href="overview/categories/injection/tokenizer.html" class="internal-link" target="_self" rel="noopener nofollow">tokenizer</a>
<br>Training: <a data-href="two-stage" href="overview/categories/training/two-stage.html" class="internal-link" target="_self" rel="noopener nofollow">two-stage</a> ImageNet: FID 2.16 @ 256x256 px, w/o cfg, 80 epochs - about 400k training steps as batchsize = 256: and , LightningDiT-XL as diffusion backbone and train on the latents of DINOv2-B How does the autoencoder part RAE work on large scale (e.g. LAION dataset)?
They use DINOv2 as it works best in the generative part, however they don't provide any explanation for that. What makes their features good for representation? (compare this to Diffusability, MAETok Theorem 1 and What matter)
]]></description><link>overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html</link><guid isPermaLink="false">Overview/Papers/2025/zhengDiffusionTransformersRepresentation2025.md</guid><pubDate>Thu, 22 Jan 2026 09:46:26 GMT</pubDate></item><item><title><![CDATA[Diffuse and Disperse: Image Generation with Representation Regularization]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2506.09027" target="_self">http://arxiv.org/abs/2506.09027</a>
Related:: Additionally to the diffusion loss, they add a representation regularization, called dispersive loss, that does not use external models, and is fully self-supervised.Their method aims to simplify the diffusion training by explicitly improving the representation quality of the model's internal representations of the data. They define dispersive loss functions as contrastive losses without positive pairs, for example (their Table 1) This is supposed to encourage the diffusion model to have its representations spread out in space.
The authors argue that as the regression diffusion loss already acts similarly to the loss part between positive pairs, the latter is redundant when training diffusion models.
<br>Space: <a data-tooltip-position="top" aria-label="Overview/Categories/space/flexible" data-href="Overview/Categories/space/flexible" href="overview/categories/space/flexible.html" class="internal-link" target="_self" rel="noopener nofollow">flexible</a>
<br>Rep signal: <a data-href="internal" href="overview/categories/rep_signal/internal.html" class="internal-link" target="_self" rel="noopener nofollow">internal</a>
<br>Injection: <a data-tooltip-position="top" aria-label="Overview/Categories/injection/none" data-href="Overview/Categories/injection/none" href="overview/categories/injection/none.html" class="internal-link" target="_self" rel="noopener nofollow">none</a>
<br>Training: <a data-href="end-to-end" href="overview/categories/training/end-to-end.html" class="internal-link" target="_self" rel="noopener nofollow">end-to-end</a> ImageNet: FID 16.68 @ 256x256 px, w/o cfg, 400K training steps, SiT-L as diffusion backbone (see Figure 4) They don't provide a theoretical explanation of why their method works
]]></description><link>overview/papers/2025/wangdiffusedisperseimage2025.html</link><guid isPermaLink="false">Overview/Papers/2025/wangDiffuseDisperseImage2025.md</guid><pubDate>Thu, 22 Jan 2026 09:45:56 GMT</pubDate></item><item><title><![CDATA[Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think]]></title><description><![CDATA[
Paper: <a rel="noopener nofollow" class="external-link is-unresolved" href="http://arxiv.org/abs/2410.06940" target="_self">http://arxiv.org/abs/2410.06940</a>
Related:: They add a representation alignment loss that aligns the diffusion models hidden representations to those of a vision foundation model (VFM) to speed up DiT/SiT training.They motivate their work by previous findings that better diffusion models learn hidden representations are semantically more meaningful (higher linear probing accuracy). Representations are already weakly aligned (measured via CKNNA) with those of DINOv2. To improve alignment they add an alignment loss to their overall lossHere denotes the patch index, the hidden representation at a fixed layer, the VFMs representation and a similarity function, e.g. cosine similarity. is a 3-layer MLP that is learned during training. They find that alignment at earlier layers (e.g. 8 out of 24 layers) yields best results.
The overall loss becomes where is a hyperparameter used to control the amount of alignment.
<br>Space: <a data-tooltip-position="top" aria-label="Overview/Categories/space/flexible" data-href="Overview/Categories/space/flexible" href="overview/categories/space/flexible.html" class="internal-link" target="_self" rel="noopener nofollow">flexible</a>
<br>Rep signal: <a data-href="external-vfm" href="overview/categories/rep_signal/external-vfm.html" class="internal-link" target="_self" rel="noopener nofollow">external-vfm</a>
<br>Injection: <a data-href="alignment-loss" href="overview/categories/injection/alignment-loss.html" class="internal-link" target="_self" rel="noopener nofollow">alignment-loss</a>
<br>Training: <a data-href="two-stage" href="overview/categories/training/two-stage.html" class="internal-link" target="_self" rel="noopener nofollow">two-stage</a> <br>ImageNet: FID 7.9 @ 256x256 px, w/o cfg, 400K training steps, DINOv2-B, SiT-XL/2 (see Table 8 in <a data-tooltip-position="top" aria-label="Overview/Papers/2025/singhWhatMattersRepresentation2025" data-href="Overview/Papers/2025/singhWhatMattersRepresentation2025" href="overview/papers/2025/singhwhatmattersrepresentation2025.html" class="internal-link" target="_self" rel="noopener nofollow">singhWhatMattersRepresentation2025</a>) <br>Why are earlier layers more suitable for alignment? They argue that later layers focus on high-frequency details, but <a data-tooltip-position="top" aria-label="Overview/Papers/2025/singhWhatMattersRepresentation2025" data-href="Overview/Papers/2025/singhWhatMattersRepresentation2025" href="overview/papers/2025/singhwhatmattersrepresentation2025.html" class="internal-link" target="_self" rel="noopener nofollow">singhWhatMattersRepresentation2025</a> use early layers (e.g. 4, 6 or 8, see Table 1c) as well, who don't focus on semantic/high-level structure.
They focus mainly on LDM (SiT is their base backbone model) - more extensive results in pixel diffusion might be interesting.
]]></description><link>overview/papers/2024/yurepresentationalignmentgeneration2025.html</link><guid isPermaLink="false">Overview/Papers/2024/yuRepresentationAlignmentGeneration2025.md</guid><pubDate>Thu, 22 Jan 2026 09:45:25 GMT</pubDate></item><item><title><![CDATA[VisualOverview]]></title><description><![CDATA[<a data-tooltip-position="top" aria-label="Overview/Papers/2025/liBackBasicsLet2025" data-href="Overview/Papers/2025/liBackBasicsLet2025" href="overview/papers/2025/libackbasicslet2025.html" class="internal-link" target="_self" rel="noopener nofollow">Back to Basics: Let Denoising Generative Models Denoise</a>Idea: Just Image Transformers (JiT) predict in pixel space to leverage low dimensionality of data manifold.<br><img alt="Bildschirmfoto 2026-01-21 um 15.59.33.png" src="overview/data/bildschirmfoto-2026-01-21-um-15.59.33.png" target="_self" style="width: 220px; max-width: 100%;">FID: 2.79 | space: pixel | training-regime: end-to-end<br><a data-tooltip-position="top" aria-label="Overview/Papers/2025/chenDiffusionAutoencodersAre2025" data-href="Overview/Papers/2025/chenDiffusionAutoencodersAre2025" href="overview/papers/2025/chendiffusionautoencodersare2025.html" class="internal-link" target="_self" rel="noopener nofollow">Diffusion Autoencoders are Scalable Image Tokenizers</a>Idea: Replace the VAE decocer with a diffusion model conditioned on the latent of an encoder to not rely on complex losses (e.g. GAN &amp; LPIPS).<br><img alt="Bildschirmfoto 2026-01-21 um 16.35.48.png" src="overview/data/bildschirmfoto-2026-01-21-um-16.35.48.png" target="_self" style="width: 300px; max-width: 100%;">FID: 6.29* | space: latent | training-regime: end-to-end<br><a data-tooltip-position="top" aria-label="Overview/Papers/2025/singhWhatMattersRepresentation2025" data-href="Overview/Papers/2025/singhWhatMattersRepresentation2025" href="overview/papers/2025/singhwhatmattersrepresentation2025.html" class="internal-link" target="_self" rel="noopener nofollow">WHAT MATTERS FOR REPRESENTATION ALIGNMENT: GLOBAL INFORMATION OR SPATIAL STRUCTURE?</a>Idea: iREPA tweaks REPA by replacing the projection MLP with a conv layer and adding spatial normalization to better preserve ‚Äúspatial structure‚Äù during teacher-to-diffusion feature alignment.<br><img alt="Bildschirmfoto 2026-01-21 um 16.41.45.png" src="overview/data/bildschirmfoto-2026-01-21-um-16.41.45.png" target="_self">FID: 7.52 | space: flexible | training-regime: two-stage<br><a data-tooltip-position="top" aria-label="Overview/Papers/2025/zhengDiffusionTransformersRepresentation2025" data-href="Overview/Papers/2025/zhengDiffusionTransformersRepresentation2025" href="overview/papers/2025/zhengdiffusiontransformersrepresentation2025.html" class="internal-link" target="_self" rel="noopener nofollow">Diffusion Transformers With Represenation Autoencoders</a>Idea: Train the diffusion model in the latent space of a strong VFM (e.g. DINOv2).<br><img alt="Bildschirmfoto 2026-01-21 um 16.13.27.png" src="overview/data/bildschirmfoto-2026-01-21-um-16.13.27.png" target="_self" style="width: 300px; max-width: 100%;">FID: 2.16* | space: latent | training-regime: two-stage<br><a data-tooltip-position="top" aria-label="Overview/Papers/2025/skorokhodovImprovingDiffusabilityAutoencoders2025" data-href="Overview/Papers/2025/skorokhodovImprovingDiffusabilityAutoencoders2025" href="overview/papers/2025/skorokhodovimprovingdiffusabilityautoencoders2025.html" class="internal-link" target="_self" rel="noopener nofollow">Improving the Diffusability of Autoencoders</a> Idea: They fine tune the VAE using scale equivariance, such that the latent space has less high-frequency signals, simplifying the training task for the diffusion model.<br><img alt="Bildschirmfoto 2026-01-21 um 17.04.23.png" src="overview/data/bildschirmfoto-2026-01-21-um-17.04.23.png" target="_self">FID: 9.61* | space: latent | training-regime: two-stage<br><a data-tooltip-position="top" aria-label="Overview/Papers/2024/yuRepresentationAlignmentGeneration2025" data-href="Overview/Papers/2024/yuRepresentationAlignmentGeneration2025" href="overview/papers/2024/yurepresentationalignmentgeneration2025.html" class="internal-link" target="_self" rel="noopener nofollow">Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think</a>Idea: REPA adds a representation alignment loss that aligns the diffusion models hidden representations to those of a vision foundation model (VFM) to speed up DiT/SiT training.<br><img alt="Bildschirmfoto 2026-01-21 um 16.45.44.png" src="overview/data/bildschirmfoto-2026-01-21-um-16.45.44.png" target="_self" style="width: 300px; max-width: 100%;">FID: 7.9 | space: flexible | training-regime: two-stage<br><a data-tooltip-position="top" aria-label="Overview/Papers/2025/lengREPAEUnlockingVAE2025a" data-href="Overview/Papers/2025/lengREPAEUnlockingVAE2025a" href="overview/papers/2025/lengrepaeunlockingvae2025a.html" class="internal-link" target="_self" rel="noopener nofollow">REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers</a>Idea: End-to-end training where the VAE component is optimized by backpropagating the REPA loss.<br><img alt="Bildschirmfoto 2026-01-21 um 16.25.46.png" src="overview/data/bildschirmfoto-2026-01-21-um-16.25.46.png" target="_self" style="width: 200px; max-width: 100%;">FID: 4.07 | space: latent | training-regime: end-to-end<br><a data-tooltip-position="top" aria-label="Overview/Papers/2025/yuPixelDiTPixelDiffusion2025" data-href="Overview/Papers/2025/yuPixelDiTPixelDiffusion2025" href="overview/papers/2025/yupixelditpixeldiffusion2025.html" class="internal-link" target="_self" rel="noopener nofollow">PixelDiT: Pixel Diffusion Transformers for Image Generation</a>Idea: Dual level transformer based architecture that focuses on semantics first and pixel-level afterwards to operate efficiently in pixel space.<br><img alt="Bildschirmfoto 2026-01-21 um 16.19.47.png" src="overview/data/bildschirmfoto-2026-01-21-um-16.19.47.png" target="_self" style="width: 250px; max-width: 100%;">
FID: 2.36* | space: pixel | training-regime: end-to-end<br><a data-tooltip-position="top" aria-label="wangREPAWorksIt2025a" data-href="wangREPAWorksIt2025a" href="overview/papers/2025/wangrepaworksit2025a.html" class="internal-link" target="_self" rel="noopener nofollow">REPA Works Until It Doesn‚Äôt: Early-Stopped, Holistic Alignment Supercharges Diffusion Training</a>Idea: Compared to always-on REPA feature alignment for SiT/DiT, HASTE adds teacher attention-map distillation and turns alignment off mid-training via a stage-wise termination switch.<br><img alt="Bildschirmfoto 2026-01-21 um 16.50.44.png" src="overview/data/bildschirmfoto-2026-01-21-um-16.50.44.png" target="_self" style="width: 350px; max-width: 100%;">FID: 8.9 | space: flexible | training-regime: two-stage<br><a data-tooltip-position="top" aria-label="Overview/Papers/2025/wangDiffuseDisperseImage2025" data-href="Overview/Papers/2025/wangDiffuseDisperseImage2025" href="overview/papers/2025/wangdiffusedisperseimage2025.html" class="internal-link" target="_self" rel="noopener nofollow">Diffuse and Disperse: Image Generation with Representation Regularization</a> Idea: Use a dispersive loss so that the model leverages the full space (contrastive loss w/o positive pairs).<br><img alt="Bildschirmfoto 2026-01-21 um 16.09.13.png" src="overview/data/bildschirmfoto-2026-01-21-um-16.09.13.png" target="_self">FID: 16.68 | space: flexible | training-regime: end-to-end<br><a data-tooltip-position="top" aria-label="wuRepresentationEntanglementGeneration2025" data-href="wuRepresentationEntanglementGeneration2025" href="overview/papers/2025/wurepresentationentanglementgeneration2025.html" class="internal-link" target="_self" rel="noopener nofollow">Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think</a> Idea: Additionally to the REPA loss, learn to denoise the CLS token.<br>
<img alt="Bildschirmfoto 2026-01-21 um 16.31.07.png" src="overview/data/bildschirmfoto-2026-01-21-um-16.31.07.png" target="_self" style="width: 200px; max-width: 100%;">FID: 4.6 | space: flexible | training-regime: two-stage<br><a data-tooltip-position="top" aria-label="kouzelisBoostingGenerativeImage2025" data-href="kouzelisBoostingGenerativeImage2025" href="overview/papers/2025/kouzelisboostinggenerativeimage2025.html" class="internal-link" target="_self" rel="noopener nofollow">Boosting Generative Image Modeling via Joint Image-Feature Synthesis</a>Idea: The diffusion model jointly learn the latents of a VAE and the latents of a VFM, additionally to representation alignment.<br><img alt="Bildschirmfoto 2026-01-21 um 16.54.26.png" src="overview/data/bildschirmfoto-2026-01-21-um-16.54.26.png" target="_self">FID: 9.4 | space: flexible | training-regime: two-stagevery similar methods
essentially PCA (ReDi) vs CLS (REG) Latent space Pixel space Flexible]]></description><link>overview/maps/visualoverview.html</link><guid isPermaLink="false">Overview/Maps/VisualOverview.canvas</guid><pubDate>Thu, 22 Jan 2026 09:34:39 GMT</pubDate><enclosure url="." length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;.&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bildschirmfoto 2026-01-21 um 17.04.23]]></title><description><![CDATA[<img src="overview/data/bildschirmfoto-2026-01-21-um-17.04.23.png" target="_self">]]></description><link>overview/data/bildschirmfoto-2026-01-21-um-17.04.23.html</link><guid isPermaLink="false">Overview/Data/Bildschirmfoto 2026-01-21 um 17.04.23.png</guid><pubDate>Wed, 21 Jan 2026 16:04:25 GMT</pubDate><enclosure url="." length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;.&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bildschirmfoto 2026-01-21 um 16.54.26]]></title><description><![CDATA[<img src="overview/data/bildschirmfoto-2026-01-21-um-16.54.26.png" target="_self">]]></description><link>overview/data/bildschirmfoto-2026-01-21-um-16.54.26.html</link><guid isPermaLink="false">Overview/Data/Bildschirmfoto 2026-01-21 um 16.54.26.png</guid><pubDate>Wed, 21 Jan 2026 15:54:29 GMT</pubDate><enclosure url="." length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;.&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bildschirmfoto 2026-01-21 um 16.50.44]]></title><description><![CDATA[<img src="overview/data/bildschirmfoto-2026-01-21-um-16.50.44.png" target="_self">]]></description><link>overview/data/bildschirmfoto-2026-01-21-um-16.50.44.html</link><guid isPermaLink="false">Overview/Data/Bildschirmfoto 2026-01-21 um 16.50.44.png</guid><pubDate>Wed, 21 Jan 2026 15:50:47 GMT</pubDate><enclosure url="." length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;.&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bildschirmfoto 2026-01-21 um 16.45.44]]></title><description><![CDATA[<img src="overview/data/bildschirmfoto-2026-01-21-um-16.45.44.png" target="_self">]]></description><link>overview/data/bildschirmfoto-2026-01-21-um-16.45.44.html</link><guid isPermaLink="false">Overview/Data/Bildschirmfoto 2026-01-21 um 16.45.44.png</guid><pubDate>Wed, 21 Jan 2026 15:45:48 GMT</pubDate><enclosure url="." length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;.&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bildschirmfoto 2026-01-21 um 16.41.45]]></title><description><![CDATA[<img src="overview/data/bildschirmfoto-2026-01-21-um-16.41.45.png" target="_self">]]></description><link>overview/data/bildschirmfoto-2026-01-21-um-16.41.45.html</link><guid isPermaLink="false">Overview/Data/Bildschirmfoto 2026-01-21 um 16.41.45.png</guid><pubDate>Wed, 21 Jan 2026 15:41:48 GMT</pubDate><enclosure url="." length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;.&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bildschirmfoto 2026-01-21 um 16.35.48]]></title><description><![CDATA[<img src="overview/data/bildschirmfoto-2026-01-21-um-16.35.48.png" target="_self">]]></description><link>overview/data/bildschirmfoto-2026-01-21-um-16.35.48.html</link><guid isPermaLink="false">Overview/Data/Bildschirmfoto 2026-01-21 um 16.35.48.png</guid><pubDate>Wed, 21 Jan 2026 15:35:52 GMT</pubDate><enclosure url="." length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;.&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bildschirmfoto 2026-01-21 um 16.31.07]]></title><description><![CDATA[<img src="overview/data/bildschirmfoto-2026-01-21-um-16.31.07.png" target="_self">]]></description><link>overview/data/bildschirmfoto-2026-01-21-um-16.31.07.html</link><guid isPermaLink="false">Overview/Data/Bildschirmfoto 2026-01-21 um 16.31.07.png</guid><pubDate>Wed, 21 Jan 2026 15:31:13 GMT</pubDate><enclosure url="." length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;.&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bildschirmfoto 2026-01-21 um 16.25.46]]></title><description><![CDATA[<img src="overview/data/bildschirmfoto-2026-01-21-um-16.25.46.png" target="_self">]]></description><link>overview/data/bildschirmfoto-2026-01-21-um-16.25.46.html</link><guid isPermaLink="false">Overview/Data/Bildschirmfoto 2026-01-21 um 16.25.46.png</guid><pubDate>Wed, 21 Jan 2026 15:25:51 GMT</pubDate><enclosure url="." length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;.&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bildschirmfoto 2026-01-21 um 16.19.47]]></title><description><![CDATA[<img src="overview/data/bildschirmfoto-2026-01-21-um-16.19.47.png" target="_self">]]></description><link>overview/data/bildschirmfoto-2026-01-21-um-16.19.47.html</link><guid isPermaLink="false">Overview/Data/Bildschirmfoto 2026-01-21 um 16.19.47.png</guid><pubDate>Wed, 21 Jan 2026 15:19:52 GMT</pubDate><enclosure url="." length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;.&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bildschirmfoto 2026-01-21 um 16.13.27]]></title><description><![CDATA[<img src="overview/data/bildschirmfoto-2026-01-21-um-16.13.27.png" target="_self">]]></description><link>overview/data/bildschirmfoto-2026-01-21-um-16.13.27.html</link><guid isPermaLink="false">Overview/Data/Bildschirmfoto 2026-01-21 um 16.13.27.png</guid><pubDate>Wed, 21 Jan 2026 15:13:30 GMT</pubDate><enclosure url="." length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;.&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bildschirmfoto 2026-01-21 um 16.09.13]]></title><description><![CDATA[<img src="overview/data/bildschirmfoto-2026-01-21-um-16.09.13.png" target="_self">]]></description><link>overview/data/bildschirmfoto-2026-01-21-um-16.09.13.html</link><guid isPermaLink="false">Overview/Data/Bildschirmfoto 2026-01-21 um 16.09.13.png</guid><pubDate>Wed, 21 Jan 2026 15:09:17 GMT</pubDate><enclosure url="." length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;.&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bildschirmfoto 2026-01-21 um 15.59.33]]></title><description><![CDATA[<img src="overview/data/bildschirmfoto-2026-01-21-um-15.59.33.png" target="_self">]]></description><link>overview/data/bildschirmfoto-2026-01-21-um-15.59.33.html</link><guid isPermaLink="false">Overview/Data/Bildschirmfoto 2026-01-21 um 15.59.33.png</guid><pubDate>Wed, 21 Jan 2026 14:59:36 GMT</pubDate><enclosure url="." length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;.&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[flexible]]></title><link>overview/categories/training/flexible.html</link><guid isPermaLink="false">Overview/Categories/training/flexible.md</guid><pubDate>Wed, 21 Jan 2026 14:04:50 GMT</pubDate></item><item><title><![CDATA[none]]></title><link>overview/categories/injection/none.html</link><guid isPermaLink="false">Overview/Categories/injection/none.md</guid><pubDate>Mon, 19 Jan 2026 16:21:27 GMT</pubDate></item><item><title><![CDATA[two-stage]]></title><link>overview/categories/training/two-stage.html</link><guid isPermaLink="false">Overview/Categories/training/two-stage.md</guid><pubDate>Mon, 19 Jan 2026 10:35:13 GMT</pubDate></item><item><title><![CDATA[end-to-end]]></title><link>overview/categories/training/end-to-end.html</link><guid isPermaLink="false">Overview/Categories/training/end-to-end.md</guid><pubDate>Mon, 19 Jan 2026 10:35:08 GMT</pubDate></item><item><title><![CDATA[flexible]]></title><link>overview/categories/space/flexible.html</link><guid isPermaLink="false">Overview/Categories/space/flexible.md</guid><pubDate>Mon, 19 Jan 2026 10:17:52 GMT</pubDate></item><item><title><![CDATA[tokenizer]]></title><link>overview/categories/injection/tokenizer.html</link><guid isPermaLink="false">Overview/Categories/injection/tokenizer.md</guid><pubDate>Mon, 19 Jan 2026 10:17:34 GMT</pubDate></item><item><title><![CDATA[joint-modeling]]></title><link>overview/categories/injection/joint-modeling.html</link><guid isPermaLink="false">Overview/Categories/injection/joint-modeling.md</guid><pubDate>Mon, 19 Jan 2026 10:17:28 GMT</pubDate></item><item><title><![CDATA[alignment-loss]]></title><link>overview/categories/injection/alignment-loss.html</link><guid isPermaLink="false">Overview/Categories/injection/alignment-loss.md</guid><pubDate>Mon, 19 Jan 2026 10:17:21 GMT</pubDate></item><item><title><![CDATA[none]]></title><link>overview/categories/rep_signal/none.html</link><guid isPermaLink="false">Overview/Categories/rep_signal/none.md</guid><pubDate>Mon, 19 Jan 2026 10:17:01 GMT</pubDate></item><item><title><![CDATA[internal]]></title><link>overview/categories/rep_signal/internal.html</link><guid isPermaLink="false">Overview/Categories/rep_signal/internal.md</guid><pubDate>Mon, 19 Jan 2026 10:16:48 GMT</pubDate></item><item><title><![CDATA[external-vfm]]></title><link>overview/categories/rep_signal/external-vfm.html</link><guid isPermaLink="false">Overview/Categories/rep_signal/external-vfm.md</guid><pubDate>Mon, 19 Jan 2026 10:15:59 GMT</pubDate></item><item><title><![CDATA[pixel]]></title><link>overview/categories/space/pixel.html</link><guid isPermaLink="false">Overview/Categories/space/pixel.md</guid><pubDate>Fri, 16 Jan 2026 15:38:56 GMT</pubDate></item><item><title><![CDATA[latent]]></title><link>overview/categories/space/latent.html</link><guid isPermaLink="false">Overview/Categories/space/latent.md</guid><pubDate>Fri, 16 Jan 2026 15:38:44 GMT</pubDate></item></channel></rss>